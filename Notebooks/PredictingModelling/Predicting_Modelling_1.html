
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Predicting Modelling 1 &#8212; DIMPAH</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">DIMPAH</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../Introduction.html">
   Introduction to Python
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../social_analytics.html">
   Social Analytics
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../AI_for_society.html">
   AI for society
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../PredictingModelling.html">
   PredictingModelling
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../DigitalMarketing.html">
   Digital Marketing
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../HistoricalCultures.html">
   Historical Cultures
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../SocialSensing.html">
   Social Sensing
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../VisualsArts.html">
   Visuals Arts
  </a>
  <ul class="simple collapse-ul">
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/Notebooks/PredictingModelling/Predicting_Modelling_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/zarahvh/dimpah"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/zarahvh/dimpah/issues/new?title=Issue%20on%20page%20%2FNotebooks/PredictingModelling/Predicting_Modelling_1.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/zarahvh/dimpah/master?urlpath=tree/Notebooks/PredictingModelling/Predicting_Modelling_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#background-1-the-data-science-process">
   Background 1: The Data Science Process
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#social-and-cultural-analytics-and-its-data">
     Social and cultural analytics and its data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predicting-taste">
   Predicting Taste
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#identify-the-problem-machine-tasting-wines">
   Identify the problem: Machine-tasting  Wines
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#collect-data">
     Collect Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prepare-data">
     Prepare Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#define-the-training-data">
     Define the Training Data
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#modelling-and-predicting">
   Modelling and Predicting
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#background-2-neural-networks">
     Background 2: Neural Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Modelling and Predicting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#predicting">
     Predicting
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#evaluate-model">
   Evaluate Model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-the-results">
     Interpret the Results
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-engineering">
   Feature engineering
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="predicting-modelling-1">
<h1>Predicting Modelling 1<a class="headerlink" href="#predicting-modelling-1" title="Permalink to this headline">¶</a></h1>
<p>Today, we will learn that machine learning is much less scary than science fiction will want us to believe. This is not because we have benevolent machines, which only want our best, but simply because these machines are quite far away from living their own life without our input, as Skynet manages in ‘Terminator’. For the time being, machines still learn best when provided with human input. Furthermore, machines learn in most applications not because they want to start to understand the meaning of life and find out that humans are obstacles to true life, but because they learn to complete a particular task. Machines learn to be part of the workbenches of digital productions.</p>
<p>It is maybe less a link to artificial intelligence in science fiction than the fact that machines learn from our examples and need to be fed with large amounts of data to learn that makes machine learning an ethically difficult endeavour. Machine learning demands ever more data, which requires vast amounts of energy. Most aspects of our lives are recorded in vast data stores that are easily accessible to machines. Governments, businesses and individuals are recording and reporting all manners of information from the monumental to the mundane. As long as the activities can be transformed into digital formats, you can be certain that somebody will record it.</p>
<p>In such a world, machines learn by consuming data and humans continuously add new digital methods of machine learning that can exploit this data. These can be some of the statistical methods we have already met or more advanced ones, we will meet today. The digital methods we learn about today have in common that they aim to predict new observations from old observations. They are all empirical and predictive using models.</p>
<p>Machine learning algorithms are all around you. They have tried to predict the outcomes of elections and referenda, can identify spam messages, predict crime and natural disasters, target donors and voters as well as finally have learned how to drive cars. Recently, they got it wrong quite often: <a class="reference external" href="http://www.kdnuggets.com/2016/11/trump-shows-limits-prediction.html">http://www.kdnuggets.com/2016/11/trump-shows-limits-prediction.html</a></p>
<p>Many stories are told about the uses and abuses of machine learning. You can find some in the readings. Given how much machine learning is now part of our everyday life, it is maybe surprising that there are not even more stories.</p>
<p>We also still lack an ethics of machine learning, which is developing so fast that it is difficult for laws and norms to stay up to date. There is, for instance, an on-going debate how biased machine learning algorithms are with regard to race and gender. Machine learning has also made it possible to identify people based on the region they live, the products they buy, etc.</p>
<p>As a machine learning practitioner, you are often required to exclude revealing data that is ethically problematic. But this is not an easy task, as sometimes the connections are not obvious and might only be revealed after you have trained the machine to learn.</p>
<p>We start with the same libraries. Run the cell below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="background-1-the-data-science-process">
<h2>Background 1: The Data Science Process<a class="headerlink" href="#background-1-the-data-science-process" title="Permalink to this headline">¶</a></h2>
<div class="section" id="social-and-cultural-analytics-and-its-data">
<h3>Social and cultural analytics and its data<a class="headerlink" href="#social-and-cultural-analytics-and-its-data" title="Permalink to this headline">¶</a></h3>
<p>Just like humans, machines use data to generalize. They abstract the data and develop its underlying principles, because humans tell them how. In the words of machine learning, machines form a model, which assigns meaning and represents knowledge. The model summarizes the data and makes explicit patterns among data.</p>
<p>There are many different types of models. We have already seen some and others you will know from school. Models can be (statistical) equations, figures like graphs or trees, rules or clusters. Machines don’t choose the type of models, we choose them for them when analysing the task at hand and the available data.</p>
<p>The computer learns to fit the model to the data in a process called training. However, computational modelling does not end here. We also need to test the model in a separate testing process. The model thus does not include anything else but what can be found in the data already. It can nevertheless be interesting, as the model might surface connections that we did not recognize before. Newton discovered gravity this way by fitting a series of equations (a model) to observations of falling apples – if the myth is to be believed. Gravity was always there but it was observed for the first time in a model. Modelling is then never perfect. It generally involves some kind of bias or systematic error. Newton’s laws of gravity are not as universal as he thought they would be.</p>
<p>Errors like this do not have to be a bad thing, because they can lead the computer to be able to learn a better model, correcting previous mistakes. But generally, bias is to be avoided. All learning has weaknesses and is biased in a particular way. Researchers are still looking for the universal model that is better than the rest of them but will probably never find it. Therefore, it is really important to understand how a model can overcome bias. This is the purpose of testing it on new data.</p>
<p>Unfortunately, especially in our domain of social and cultural analytics, models often fall short of desirable performance. Humans are difficult for computers and their data is very noisy. This means that social and cultural data includes many errors because observations have not been measured correctly or maybe they are simply impossible to measure. How do you quantify, for instance, love? It seems impossible, but online match-making agencies still make a business out of predicting love.</p>
<p>Humans are also inconsistent and report data wrongly. Finally, especially in history we simply do not have data for all time periods. Often, the records have simply been lost. Even if we have data, it will include many missing values or will be badly captured according to diverse and sometimes contradictory standards.</p>
<p>A final complication with data in social and cultural analytics that has only recently emerged is the limited access we have to the data. Because it is so valuable, it is kept behind the walls of company servers and is not shared.</p>
<p>So, machine learning is not artificial intelligence yet but a laborious collaboration between humans and machines that involves trying models and fighting with (bad) data. Machine learning then leads to data science, which is a process that consists of a series of repeatable steps, which we will learn about today. Schutt and O’Neil (2013), have given us an excellent overview of the art of data science.</p>
<p><img alt="alt text" src="../../_images/process.png" /></p>
<p>According to the Figure, we first need to collect (raw) data in a form that we can process it. The next step explores the data and cleans it. People in data science like to emphasize that this is about 80% of the whole work. Then, we need a question we would like to answer with the data. This question will of course be at the beginning of our work but will likely also change after the initial exploration. Based on the question and the exploration, we start with the model and train it using a subset of the data. After training, we need to evaluate the model’s performance by running a series of test predictions against test data. The result of the evaluation will then be used to improve the model’s performance iteratively until we are satisfied that the model performs as best as possible, and decisions can be confidently made.</p>
<p>Before we experience the art of machine learning and prediction, let’s quickly remind ourselves of what data is in the eye of the machine. Data generally describes a series of observations, which in R are generally captured in the rows of a data frame. Each observation is defined by its features (characteristics), which are the columns of a data frame. If a feature represents a characteristic measured in numbers, it is unsurprisingly called numeric. For instance, the years of the State of the Union Speeches were numerical. Alternatively, if a feature measures an attribute that is represented by a set of categories, the feature is called categorical or nominal. For instance, the colour codes for red, green and blue are categorical. A special case of categorical variables is called ordinal, which designates a nominal variable with categories falling in an ordered list. Movie reviews on Netflix are, for instance, ordinal, because they only cover numbers from 1 to 5.</p>
<p>You might also remember that we distinguished earlier supervised learning from unsupervised learning. We learned that clustering algorithms are an example of unsupervised learning where a machine discovers patterns/clusters in the data by itself. Today, we mainly work on the much larger group of supervised learning algorithms, where an algorithm is given a set of training data and then learns a combination of features that predicts certain behaviour such as whether an earthquake will take place soon or a crime will be committed. What we are trying to predict is also called a target variable.</p>
</div>
</div>
<div class="section" id="predicting-taste">
<h2>Predicting Taste<a class="headerlink" href="#predicting-taste" title="Permalink to this headline">¶</a></h2>
<p>Today, we will predict something that seems to define a human as inherently subjective. We will predict taste and in particular we will try to predict whether wine tastes good or bad. In the language of machine learning, this is a classification task. Our classification will predict whether any wine will fall into either one of two classes: good or bad wine.</p>
<p>We will thus solve an ancient problem of philosophy, which interogates the subjectivity of taste or the aesthetic judgement (<a class="reference external" href="http://plato.stanford.edu/entries/aesthetic-judgment/">http://plato.stanford.edu/entries/aesthetic-judgment/</a>). For the German philosopher Kant, taste judgments are universal and subjective at the same time. A key part of his Critique of Judgement, Kant demands more from taste than we are generally willing to attribute to it: ‘Many things may for [a person] possess charm and agreeableness — no one cares about that; but when he puts a thing on a pedestal and calls it beautiful, he demands the same delight from others. He judges not merely for himself, but for all men, and then speaks of beauty as if it were a property of things. (…). He blames them if they judge differently, and denies them taste, which he still requires of them as something they ought to have; (…).’ (<a class="reference external" href="http://oll.libertyfund.org/titles/kant-the-critique-of-judgement">http://oll.libertyfund.org/titles/kant-the-critique-of-judgement</a>, §7). Today, we will use the machine to find out how something can be subjective and universal at the same time.</p>
<p>To illustrate how machines classify, let’s first go through a simplified dataset that helps us understand taste. Because we like it sweet and crunchy, we create a training dataset by tasting 1,000 foods and record for each of them how crunchy and how sweet they were. Both crunchy and sweet are ordinal features with a range from 1 to 10. Next, we would like to map this data into a so-called feature-space with 2-axes: one for crunchiness and one for sweetness. This example is taken from the excellent Lantz (2013) (Machine learning with R. Packt Publishing Ltd.)</p>
<p>Lantz produced a nice visualisation of such a feature space with a few example foods:
<img alt="alt text" src="../../_images/lantz-1.png" /></p>
<p>Lantz notices that in this feature space ‘similar types of food tend to be grouped closely together. (…), vegetables tend to be crunchy but not sweet, fruits tend to be sweet and either crunchy or not crunchy, while proteins tend to be neither crunchy nor sweet.’ (p. 68). Similarity is thus based on the distance of the items in the feature space.
<img alt="alt text" src="../../_images/lantz-2.png" /></p>
<p>Next, we taste for the first time a tomato and add it to the feature space.
<img alt="alt text" src="../../_images/lantz-3.png" /></p>
<p>Based on this mapping how would we classify the tomato? Is it a vegetable or a fruit? The figure is not very conclusive because we cannot really determine which group the tomato is closer to in the feature space.</p>
<p>You have just learned how a machine would learn and think about the tomato as well as which decisions it would have to make to understand tomatoes. Machines learn similarities in feature spaces using distances.</p>
</div>
<div class="section" id="identify-the-problem-machine-tasting-wines">
<h2>Identify the problem: Machine-tasting  Wines<a class="headerlink" href="#identify-the-problem-machine-tasting-wines" title="Permalink to this headline">¶</a></h2>
<p>Let’s go next through our example of tasting wines next and explore the individual steps of machine learning more closely. The data comes from <a class="reference external" href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a>. Check it out. It’s a famous repository for machine learning datasets.</p>
<p>The wine data (<a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">http://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>) consists of 2 CSV files, one for white wines and another for red ones. The two datasets are related to red and white variants of the Portuguese Vinho Verde wine, and were first used in Cortez et al (2009) (Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4): 547-553).</p>
<p>I first thought of this example, when I learned that somebody else had already ‘outsmarted’ the best wine experts with machine learning: <a class="reference external" href="https://wineindustryinsight.com/?p=59721">https://wineindustryinsight.com/?p=59721</a>. Today, we try and reproduce this approach.</p>
<p>The first step for us is to download the data so that we can work with it. Let us repeat one more time the steps in detail. Perhaps the most common data format of freely available structured data are Comma-Separated Values (CSV) files, which, as the name suggests, uses the comma as a delimiter. CSV files can be imported to and exported from many common data repositories. To load CSV into Python, we use pandas read_csv() function. You use it by specifying a path to the file you want to import, e.g. /path/to/mydata.csv, when calling the pd.read_csv() function after importing pandas again. Here we use it to load the data directly from the web. Can you see how it is done? Also, for the first time we specify the delimiter=’;’, which separates the records.</p>
<div class="section" id="collect-data">
<h3>Collect Data<a class="headerlink" href="#collect-data" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">red</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
     <span class="s1">&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">)</span>
<span class="n">white</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
     <span class="s1">&#39;http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;;&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This creates two data frames, one for each type of wine. read_csv() directly accesses the data frames from the web, as you can see, because it uses an http address. Please, note that I generally would advise you to download the data first, as you can never be certain whether you will always have a working Internet connection.
As we would like to follow the work by Cortez et al. as closely as possible, we next add another feature/column to capture the colour of the wine.</p>
<p>Run <code class="docutils literal notranslate"><span class="pre">red['color']</span> <span class="pre">=</span> <span class="pre">'red'</span></code> to define a column called color in red that only contains the string ‘red’.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">red</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>Now do the same thing for white, but here the value should be ‘white’. Also print out the first could of rows in the data frame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">white</span><span class="p">[</span><span class="s1">&#39;color&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;white&#39;</span>
<span class="n">white</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>color</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.0</td>
      <td>0.27</td>
      <td>0.36</td>
      <td>20.7</td>
      <td>0.045</td>
      <td>45.0</td>
      <td>170.0</td>
      <td>1.0010</td>
      <td>3.00</td>
      <td>0.45</td>
      <td>8.8</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.3</td>
      <td>0.30</td>
      <td>0.34</td>
      <td>1.6</td>
      <td>0.049</td>
      <td>14.0</td>
      <td>132.0</td>
      <td>0.9940</td>
      <td>3.30</td>
      <td>0.49</td>
      <td>9.5</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.1</td>
      <td>0.28</td>
      <td>0.40</td>
      <td>6.9</td>
      <td>0.050</td>
      <td>30.0</td>
      <td>97.0</td>
      <td>0.9951</td>
      <td>3.26</td>
      <td>0.44</td>
      <td>10.1</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
      <td>white</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we create single data frame for all the wines and declare that colour is factor. pd.concat is a function to bind two data frames row by row. You can also use to concat two data frame column by column with the axis parameter. Check out the specification online. Run <code class="docutils literal notranslate"><span class="pre">wines_df</span> <span class="pre">=</span> <span class="pre">pd.concat([white,</span> <span class="pre">red],</span> <span class="pre">ignore_index=True)</span></code>. With ignore_index=True, we make sure that the new data frame also has its own index.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">white</span><span class="p">,</span> <span class="n">red</span><span class="p">],</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This completes our first step, the data acquisition/collection. It is fairly easy, as we reuse existing material. The data is also complete, and we do not have to take care of any missing values. As described earlier, we want to the machine to learn how to taste good and bad wine. Let’s take a first look at the dataset using .info() first and then head(). Do you know how?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 6497 entries, 0 to 6496
Data columns (total 13 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   fixed acidity         6497 non-null   float64
 1   volatile acidity      6497 non-null   float64
 2   citric acid           6497 non-null   float64
 3   residual sugar        6497 non-null   float64
 4   chlorides             6497 non-null   float64
 5   free sulfur dioxide   6497 non-null   float64
 6   total sulfur dioxide  6497 non-null   float64
 7   density               6497 non-null   float64
 8   pH                    6497 non-null   float64
 9   sulphates             6497 non-null   float64
 10  alcohol               6497 non-null   float64
 11  quality               6497 non-null   int64  
 12  color                 6497 non-null   object 
dtypes: float64(11), int64(1), object(1)
memory usage: 660.0+ KB
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
      <th>color</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.0</td>
      <td>0.27</td>
      <td>0.36</td>
      <td>20.7</td>
      <td>0.045</td>
      <td>45.0</td>
      <td>170.0</td>
      <td>1.0010</td>
      <td>3.00</td>
      <td>0.45</td>
      <td>8.8</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.3</td>
      <td>0.30</td>
      <td>0.34</td>
      <td>1.6</td>
      <td>0.049</td>
      <td>14.0</td>
      <td>132.0</td>
      <td>0.9940</td>
      <td>3.30</td>
      <td>0.49</td>
      <td>9.5</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.1</td>
      <td>0.28</td>
      <td>0.40</td>
      <td>6.9</td>
      <td>0.050</td>
      <td>30.0</td>
      <td>97.0</td>
      <td>0.9951</td>
      <td>3.26</td>
      <td>0.44</td>
      <td>10.1</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
      <td>white</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
      <td>6</td>
      <td>white</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>There is a column called quality, which matches our classification task. We will use this column as the classification ‘target’. Quality is an ordinal feature from 1 to 9 with 9 indicating top quality. Now, let’s see how quality values are distributed. We could simply run table to get the frequencies for each quality class, but we decide to plot the classes with <code class="docutils literal notranslate"><span class="pre">sns.histplot(data=wines_df,</span> <span class="pre">x='quality',</span> <span class="pre">binwidth=2)</span></code>. With binwidth=2, we include two classifications in each bin.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">wines_df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;quality&#39;</span><span class="p">,</span> <span class="n">binwidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;quality&#39;, ylabel=&#39;Count&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/Predicting_Modelling_1_18_1.png" src="../../_images/Predicting_Modelling_1_18_1.png" />
</div>
</div>
<p>In order to make our life a little easier, we now would like to reduce the 9 quality classes to 2 (good or bad). This is also part of the original example. Remember the magic of np.where? Then, you know that it is easy with <code class="docutils literal notranslate"><span class="pre">wines_df['quality']</span> <span class="pre">=</span> <span class="pre">np.where(wines_df['quality']</span> <span class="pre">&lt;</span> <span class="pre">6,</span> <span class="pre">'bad',</span> <span class="pre">'good')</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">wines_df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;bad&#39;</span><span class="p">,</span> <span class="s1">&#39;good&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We have overwritten the original quality column with a new quality integer with 2 levels. Let’s see how this is distributed with value_counts(). Run <code class="docutils literal notranslate"><span class="pre">wines_df['quality'].value_counts()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>good    4113
bad     2384
Name: quality, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Unfortunately, we now have many more ‘good’ quality wine observations, which might be a problem later when we start training a model. Why do you think this might be the case? But it can’t be helped and we go on analysing.</p>
<p>It’s time to prepare our data for its machine learning adventures.</p>
</div>
<div class="section" id="prepare-data">
<h3>Prepare Data<a class="headerlink" href="#prepare-data" title="Permalink to this headline">¶</a></h3>
<p>The next step is very important for many machine learning algorithms based on feature spaces. We need to standardize the features, as the distances in the space are dependent on how the features are measured. In particular, if certain features have much larger values than others, the distance measurements will be strongly dominated by the larger values. This wasn’t a problem for us before with the food data, as both sweetness and crunchiness were measured on a scale from 1 to 10. But suppose we added another measure on a scale from 1 to 1,000,000. This measure would dwarf the contribution of the other scales. The distances in the feature space would get out of scale.</p>
<p>We only need to normalize numeric data. Looking back at the results from str(wines_df), columns/features 1 to 11 are numeric. Next we, define a function to normalise these so that they are all on a scale between 0 and 1. We use the so-called min-max normalisation. Consider an example, where the residual sugar of wine is say 50, while we want to transform this to the range 0 to 1. So first we find the maximum value of residual sugar which is in our example 100 and the minimum value of residual sugar, say 20, then the new scaled value for will be: (50-20)/(100-20)=0.375. Can you see why this value is guaranteed to be between 0 and 1?</p>
<p>Let’s define a function that takes care of the normalization for us. You hopefully remember how you can define your functions in Python? Anyway, just look at the next cell and run it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>

<span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we apply normalize to all the numeric columns in wines_df. There is a wonderful function in Pandas to do this. Type in <code class="docutils literal notranslate"><span class="pre">wines_numeric_df</span> <span class="pre">=</span> <span class="pre">wines_df.select_dtypes(include='number')</span></code> and print out the first couple of rows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get all numeric columns</span>
<span class="n">wines_numeric_df</span> <span class="o">=</span> <span class="n">wines_df</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="s1">&#39;number&#39;</span><span class="p">)</span>
<span class="n">wines_numeric_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>7.0</td>
      <td>0.27</td>
      <td>0.36</td>
      <td>20.7</td>
      <td>0.045</td>
      <td>45.0</td>
      <td>170.0</td>
      <td>1.0010</td>
      <td>3.00</td>
      <td>0.45</td>
      <td>8.8</td>
    </tr>
    <tr>
      <th>1</th>
      <td>6.3</td>
      <td>0.30</td>
      <td>0.34</td>
      <td>1.6</td>
      <td>0.049</td>
      <td>14.0</td>
      <td>132.0</td>
      <td>0.9940</td>
      <td>3.30</td>
      <td>0.49</td>
      <td>9.5</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.1</td>
      <td>0.28</td>
      <td>0.40</td>
      <td>6.9</td>
      <td>0.050</td>
      <td>30.0</td>
      <td>97.0</td>
      <td>0.9951</td>
      <td>3.26</td>
      <td>0.44</td>
      <td>10.1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
    </tr>
    <tr>
      <th>4</th>
      <td>7.2</td>
      <td>0.23</td>
      <td>0.32</td>
      <td>8.5</td>
      <td>0.058</td>
      <td>47.0</td>
      <td>186.0</td>
      <td>0.9956</td>
      <td>3.19</td>
      <td>0.40</td>
      <td>9.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Apply is a powerful function in Pandas that let’s you apply a function across seceral columns. Run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wines_normalized_df</span> <span class="o">=</span> <span class="n">wines_numeric_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span>
<span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_normalized_df</span> <span class="o">=</span> <span class="n">wines_numeric_df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">normalize</span><span class="p">)</span>
<span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.264463</td>
      <td>0.126667</td>
      <td>0.216867</td>
      <td>0.308282</td>
      <td>0.059801</td>
      <td>0.152778</td>
      <td>0.377880</td>
      <td>0.267785</td>
      <td>0.217054</td>
      <td>0.129213</td>
      <td>0.115942</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.206612</td>
      <td>0.146667</td>
      <td>0.204819</td>
      <td>0.015337</td>
      <td>0.066445</td>
      <td>0.045139</td>
      <td>0.290323</td>
      <td>0.132832</td>
      <td>0.449612</td>
      <td>0.151685</td>
      <td>0.217391</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.355372</td>
      <td>0.133333</td>
      <td>0.240964</td>
      <td>0.096626</td>
      <td>0.068106</td>
      <td>0.100694</td>
      <td>0.209677</td>
      <td>0.154039</td>
      <td>0.418605</td>
      <td>0.123596</td>
      <td>0.304348</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.280992</td>
      <td>0.100000</td>
      <td>0.192771</td>
      <td>0.121166</td>
      <td>0.081395</td>
      <td>0.159722</td>
      <td>0.414747</td>
      <td>0.163678</td>
      <td>0.364341</td>
      <td>0.101124</td>
      <td>0.275362</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.280992</td>
      <td>0.100000</td>
      <td>0.192771</td>
      <td>0.121166</td>
      <td>0.081395</td>
      <td>0.159722</td>
      <td>0.414747</td>
      <td>0.163678</td>
      <td>0.364341</td>
      <td>0.101124</td>
      <td>0.275362</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Finally, let’s add the quality column to the new normalized data frame. This time it only contains good and bad. Type in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wines_normalized_df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">wines_df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span>
<span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_normalized_df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">wines_df</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span>
<span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
      <th>quality</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.264463</td>
      <td>0.126667</td>
      <td>0.216867</td>
      <td>0.308282</td>
      <td>0.059801</td>
      <td>0.152778</td>
      <td>0.377880</td>
      <td>0.267785</td>
      <td>0.217054</td>
      <td>0.129213</td>
      <td>0.115942</td>
      <td>good</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.206612</td>
      <td>0.146667</td>
      <td>0.204819</td>
      <td>0.015337</td>
      <td>0.066445</td>
      <td>0.045139</td>
      <td>0.290323</td>
      <td>0.132832</td>
      <td>0.449612</td>
      <td>0.151685</td>
      <td>0.217391</td>
      <td>good</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.355372</td>
      <td>0.133333</td>
      <td>0.240964</td>
      <td>0.096626</td>
      <td>0.068106</td>
      <td>0.100694</td>
      <td>0.209677</td>
      <td>0.154039</td>
      <td>0.418605</td>
      <td>0.123596</td>
      <td>0.304348</td>
      <td>good</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.280992</td>
      <td>0.100000</td>
      <td>0.192771</td>
      <td>0.121166</td>
      <td>0.081395</td>
      <td>0.159722</td>
      <td>0.414747</td>
      <td>0.163678</td>
      <td>0.364341</td>
      <td>0.101124</td>
      <td>0.275362</td>
      <td>good</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.280992</td>
      <td>0.100000</td>
      <td>0.192771</td>
      <td>0.121166</td>
      <td>0.081395</td>
      <td>0.159722</td>
      <td>0.414747</td>
      <td>0.163678</td>
      <td>0.364341</td>
      <td>0.101124</td>
      <td>0.275362</td>
      <td>good</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We are now satisfied with the data, done our cleaning and all preparations. We can start the modelling process in order to predict how a wine will taste. But first we want to save the data after all the hard work. In Pandas, this means we pickle it: <a class="reference external" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html">https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html</a>. Run <code class="docutils literal notranslate"><span class="pre">wines_normalized_df.to_pickle('data/wines_normalized_df.pkl')</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="s1">&#39;data/wines_normalized_df.pkl&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="define-the-training-data">
<h3>Define the Training Data<a class="headerlink" href="#define-the-training-data" title="Permalink to this headline">¶</a></h3>
<p>From info() and value_counts(), we know that we have 6,497 wine quality observations with 2,384 labelled bad and 4,113 labelled good. Because we aim to predict new things, our next step should be to find out about things we do not already know and how the model would be able to predict unknown data. If we had access to more wine data, we could apply our model to unknown wine observations and see how well the predictions compare to new wines. But we cannot know about data we do not have. So, we simulate such a scenario by dividing our data into a training dataset that will be used to build the model and a test dataset. We will use the test dataset to simulate the prediction and find out how well our model behaves.</p>
<p>We will use 75% of our data for the training and 25% for testing. First, we randomly mix the data to ensure that all qualities are evenly distributed in both training and test data. Pandas has a function for that called sample. Create the training data with <code class="docutils literal notranslate"><span class="pre">train_set</span> <span class="pre">=</span> <span class="pre">wines_normalized_df.sample(frac=0.75)</span></code>. frac is the fraction of data to be included in the training data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_set</span> <span class="o">=</span> <span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We also need a test dataset. This will be simply the rest. So, we ‘drop’ everything from wines_normalized_df that is included in the training dataset. Run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">test_set</span> <span class="o">=</span> <span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">train_set</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">test_set</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_set</span> <span class="o">=</span> <span class="n">wines_normalized_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">train_set</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">test_set</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 1624 entries, 3 to 6495
Data columns (total 12 columns):
 #   Column                Non-Null Count  Dtype  
---  ------                --------------  -----  
 0   fixed acidity         1624 non-null   float64
 1   volatile acidity      1624 non-null   float64
 2   citric acid           1624 non-null   float64
 3   residual sugar        1624 non-null   float64
 4   chlorides             1624 non-null   float64
 5   free sulfur dioxide   1624 non-null   float64
 6   total sulfur dioxide  1624 non-null   float64
 7   density               1624 non-null   float64
 8   pH                    1624 non-null   float64
 9   sulphates             1624 non-null   float64
 10  alcohol               1624 non-null   float64
 11  quality               1624 non-null   object 
dtypes: float64(11), object(1)
memory usage: 164.9+ KB
</pre></div>
</div>
</div>
</div>
<p>We are ready to model and because things are looking good with go directly to one the most advanced machine learning technique that uses the human brain as an inspiration. Neural Networks have become synonymous with the recent success of artificial intelligence.</p>
</div>
</div>
<div class="section" id="modelling-and-predicting">
<h2>Modelling and Predicting<a class="headerlink" href="#modelling-and-predicting" title="Permalink to this headline">¶</a></h2>
<div class="section" id="background-2-neural-networks">
<h3>Background 2: Neural Networks<a class="headerlink" href="#background-2-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>With the training data, we are ready to start learning a model for tasting wine. To classify our test instance, we will work with the best that current machine learning has to offer. We employ the help of neural networks, machines assembled in similar ways to the hundreds, thousands or millions of brain cells. Kant would be proud of us – maybe.
<img alt="alt text" src="../../_images/neural-networks-1.png" /></p>
<p>Each neuron is made up of a cell body with a number of connections coming off it. These are numerous dendrites (carrying information toward the cell body) and a single axon (carrying information away). But computers are not alive. They are mechanical boxes and made not of the complex chains of brain cells, which are densely interconnected in complex and parallel ways - each one connected to perhaps 10,000 other brain cells. Computers are designed to store lots of data and rearrange that – as we have done many times and need instructions for that. To the day, we do not fully understand how brains learn. They can spontaneously put information together in astounding new ways and forge new connections. No computer currently comes close to that.</p>
<p>The basic idea behind a neural network is to simulate those densely interconnected brain cells inside a computer so you can get it to learn things, recognize patterns and make decisions. Neural networks learn to improve their own analysis of the data. But neural networks remain mathematical equations and mean nothing to the computers themselves – unlike our own brain activities. They are still just highly interconnected numbers in boxes who constantly change.</p>
<p>A typical neural network has anything from a few dozen to hundreds, thousands, or even millions of artificial neurons called units arranged in a series of layers, each of which connects to the layers on either side. Some of them are input units. In our case, these will be defined by the data for each feature in each observation. Each feature forms one input unit. Neural networks also have an output layer that responds to the information that is learned. In our case, these are the quality judgments we make with regard to the wines.
<img alt="alt text" src="../../_images/neural-networks-2.png" /></p>
<p>In-between the input units and output units are one or more layers of hidden units, which together form the majority of the artificial brain. The connections between one unit and another are represented by a number called a weight, which can be either positive (if one unit excites another) or negative (if one unit suppresses or inhibits another). The higher the weight, the more influence one unit has on another. Inputs are fed in from the left, activate the hidden units in the middle and feed out outputs from the right.</p>
<p>But information flows backwards from the output units, too. For a neural network to learn, there has to be an element of feedback involved – just like we humans learn. With feedback, we compare what we tried to achieve with what we actually achieved and adjust our behaviour accordingly. Neural networks learn things in exactly the same way with a feedback process called backpropagation. Because we know from the training data the output we tried to achieve, we can compare it with the calculated values and modify the connections in the network to improve the outcome, working from the output units through the hidden units to the input units. Over time, this backpropagation causes the network to learn until a stable state is achieved. In our case, the network will learn how we taste wine.</p>
</div>
<div class="section" id="id1">
<h3>Modelling and Predicting<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Fortunately for us, we do not have to implement Neural Networks by ourselves but can rely on many existing algorithms in Python.</p>
<p>One of the most common machine learning libraries in Python used is scikit-learn: <a class="reference external" href="https://scikit-learn.org/">https://scikit-learn.org/</a>. We will use scikit’s MLP-classifier, a Multi-layer Perceptron classifier. Perceptron are basically these little mathematical entities that are supposed to simulate the human brain cells: <a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">https://en.wikipedia.org/wiki/Perceptron</a>. They can be multi-layered to form very complex networks. Run the next cell to load the necessary libraries.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>

<span class="kn">import</span> <span class="nn">sklearn</span> <span class="k">as</span> <span class="nn">sk</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
</pre></div>
</div>
</div>
</div>
<p>We first divide our train and test set into predictor and target variables.  The next cell is very typical to all machine learning work. For each of the two training and test datasets, it creates the part that is everything except the target variable (X) and the target variable (y). Run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">train_set</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train_set</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_set</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">test_set</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_set</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
<p>Remember that values transforms a Pandas series into an array. The rest of the syntax is hopefully clear?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">train_set</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">train_set</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_set</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_set</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">test_set</span><span class="o">.</span><span class="n">columns</span> <span class="o">!=</span> <span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_set</span><span class="p">[</span><span class="s1">&#39;quality&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<p>And then build the classifier. <a class="reference external" href="https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/">https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/</a> has nice instructions. Type in <code class="docutils literal notranslate"><span class="pre">classifier</span> <span class="pre">=</span> <span class="pre">MLPClassifier(hidden_layer_sizes=(20,10),</span> <span class="pre">max_iter=300)</span></code>, which defines an MLP with two hidden layers of 20 and then 10 nodes. max_iter=300 tells the modelling to stop after 300 iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In the Python machine learning pipeline, we now need to ‘fit’ the model with the training data. Run <code class="docutils literal notranslate"><span class="pre">model.fit(X_train,</span> <span class="pre">Y_train)</span></code> to do so.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn&#39;t converged yet.
  warnings.warn(
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MLPClassifier(hidden_layer_sizes=(20, 10), max_iter=300)
</pre></div>
</div>
</div>
</div>
<p>Run the next two cells to get an insight into how the model is consituted. The first cell prints out some of the weights attached to the neuron links and the second visualises the whole network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weights between input and first hidden layer:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Weights between first hidden and second hidden layer:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coefs_</span><span class="p">[</span><span class="mi">1</span><span class="p">][:</span><span class="mi">3</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Weights between input and first hidden layer:
[[-9.42277038e-001  2.44114680e-001  7.86876633e-002  3.48832363e-001
   8.48237170e-002  3.67171846e-001 -8.88379168e-002 -3.81984195e-002
  -2.96493960e-001  1.30767039e-001  3.67692060e-001  2.67741877e-122
   3.55696118e-001  3.55110071e-162 -1.64774103e-001  3.49616707e-002
   5.08166358e-001  8.15271141e-002  4.28183535e-001 -1.01785030e-140]
 [ 5.46716584e-001 -1.18378977e+000  8.83685084e-003  5.85507247e-003
  -2.35257753e-002 -1.55056268e-001 -5.83034987e-001 -3.34624539e-001
   1.80311257e-001  2.14130675e-001  4.63325885e-001 -2.25506925e-157
   4.47645811e-001  5.10244403e-131 -1.03049699e+000 -4.79946272e-001
   2.68894995e-001  6.45769007e-001  7.20636299e-002  3.72557671e-172]
 [-3.31137980e-001 -5.68388215e-001 -1.21761433e-001  1.00523849e-001
  -5.37913832e-001  4.12752616e-001 -1.50606682e-001 -1.78332686e-001
  -7.41072954e-001 -4.90921149e-001  1.31472581e-001  4.83052128e-145
  -4.09703259e-001  1.42533292e-162 -3.03464632e-003 -2.87125538e-001
   2.72545983e-001 -6.15074431e-002 -2.39942613e-002 -1.03289752e-148]]

Weights between first hidden and second hidden layer:
[[-7.52027592e-001  9.49812291e-002  1.26172610e+000 -7.59586927e-001
  -3.65448350e-001  5.80019893e-001  1.17790436e-002 -3.71434353e-165
  -2.18974051e+000  1.01597095e+000]
 [ 1.80173277e+000  1.60105698e+000 -1.45915391e+000  1.39560177e+000
   1.68063721e+000  3.18076018e-001 -1.31055362e-003 -5.95919175e-137
   2.30567844e+000 -9.38231078e-001]
 [ 9.06964140e-001  2.32028815e-001 -6.06152220e-001  9.67365038e-001
   6.97286797e-001 -4.63272113e-001  8.44724483e-002 -9.11021751e-143
  -4.24279428e-001  5.01073464e-001]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>

<span class="kn">import</span> <span class="nn">VisualizeNN</span> <span class="k">as</span> <span class="nn">VisNN</span>

<span class="n">input_features</span> <span class="o">=</span> <span class="mi">11</span>
<span class="n">output_features</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">network_structure</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="n">input_features</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">hidden_layer_sizes</span><span class="p">),</span> <span class="p">[</span><span class="n">output_features</span><span class="p">]))</span>

<span class="n">network</span><span class="o">=</span><span class="n">VisNN</span><span class="o">.</span><span class="n">DrawNN</span><span class="p">(</span><span class="n">network_structure</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">coefs_</span><span class="p">)</span>
<span class="n">network</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Predicting_Modelling_1_51_0.png" src="../../_images/Predicting_Modelling_1_51_0.png" />
</div>
</div>
<p>The blue connections are negative weights and the orange ones are positive.</p>
</div>
<div class="section" id="predicting">
<h3>Predicting<a class="headerlink" href="#predicting" title="Permalink to this headline">¶</a></h3>
<p>Next, we can start predicting unknown behaviour, which - as said - we simulate with the test dataset. Run <code class="docutils literal notranslate"><span class="pre">y_pred</span> <span class="pre">=</span> <span class="pre">model.predict(X_test)</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check out the details of our predictions and compare predictions with test data using scikit’s confusion matrix function which prints the true positives, false negatives, false positives an true negatives consequently. Don’t know what those are? Check out: <a class="reference external" href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives">https://en.wikipedia.org/wiki/False_positives_and_false_negatives</a>.</p>
<p>Run the next cell.</p>
</div>
</div>
<div class="section" id="evaluate-model">
<h2>Evaluate Model<a class="headerlink" href="#evaluate-model" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[411, 172],
       [184, 857]])
</pre></div>
</div>
</div>
</div>
<p>Check <a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_matrix">https://en.wikipedia.org/wiki/Confusion_matrix</a> for how to read this matrix but also don’t worry, we will make a nicer representation. For the moment, we are only interested in the overall performance by looking at the accuracy of our prediction.</p>
<p>Accuracy is defined as the number of times our predictions have been correct compared to the overall number of predictions. So, we take all cases where the predictions where right in the above table (bad-bad and good-good) and compare these with the overall number of observations in the test data or len(test_set). Please replace in the calculation below the numbers you have got.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">355</span> <span class="o">+</span> <span class="mi">875</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.7573891625615764
</pre></div>
</div>
</div>
</div>
<p>~75% of our predictions are correct. Please, note that the exact number can be either a bit higher or lower depending on the random test and training datasets.</p>
<p>Not bad – especially considering that most wine experts would probably not be able to agree to such a degree. However, we would of course like to improve on our predictions. So, let’s investigate this further.</p>
<p>But first let’s make this better. Run the next cell for a visualisation of the confusion matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">plot_confusion_matrix</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x12b2d5190&gt;
</pre></div>
</div>
<img alt="../../_images/Predicting_Modelling_1_60_1.png" src="../../_images/Predicting_Modelling_1_60_1.png" />
</div>
</div>
<p>From the matrix, we can see that the machine is much better at predicting good quality wine rather than bad one. This should not be surprising, since we already know that we do not have enough training data for bad wine.</p>
<div class="section" id="interpret-the-results">
<h3>Interpret the Results<a class="headerlink" href="#interpret-the-results" title="Permalink to this headline">¶</a></h3>
<p>In order to further interpret the model, a good approach is to understand which features have influenced the models behaviour and which features are redundant because the results they support are supported by other features. This way we get closer to the secret why people like certain wines. Let’s find out first which features influence the quality decisions most.</p>
<p>Model interpretation is a big research topics at the moment (<a class="reference external" href="https://medium.com/analytics-vidhya/why-should-i-trust-your-model-bdda6be94c6f">https://medium.com/analytics-vidhya/why-should-i-trust-your-model-bdda6be94c6f</a>). It is a whole new research area called trusted/explainable AI. We will use an algorithm called PermutationImportance, which checks how important a feature is based on permutating the order of the features. ELI5 stands for the phrase: Explain Like I’m 5. It is also one of the standard Python to explain models.</p>
<p>Run the cell below to load the library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>

<span class="kn">import</span> <span class="nn">eli5</span>
<span class="kn">from</span> <span class="nn">eli5.sklearn</span> <span class="kn">import</span> <span class="n">PermutationImportance</span>
</pre></div>
</div>
</div>
</div>
<p>In order to print out the table of feature importance, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">perm</span> <span class="o">=</span> <span class="n">PermutationImportance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">eli5</span><span class="o">.</span><span class="n">show_weights</span><span class="p">(</span><span class="n">perm</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">wines_numeric_df</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">perm</span> <span class="o">=</span> <span class="n">PermutationImportance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">eli5</span><span class="o">.</span><span class="n">show_weights</span><span class="p">(</span><span class="n">perm</span><span class="p">,</span> <span class="n">feature_names</span> <span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">wines_numeric_df</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <style>
    table.eli5-weights tr:hover {
        filter: brightness(85%);
    }
</style>






































        <table class="eli5-weights eli5-feature-importances" style="border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;">
    <thead>
    <tr style="border: none;">
        <th style="padding: 0 1em 0 0.5em; text-align: right; border: none;">Weight</th>
        <th style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">Feature</th>
    </tr>
    </thead>
    <tbody>

        <tr style="background-color: hsl(120, 100.00%, 80.00%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.1084

                    &plusmn; 0.0148

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                alcohol
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 83.65%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0813

                    &plusmn; 0.0105

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                volatile acidity
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 88.75%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0477

                    &plusmn; 0.0140

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                total sulfur dioxide
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 91.21%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0335

                    &plusmn; 0.0051

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                sulphates
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 91.28%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0331

                    &plusmn; 0.0095

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                free sulfur dioxide
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 91.83%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0302

                    &plusmn; 0.0098

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                residual sugar
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 94.12%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0188

                    &plusmn; 0.0102

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                density
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 95.64%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0123

                    &plusmn; 0.0054

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                pH
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 96.74%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0081

                    &plusmn; 0.0042

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                fixed acidity
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 96.98%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0073

                    &plusmn; 0.0057

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                citric acid
            </td>
        </tr>

        <tr style="background-color: hsl(120, 100.00%, 97.87%); border: none;">
            <td style="padding: 0 1em 0 0.5em; text-align: right; border: none;">
                0.0044

                    &plusmn; 0.0009

            </td>
            <td style="padding: 0 0.5em 0 0.5em; text-align: left; border: none;">
                chlorides
            </td>
        </tr>


    </tbody>
</table>



















</div></div>
</div>
<p>To plot this output, we need to transform it into a data frame first. We can do this in one line. Run and understand the cell below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">wines_numeric_df</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">perm</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="s1">&#39;Importance&#39;</span><span class="p">])</span>
<span class="n">features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature</th>
      <th>Importance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>fixed acidity</td>
      <td>0.008128</td>
    </tr>
    <tr>
      <th>1</th>
      <td>volatile acidity</td>
      <td>0.081281</td>
    </tr>
    <tr>
      <th>2</th>
      <td>citric acid</td>
      <td>0.007266</td>
    </tr>
    <tr>
      <th>3</th>
      <td>residual sugar</td>
      <td>0.030172</td>
    </tr>
    <tr>
      <th>4</th>
      <td>chlorides</td>
      <td>0.004433</td>
    </tr>
    <tr>
      <th>5</th>
      <td>free sulfur dioxide</td>
      <td>0.033128</td>
    </tr>
    <tr>
      <th>6</th>
      <td>total sulfur dioxide</td>
      <td>0.047660</td>
    </tr>
    <tr>
      <th>7</th>
      <td>density</td>
      <td>0.018842</td>
    </tr>
    <tr>
      <th>8</th>
      <td>pH</td>
      <td>0.012315</td>
    </tr>
    <tr>
      <th>9</th>
      <td>sulphates</td>
      <td>0.033498</td>
    </tr>
    <tr>
      <th>10</th>
      <td>alcohol</td>
      <td>0.108374</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>With Pandas’ sort_values(), you can order features according to the their importance. Can you find out how? Tip: use ascending=False and inplace = True as parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s1">&#39;Importance&#39;</span><span class="p">],</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can plot features with <code class="docutils literal notranslate"><span class="pre">sns.barplot(x='Importance',</span> <span class="pre">y='Feature',</span> <span class="pre">data=features)</span></code>.
This will be a horizontal plot as we set x to the numerical variable and y to the categorical one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Importance&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Feature&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;Importance&#39;, ylabel=&#39;Feature&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/Predicting_Modelling_1_70_1.png" src="../../_images/Predicting_Modelling_1_70_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="feature-engineering">
<h2>Feature engineering<a class="headerlink" href="#feature-engineering" title="Permalink to this headline">¶</a></h2>
<p>Feature engineering is a big topic, which we skipped over a bit. According to <a class="reference external" href="https://en.wikipedia.org/wiki/Feature_engineering">https://en.wikipedia.org/wiki/Feature_engineering</a>, it is the ‘process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.’ For example, we could reduce the features, as not all of them are equally important according to our feature  plot. Or, in order to check for redundant features that we do not need for the prediction, we can use correlation for numerical features. Remember that a correlation indicates the extent to which two or more features fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel. The higher the correlation between variables therefore the easier it will be to use just one of them, as the others do not influence the overall outcome. Correlation can also be an issue for the use of explainability tools, according to <a class="reference external" href="https://towardsdatascience.com/stop-permuting-features-c1412e31b63f">https://towardsdatascience.com/stop-permuting-features-c1412e31b63f</a>.</p>
<p>We will plot a heatmap of the correlations using seaborn. The correlation coefficient has values between -1 to 1. A value closer to 0 implies weaker correlation (exact 0 implying no correlation). A value closer to 1 implies stronger positive correlation. A value closer to -1 implies stronger negative correlation.</p>
<p>Run <code class="docutils literal notranslate"><span class="pre">cor</span> <span class="pre">=</span> <span class="pre">train_set.corr()</span></code> to determine the correlations. Also print out cor …</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cor</span> <span class="o">=</span> <span class="n">train_set</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">cor</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fixed acidity</th>
      <th>volatile acidity</th>
      <th>citric acid</th>
      <th>residual sugar</th>
      <th>chlorides</th>
      <th>free sulfur dioxide</th>
      <th>total sulfur dioxide</th>
      <th>density</th>
      <th>pH</th>
      <th>sulphates</th>
      <th>alcohol</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>fixed acidity</th>
      <td>1.000000</td>
      <td>0.218096</td>
      <td>0.335194</td>
      <td>-0.113703</td>
      <td>0.290381</td>
      <td>-0.280918</td>
      <td>-0.328383</td>
      <td>0.450959</td>
      <td>-0.255762</td>
      <td>0.293915</td>
      <td>-0.088182</td>
    </tr>
    <tr>
      <th>volatile acidity</th>
      <td>0.218096</td>
      <td>1.000000</td>
      <td>-0.378784</td>
      <td>-0.198895</td>
      <td>0.375398</td>
      <td>-0.354714</td>
      <td>-0.417297</td>
      <td>0.264579</td>
      <td>0.257097</td>
      <td>0.232015</td>
      <td>-0.030796</td>
    </tr>
    <tr>
      <th>citric acid</th>
      <td>0.335194</td>
      <td>-0.378784</td>
      <td>1.000000</td>
      <td>0.133504</td>
      <td>0.047171</td>
      <td>0.134693</td>
      <td>0.194409</td>
      <td>0.102607</td>
      <td>-0.337214</td>
      <td>0.049447</td>
      <td>-0.020972</td>
    </tr>
    <tr>
      <th>residual sugar</th>
      <td>-0.113703</td>
      <td>-0.198895</td>
      <td>0.133504</td>
      <td>1.000000</td>
      <td>-0.131175</td>
      <td>0.404606</td>
      <td>0.488792</td>
      <td>0.558118</td>
      <td>-0.255361</td>
      <td>-0.182162</td>
      <td>-0.358213</td>
    </tr>
    <tr>
      <th>chlorides</th>
      <td>0.290381</td>
      <td>0.375398</td>
      <td>0.047171</td>
      <td>-0.131175</td>
      <td>1.000000</td>
      <td>-0.193111</td>
      <td>-0.270393</td>
      <td>0.351522</td>
      <td>0.035402</td>
      <td>0.399610</td>
      <td>-0.251057</td>
    </tr>
    <tr>
      <th>free sulfur dioxide</th>
      <td>-0.280918</td>
      <td>-0.354714</td>
      <td>0.134693</td>
      <td>0.404606</td>
      <td>-0.193111</td>
      <td>1.000000</td>
      <td>0.726000</td>
      <td>0.029048</td>
      <td>-0.156644</td>
      <td>-0.181837</td>
      <td>-0.188197</td>
    </tr>
    <tr>
      <th>total sulfur dioxide</th>
      <td>-0.328383</td>
      <td>-0.417297</td>
      <td>0.194409</td>
      <td>0.488792</td>
      <td>-0.270393</td>
      <td>0.726000</td>
      <td>1.000000</td>
      <td>0.032429</td>
      <td>-0.239324</td>
      <td>-0.267693</td>
      <td>-0.274701</td>
    </tr>
    <tr>
      <th>density</th>
      <td>0.450959</td>
      <td>0.264579</td>
      <td>0.102607</td>
      <td>0.558118</td>
      <td>0.351522</td>
      <td>0.029048</td>
      <td>0.032429</td>
      <td>1.000000</td>
      <td>0.017764</td>
      <td>0.258349</td>
      <td>-0.680472</td>
    </tr>
    <tr>
      <th>pH</th>
      <td>-0.255762</td>
      <td>0.257097</td>
      <td>-0.337214</td>
      <td>-0.255361</td>
      <td>0.035402</td>
      <td>-0.156644</td>
      <td>-0.239324</td>
      <td>0.017764</td>
      <td>1.000000</td>
      <td>0.176837</td>
      <td>0.116070</td>
    </tr>
    <tr>
      <th>sulphates</th>
      <td>0.293915</td>
      <td>0.232015</td>
      <td>0.049447</td>
      <td>-0.182162</td>
      <td>0.399610</td>
      <td>-0.181837</td>
      <td>-0.267693</td>
      <td>0.258349</td>
      <td>0.176837</td>
      <td>1.000000</td>
      <td>-0.004784</td>
    </tr>
    <tr>
      <th>alcohol</th>
      <td>-0.088182</td>
      <td>-0.030796</td>
      <td>-0.020972</td>
      <td>-0.358213</td>
      <td>-0.251057</td>
      <td>-0.188197</td>
      <td>-0.274701</td>
      <td>-0.680472</td>
      <td>0.116070</td>
      <td>-0.004784</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The table shows how two features are correlated. At the moment, nothing really stands out, but let’s plot the table as a heatmap to get a better overview.</p>
<p>Run the cell below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Keep cell</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cor</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Reds</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Predicting_Modelling_1_74_0.png" src="../../_images/Predicting_Modelling_1_74_0.png" />
</div>
</div>
<p>corr() delivers the correlations between all features in train_set and sns.heatmap plots them. In the plot, you can clearly see that the two sulfur.dioxide measures are correlated. A negative correlation indicates the extent to which one variable increases as the other decreases.</p>
<p>The next step would be to try and improve the model performance. We could, for instance, make the neural network more complex or change the normalisation. The possibilities are literally endless. This kind of work is what keeps an analyst really occupied. In our case this might be difficult though as we do not have enough data on bad wines. We could try and get more data and organise another tasting competition, but going to Portugal is expensive. We rather look at a recent innovation of the neural network called ‘deep learning’ next. Deep learning is essentially a way to learn much more complex neural network architectures, more layers of hidden neurons and more complex connections.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Notebooks/PredictingModelling"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By DIMPAH<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>