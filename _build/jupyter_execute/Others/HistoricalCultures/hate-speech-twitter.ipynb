{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ancient-separate",
   "metadata": {},
   "source": [
    "#  Hate Speech Detector\n",
    "\n",
    "In today's session, we learned that in order to detect sentiments we can simply compare freqeuencies of positive and negative words. To this end, we downloaded a dictionary of such terms from the web and then determined their respective frequency. If there are more positive terms in a document than negative ones, we considered it to have a positive sentiment and otherwise a negative one.\n",
    "\n",
    "There are many such dictinonaries produced by linguists but also other communities such as journalists. We can use these with the same approach we used for detecting sentiments to understand texts in different contexts. Journalists, for instance, have developed https://www.hatebase.org/, the world's largest online repository of structured, multilingual, usage-based hate speech. \n",
    "\n",
    "Here, we will use hatebase to develop a hate speech detector for tweets by counting the number of hate words in tweets. We will concentrate on the English language. You can go to https://www.hatebase.org/ and explore the search functions to take a look at the English terms in hatebase. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-while",
   "metadata": {},
   "source": [
    "Next, we need to download the hatebase dictionary, which is unfortunately not that easy. You need to register for an API key and then work relatively hard to get the API to return all English hate speech terms. \n",
    "\n",
    "I have commented out the hate_vocabulary(api_key) function that speaks to https://www.hatebase.org/ and instead provided you with a direct import from a local CSV file. If you want to, for instance, download the dictionary for another language than English, you need to un-commnent those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprised-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hate_df = pd.read_csv('https://raw.githubusercontent.com/goto4711/social-cultural-analytics/master/hate-vocab-eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-repeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>meaning</th>\n",
       "      <th>offensiveness</th>\n",
       "      <th>number_of_sightings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>abbo</td>\n",
       "      <td>Australian Aboriginal person. Originally, this...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ABC</td>\n",
       "      <td>[1] American-born Chinese [2] Australian-born ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>American-Born Confused Desi, Indian Americans,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>abo</td>\n",
       "      <td>Australian Aboriginal person. Originally, this...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>af</td>\n",
       "      <td>An African, used by white Rhodesians.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  word                                            meaning  \\\n",
       "0           1  abbo  Australian Aboriginal person. Originally, this...   \n",
       "1           2   ABC  [1] American-born Chinese [2] Australian-born ...   \n",
       "2           3  ABCD  American-Born Confused Desi, Indian Americans,...   \n",
       "3           4   abo  Australian Aboriginal person. Originally, this...   \n",
       "4           5    af              An African, used by white Rhodesians.   \n",
       "\n",
       "   offensiveness  number_of_sightings  \n",
       "0            0.0                    0  \n",
       "1            0.0                    0  \n",
       "2            0.0                    0  \n",
       "3            0.0                   37  \n",
       "4            0.0                  461  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-travel",
   "metadata": {},
   "source": [
    "Next we access Twitter the way we learned today. The code is set to a query Twitter about 'Trump' below but is not active. In order to activate it, you need to add your Twitter API details. You can of course also change the search_term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sensitive-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import requests\n",
    "from ipynb.fs.full.keys import *\n",
    "\n",
    "consumer_key = twit_key\n",
    "consumer_secret = twit_secr\n",
    "access_token = twit_token\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "search_term = 'Trump'\n",
    "tweets = api.search(q=search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polish-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ajplus: A woman who says Ghislaine Maxwell recruited her to be abused by Jeffrey Epstein when she was 14 testified Epstein later introd…\n",
      "https://t.co/W5CPB9BqxN\n",
      "RT @OccupyDemocrats: BREAKING: Pres. Biden gets a standing ovation while celebrating artists like Bette Midler and Joni Mitchell at tonight…\n",
      "RT @donwinslow: There are two Democratic parties:\n",
      "\n",
      "One who believes that we're on the edge of losing our Democracy and that this is an all…\n",
      "Random Trump https://t.co/yylmRz2Vno - #randomtrump #trump #donaldtrump #giphy #gif #gifs https://t.co/9sWV0TO2hz\n",
      "RT @KeithOlbermann: Trump has now confessed twice in 36 hours: once by Freudian double-negative slip, and once by boasting he fired the FBI…\n",
      "RT @SteveOAirForce: For all you Trump loving, antivaxxer, fake Christians out there, read this! https://t.co/wOJeHkEAN7\n",
      "House Minority Leader Kevin McCarthy appears to have settled on a strategy to deal with a handful of Republican law… https://t.co/3TbOcodzfV\n",
      "\"Trump admits to obstruction of justice in Fox News interview\" here: https://t.co/zvnnstbgnE \n",
      "To read it on the web… https://t.co/EaguhDLLkI\n",
      "RT @KeithOlbermann: Trump has now confessed twice in 36 hours: once by Freudian double-negative slip, and once by boasting he fired the FBI…\n",
      "@gscucci Joe will respect trumps position as POTUS  even though Joe will never respect trump as a person.\n",
      "RT @MuellerSheWrote: Yes. McConnell voted to confirm Garland. McConnell didn’t participate in the coup and said trump caused it. You know w…\n",
      "Why isn't trump in prison yet?\n",
      "When we warned he would incite violence and death they said we were overreacting. \n",
      "When we warned women's rights wo… https://t.co/PAzZC01aQ5\n",
      "RT @donwinslow: Mark Meadows head is so far up Donald Trump's ______ that he had to go to the post office and file a change of address.\n",
      "\n",
      "He…\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-south",
   "metadata": {},
   "source": [
    "Rather than performing a live Twitter search, I have saved the a search on 'Trump' from the day of his 2017 inauguration. That's the read.csv command further down. Please, note that the file was created with the old twitteR library, which means that some of the column names are different from what you are used to. But the one we are interested in is still called 'text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alone-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"https://raw.githubusercontent.com/goto4711/social-cultural-analytics/master/trump-tweets-20-1.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-upper",
   "metadata": {},
   "source": [
    "Let's take a look at tweets. You will see all texts as well as a lot of other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "northern-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @ChrisTran1997: Wanting Trump to fail makes...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789251489794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>brrriiieee</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @TuhafAmaGercek: Donald Trump, ABD BaÅkanl...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789234659328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>sailorreihino</td>\n",
       "      <td>282</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>And you losers on the left continue to wonder ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789230518274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>DavidYDG</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @rogerwilko: #Trump speech is like the nati...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789209554946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>samella_donavan</td>\n",
       "      <td>384</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>KPHO Phoenix Devotes 24 Hours to Trump's Impac...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789196881925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>bcbeat</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @ChrisTran1997: Wanting Trump to fail makes...      False   \n",
       "1           2  RT @TuhafAmaGercek: Donald Trump, ABD BaÅkanl...      False   \n",
       "2           3  And you losers on the left continue to wonder ...      False   \n",
       "3           4  RT @rogerwilko: #Trump speech is like the nati...      False   \n",
       "4           5  KPHO Phoenix Devotes 24 Hours to Trump's Impac...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "1              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "2              0       NaN  2017-01-20 19:58:31       True         NaN   \n",
       "3              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "4              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "\n",
       "                   id  replyToUID  \\\n",
       "0  822533789251489794         NaN   \n",
       "1  822533789234659328         NaN   \n",
       "2  822533789230518274         NaN   \n",
       "3  822533789209554946         NaN   \n",
       "4  822533789196881925         NaN   \n",
       "\n",
       "                                        statusSource       screenName  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...       brrriiieee   \n",
       "1  <a href=\"http://twitter.com/#!/download/ipad\" ...    sailorreihino   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...         DavidYDG   \n",
       "3  <a href=\"http://twitter.com/download/android\" ...  samella_donavan   \n",
       "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...           bcbeat   \n",
       "\n",
       "   retweetCount  isRetweet  retweeted  longitude  latitude  \n",
       "0             7       True      False        NaN       NaN  \n",
       "1           282       True      False        NaN       NaN  \n",
       "2             0      False      False        NaN       NaN  \n",
       "3           384       True      False        NaN       NaN  \n",
       "4             0      False      False        NaN       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-pennsylvania",
   "metadata": {},
   "source": [
    "Next we start with our standard text mining workflow \n",
    "\n",
    "Unfortunately, tweets can be difficult to process, as people use very different types of language, of formatting, etc. I have therefore provided you with a clean_tweets function, which applies to all the texts in the tweets and save the results in a tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noticed-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    text = df['text']\n",
    "    tweet_list = []\n",
    "    for tweet in text:\n",
    "        tweet = tweet.split()\n",
    "        tweet = [\"\" if len(word) < 3 else word for word in tweet]\n",
    "        tweet_list.append(tweet)\n",
    "    return tweet_list\n",
    "    \n",
    "tweet_list = clean_tweets(tweets)\n",
    "tweet_list_new = []\n",
    "for tweet in tweet_list:\n",
    "    tweet_str = \" \".join(tweet)\n",
    "    tweet_list_new.append(tweet_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-shuttle",
   "metadata": {},
   "source": [
    "Next, our ususal steps to prepare a TM corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "special-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = list(stopwords.words('english'))\n",
    "\n",
    "def Corpuser(corpus):\n",
    "    corpus = word_tokenize(corpus)\n",
    "    corpus = [word.replace(\" \", \"\") for word in corpus]\n",
    "    corpus = [word.lower() for word in corpus if word.isalpha()]\n",
    "\n",
    "    corpus = [word for word in corpus if word not in stopwords]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "# tweet_corp = Corpuser(tweet_list_new)\n",
    "# print(tweet_corp)\n",
    "\n",
    "docs = []\n",
    "for tweet in tweet_list_new:\n",
    "    doc = Corpuser(tweet)\n",
    "    docs.append(str(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-arrow",
   "metadata": {},
   "source": [
    "Our next TM workflow step will be to create a term-document-matrix to count the terms in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "royal-hindu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 806 samples and 1000 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import *\n",
    "\n",
    "tf = FreqDist(docs)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "steady-graphic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbiamo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcpolitics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcworldnews</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yup</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zacharynalepa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zootopia</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3442 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0    1    2    3    4    5    6    7    8    9    ...  990  \\\n",
       "ab               0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abbiamo          0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abc              0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abcpolitics      0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abcworldnews     0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "...            ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "youtube          0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yup              0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zacharynalepa    0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zero             0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zootopia         0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "\n",
       "               991  992  993  994  995  996  997  998  999  \n",
       "ab               0    0    0    0    0    0    0    0    0  \n",
       "abbiamo          0    0    0    0    0    0    0    0    0  \n",
       "abc              0    0    0    0    0    0    0    0    0  \n",
       "abcpolitics      0    0    0    0    0    0    0    0    0  \n",
       "abcworldnews     0    0    0    0    0    0    0    0    0  \n",
       "...            ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "youtube          0    0    0    0    0    0    0    0    0  \n",
       "yup              0    0    0    0    0    0    0    0    0  \n",
       "zacharynalepa    0    0    0    0    0    0    0    0    0  \n",
       "zero             0    0    0    0    0    0    0    0    0  \n",
       "zootopia         0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[3442 rows x 1000 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "dtm = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "dtm = dtm.T\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-feedback",
   "metadata": {},
   "source": [
    "All we have to do now is find out which rownames (terms) of tdm correspond to terms in our hate speech dictionary. The columns (docs) of tdm that are larger than 0 are then the tweets which contain hate speech words.\n",
    "\n",
    "The python function isin answers the question: 'Where do the values in the hate vocabulary appear in the dataframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "starting-simulation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hate_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-38680619626f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhate_voc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhate_voc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhate_voc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhate_speech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhate_voc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hate_dict' is not defined"
     ]
    }
   ],
   "source": [
    "hate_voc = hate_dict['word'].values.tolist()\n",
    "hate_voc = [word.lower() for word in hate_voc if word.isalpha()]\n",
    "\n",
    "hate_speech = dtm[dtm.index.isin(hate_voc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "related-tomorrow",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bubble</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clam</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idiot</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nigga</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trash</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9    ...  990  991  \\\n",
       "abc         0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "bitch       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "boo         0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "bubble      0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "clam        0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "idiot       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "nigga       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "property    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "tan         0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "trash       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "\n",
       "          992  993  994  995  996  997  998  999  \n",
       "abc         0    0    0    0    0    0    0    0  \n",
       "bitch       0    0    0    0    0    0    0    0  \n",
       "boo         0    0    0    0    0    0    0    0  \n",
       "bubble      0    0    0    0    0    0    0    0  \n",
       "clam        0    0    0    0    0    0    0    0  \n",
       "idiot       0    0    0    0    0    0    0    0  \n",
       "nigga       0    0    0    0    0    0    0    0  \n",
       "property    0    0    0    0    0    0    0    0  \n",
       "tan         0    0    0    0    0    0    0    0  \n",
       "trash       0    0    0    0    0    0    0    0  \n",
       "\n",
       "[10 rows x 1000 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-florida",
   "metadata": {},
   "source": [
    "Now we only need to find the indexes of these words to see to which tweet they belong. The columns (docs) of tdm that are larger than 0 are then the tweets which contain hate speech words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "deluxe-moldova",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141, 216, 519]\n",
      "[342]\n"
     ]
    }
   ],
   "source": [
    "hate_speecht = hate_speech.T\n",
    "bitch = hate_speecht.index[hate_speecht['bitch'] > 0].to_list()\n",
    "idiot = hate_speecht.index[hate_speecht['idiot'] > 0].to_list()\n",
    "print(bitch)\n",
    "print(idiot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-version",
   "metadata": {},
   "source": [
    "Let's check out the tweets that contain 'bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "statutory-consumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Trump bitch stopped the fire pit ugh í ½í¹\n",
      "RT @TomiLahren: They will march and protest and whine and bitch and then a magical thing will happen..nothing. President Trump will just coâ¦\n",
      "Fuck trump bitch\n"
     ]
    }
   ],
   "source": [
    "tweets_bitch = tweets.iloc[bitch]['text']\n",
    "for tweet in tweets_bitch:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-freight",
   "metadata": {},
   "source": [
    "Some of these are very angry about Trump, but probably still not really hate speech. This shows the limitations of the approach to use simple words and phrases.\n",
    "\n",
    "But this approach can still be useful to filter tweets for manual review by editors. Twitters and others actually have engines like this. It is frequently used in apps like http://www.huffingtonpost.com/entry/donald-trump-stock-alert_us_586e67dce4b0c4be0af325fc, which sends alerts when Donald Trump tweets about your stocks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}