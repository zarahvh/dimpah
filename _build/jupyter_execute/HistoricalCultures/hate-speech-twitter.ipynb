{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ancient-separate",
   "metadata": {},
   "source": [
    "#  Hate Speech Detector\n",
    "\n",
    "In today's session, we learned that in order to detect sentiments we can simply compare freqeuencies of positive and negative words. To this end, we downloaded a dictionary of such terms from the web and then determined their respective frequency. If there are more positive terms in a document than negative ones, we considered it to have a positive sentiment and otherwise a negative one.\n",
    "\n",
    "There are many such dictinonaries produced by linguists but also other communities such as journalists. We can use these with the same approach we used for detecting sentiments to understand texts in different contexts. Journalists, for instance, have developed https://www.hatebase.org/, the world's largest online repository of structured, multilingual, usage-based hate speech. \n",
    "\n",
    "Here, we will use hatebase to develop a hate speech detector for tweets by counting the number of hate words in tweets. We will concentrate on the English language. You can go to https://www.hatebase.org/ and explore the search functions to take a look at the English terms in hatebase. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-while",
   "metadata": {},
   "source": [
    "Next, we need to download the hatebase dictionary, which is unfortunately not that easy. You need to register for an API key and then work relatively hard to get the API to return all English hate speech terms. \n",
    "\n",
    "I have commented out the hate_vocabulary(api_key) function that speaks to https://www.hatebase.org/ and instead provided you with a direct import from a local CSV file. If you want to, for instance, download the dictionary for another language than English, you need to un-commnent those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surprised-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hate_df = pd.read_csv('https://raw.githubusercontent.com/goto4711/social-cultural-analytics/master/hate-vocab-eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entertaining-repeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>word</th>\n",
       "      <th>meaning</th>\n",
       "      <th>offensiveness</th>\n",
       "      <th>number_of_sightings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>abbo</td>\n",
       "      <td>Australian Aboriginal person. Originally, this...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ABC</td>\n",
       "      <td>[1] American-born Chinese [2] Australian-born ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ABCD</td>\n",
       "      <td>American-Born Confused Desi, Indian Americans,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>abo</td>\n",
       "      <td>Australian Aboriginal person. Originally, this...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>af</td>\n",
       "      <td>An African, used by white Rhodesians.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  word                                            meaning  \\\n",
       "0           1  abbo  Australian Aboriginal person. Originally, this...   \n",
       "1           2   ABC  [1] American-born Chinese [2] Australian-born ...   \n",
       "2           3  ABCD  American-Born Confused Desi, Indian Americans,...   \n",
       "3           4   abo  Australian Aboriginal person. Originally, this...   \n",
       "4           5    af              An African, used by white Rhodesians.   \n",
       "\n",
       "   offensiveness  number_of_sightings  \n",
       "0            0.0                    0  \n",
       "1            0.0                    0  \n",
       "2            0.0                    0  \n",
       "3            0.0                   37  \n",
       "4            0.0                  461  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-travel",
   "metadata": {},
   "source": [
    "Next we access Twitter the way we learned today. The code is set to a query Twitter about 'Trump' below but is not active. In order to activate it, you need to add your Twitter API details. You can of course also change the search_term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sensitive-vessel",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipynb.fs.full.keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d14992c8e9de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconsumer_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwit_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipynb.fs.full.keys'"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import requests\n",
    "from ipynb.fs.full.keys import *\n",
    "\n",
    "consumer_key = twit_key\n",
    "consumer_secret = twit_secr\n",
    "access_token = twit_token\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)\n",
    "\n",
    "search_term = 'Trump'\n",
    "tweets = api.search(q=search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polish-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @EamonJavers: The CEO of Donald Trump‚Äôs new SPAC is a man named Patrick Orlando. According to his bio, he has recently ‚Äúbeen serving as‚Ä¶\n",
      "@Potomacbeat So, in my state the vote total was \n",
      "\n",
      "Biden 424,921\n",
      "Trump 365,654\n",
      "\n",
      "Let's assume for a moment that I cas‚Ä¶ https://t.co/ihfFtAW3dU\n",
      "RT @glennkirschner2: If DOJ refuses to prosecute Donald Trump for the many crimes he inarguably committed, it will NOT be a decision made o‚Ä¶\n",
      "RT @ScottAdamsSays: As of today, no one in a leadership position has explained to the public what is being done to solve the supply chain p‚Ä¶\n",
      "Nine months after being expelled from social media for his role in inciting the Jan. 6 Capitol insurrection, former‚Ä¶ https://t.co/tgpZ5gKf2B\n",
      "RT @ScottAdamsSays: As of today, no one in a leadership position has explained to the public what is being done to solve the supply chain p‚Ä¶\n",
      "@MUDDLAW Stiglitz no es el mismo economista DESPRESTIGIADO democRATA que dijo que la legislaci√≥n de Trump de reduci‚Ä¶ https://t.co/CmYFUDKGme\n",
      "#LFC trump PSG in one key area as Mohamed Salah leads resurgence of star J√ºrgen Klopp trio üî¥‚úç\n",
      "\n",
      "https://t.co/k7VLPX6Eh5\n",
      "RT @LePoint: Donald Trump a annonc√© le lancement de son propre r√©seau social baptis√© #TruthSocial. https://t.co/VOUBfw8Ve1\n",
      "RT @suemitsu: „ÄéSPECTER„Äè„ÅØ„ÄÅTRUMP„Ç∑„É™„Éº„Ç∫Âàù„ÅÆ„Éè„ÉÉ„Éî„Éº„Ç®„É≥„Éâ„Åß„Åô„ÄÇ\n",
      "RT @RonFilipkowski: Does Trump know there is club in Ohio that already has the same name as his new social media platform? ‚ÄúTruth Social.‚Äù‚Ä¶\n",
      "RT @tedlieu: Trump lost.\n",
      "Á•û„Å£„Å¶...TRUMP„ÅÆ„Åì„Å®„Åã\n",
      "TRUMP„Åô„Éº„ÅêÁáÉ„ÇÑ„Åô\n",
      "@Jim_Jordan \n",
      "Not remembering how many times you talked to tRump on 1/6, or the subject matter, is like forgetting w‚Ä¶ https://t.co/6ZkW0YuITt\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-south",
   "metadata": {},
   "source": [
    "Rather than performing a live Twitter search, I have saved the a search on 'Trump' from the day of his 2017 inauguration. That's the read.csv command further down. Please, note that the file was created with the old twitteR library, which means that some of the column names are different from what you are used to. But the one we are interested in is still called 'text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "alone-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"https://raw.githubusercontent.com/goto4711/social-cultural-analytics/master/trump-tweets-20-1.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-upper",
   "metadata": {},
   "source": [
    "Let's take a look at tweets. You will see all texts as well as a lot of other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "northern-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>favorited</th>\n",
       "      <th>favoriteCount</th>\n",
       "      <th>replyToSN</th>\n",
       "      <th>created</th>\n",
       "      <th>truncated</th>\n",
       "      <th>replyToSID</th>\n",
       "      <th>id</th>\n",
       "      <th>replyToUID</th>\n",
       "      <th>statusSource</th>\n",
       "      <th>screenName</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @ChrisTran1997: Wanting Trump to fail makes...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789251489794</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>brrriiieee</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @TuhafAmaGercek: Donald Trump, ABD Ba√Ö¬ükanl...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789234659328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/#!/download/ipad\" ...</td>\n",
       "      <td>sailorreihino</td>\n",
       "      <td>282</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>And you losers on the left continue to wonder ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789230518274</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>DavidYDG</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @rogerwilko: #Trump speech is like the nati...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789209554946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>samella_donavan</td>\n",
       "      <td>384</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>KPHO Phoenix Devotes 24 Hours to Trump's Impac...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-20 19:58:31</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>822533789196881925</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>bcbeat</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  favorited  \\\n",
       "0           1  RT @ChrisTran1997: Wanting Trump to fail makes...      False   \n",
       "1           2  RT @TuhafAmaGercek: Donald Trump, ABD Ba√Ö¬ükanl...      False   \n",
       "2           3  And you losers on the left continue to wonder ...      False   \n",
       "3           4  RT @rogerwilko: #Trump speech is like the nati...      False   \n",
       "4           5  KPHO Phoenix Devotes 24 Hours to Trump's Impac...      False   \n",
       "\n",
       "   favoriteCount replyToSN              created  truncated  replyToSID  \\\n",
       "0              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "1              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "2              0       NaN  2017-01-20 19:58:31       True         NaN   \n",
       "3              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "4              0       NaN  2017-01-20 19:58:31      False         NaN   \n",
       "\n",
       "                   id  replyToUID  \\\n",
       "0  822533789251489794         NaN   \n",
       "1  822533789234659328         NaN   \n",
       "2  822533789230518274         NaN   \n",
       "3  822533789209554946         NaN   \n",
       "4  822533789196881925         NaN   \n",
       "\n",
       "                                        statusSource       screenName  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...       brrriiieee   \n",
       "1  <a href=\"http://twitter.com/#!/download/ipad\" ...    sailorreihino   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...         DavidYDG   \n",
       "3  <a href=\"http://twitter.com/download/android\" ...  samella_donavan   \n",
       "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...           bcbeat   \n",
       "\n",
       "   retweetCount  isRetweet  retweeted  longitude  latitude  \n",
       "0             7       True      False        NaN       NaN  \n",
       "1           282       True      False        NaN       NaN  \n",
       "2             0      False      False        NaN       NaN  \n",
       "3           384       True      False        NaN       NaN  \n",
       "4             0      False      False        NaN       NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-pennsylvania",
   "metadata": {},
   "source": [
    "Next we start with our standard text mining workflow \n",
    "\n",
    "Unfortunately, tweets can be difficult to process, as people use very different types of language, of formatting, etc. I have therefore provided you with a clean_tweets function, which applies to all the texts in the tweets and save the results in a tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "noticed-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(df):\n",
    "    text = df['text']\n",
    "    tweet_list = []\n",
    "    for tweet in text:\n",
    "        tweet = tweet.split()\n",
    "        tweet = [\"\" if len(word) < 3 else word for word in tweet]\n",
    "        tweet_list.append(tweet)\n",
    "    return tweet_list\n",
    "    \n",
    "tweet_list = clean_tweets(tweets)\n",
    "tweet_list_new = []\n",
    "for tweet in tweet_list:\n",
    "    tweet_str = \" \".join(tweet)\n",
    "    tweet_list_new.append(tweet_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-shuttle",
   "metadata": {},
   "source": [
    "Next, our ususal steps to prepare a TM corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "special-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = list(stopwords.words('english'))\n",
    "\n",
    "def Corpuser(corpus):\n",
    "    corpus = word_tokenize(corpus)\n",
    "    corpus = [word.replace(\" \", \"\") for word in corpus]\n",
    "    corpus = [word.lower() for word in corpus if word.isalpha()]\n",
    "\n",
    "    corpus = [word for word in corpus if word not in stopwords]\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "# tweet_corp = Corpuser(tweet_list_new)\n",
    "# print(tweet_corp)\n",
    "\n",
    "docs = []\n",
    "for tweet in tweet_list_new:\n",
    "    doc = Corpuser(tweet)\n",
    "    docs.append(str(doc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-arrow",
   "metadata": {},
   "source": [
    "Our next TM workflow step will be to create a term-document-matrix to count the terms in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "royal-hindu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 806 samples and 1000 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import *\n",
    "\n",
    "tf = FreqDist(docs)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "steady-graphic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbiamo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcpolitics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abcworldnews</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youtube</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yup</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zacharynalepa</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zootopia</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3442 rows √ó 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0    1    2    3    4    5    6    7    8    9    ...  990  \\\n",
       "ab               0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abbiamo          0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abc              0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abcpolitics      0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "abcworldnews     0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "...            ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "youtube          0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "yup              0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zacharynalepa    0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zero             0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "zootopia         0    0    0    0    0    0    0    0    0    0  ...    0   \n",
       "\n",
       "               991  992  993  994  995  996  997  998  999  \n",
       "ab               0    0    0    0    0    0    0    0    0  \n",
       "abbiamo          0    0    0    0    0    0    0    0    0  \n",
       "abc              0    0    0    0    0    0    0    0    0  \n",
       "abcpolitics      0    0    0    0    0    0    0    0    0  \n",
       "abcworldnews     0    0    0    0    0    0    0    0    0  \n",
       "...            ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "youtube          0    0    0    0    0    0    0    0    0  \n",
       "yup              0    0    0    0    0    0    0    0    0  \n",
       "zacharynalepa    0    0    0    0    0    0    0    0    0  \n",
       "zero             0    0    0    0    0    0    0    0    0  \n",
       "zootopia         0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[3442 rows x 1000 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "dtm = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "dtm = dtm.T\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-feedback",
   "metadata": {},
   "source": [
    "All we have to do now is find out which rownames (terms) of tdm correspond to terms in our hate speech dictionary. The columns (docs) of tdm that are larger than 0 are then the tweets which contain hate speech words.\n",
    "\n",
    "The python function isin answers the question: 'Where do the values in the hate vocabulary appear in the dataframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "starting-simulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_voc = hate_dict['word'].values.tolist()\n",
    "hate_voc = [word.lower() for word in hate_voc if word.isalpha()]\n",
    "\n",
    "hate_speech = dtm[dtm.index.isin(hate_voc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "related-tomorrow",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abc</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bitch</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bubble</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clam</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idiot</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nigga</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tan</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trash</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1    2    3    4    5    6    7    8    9    ...  990  991  \\\n",
       "abc         0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "bitch       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "boo         0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "bubble      0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "clam        0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "idiot       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "nigga       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "property    0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "tan         0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "trash       0    0    0    0    0    0    0    0    0    0  ...    0    0   \n",
       "\n",
       "          992  993  994  995  996  997  998  999  \n",
       "abc         0    0    0    0    0    0    0    0  \n",
       "bitch       0    0    0    0    0    0    0    0  \n",
       "boo         0    0    0    0    0    0    0    0  \n",
       "bubble      0    0    0    0    0    0    0    0  \n",
       "clam        0    0    0    0    0    0    0    0  \n",
       "idiot       0    0    0    0    0    0    0    0  \n",
       "nigga       0    0    0    0    0    0    0    0  \n",
       "property    0    0    0    0    0    0    0    0  \n",
       "tan         0    0    0    0    0    0    0    0  \n",
       "trash       0    0    0    0    0    0    0    0  \n",
       "\n",
       "[10 rows x 1000 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-florida",
   "metadata": {},
   "source": [
    "Now we only need to find the indexes of these words to see to which tweet they belong. The columns (docs) of tdm that are larger than 0 are then the tweets which contain hate speech words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "deluxe-moldova",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141, 216, 519]\n",
      "[342]\n"
     ]
    }
   ],
   "source": [
    "hate_speecht = hate_speech.T\n",
    "bitch = hate_speecht.index[hate_speecht['bitch'] > 0].to_list()\n",
    "idiot = hate_speecht.index[hate_speecht['idiot'] > 0].to_list()\n",
    "print(bitch)\n",
    "print(idiot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-version",
   "metadata": {},
   "source": [
    "Let's check out the tweets that contain 'bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "statutory-consumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Trump bitch stopped the fire pit ugh √≠¬†¬Ω√≠¬π¬Ñ\n",
      "RT @TomiLahren: They will march and protest and whine and bitch and then a magical thing will happen..nothing. President Trump will just co√¢¬Ä¬¶\n",
      "Fuck trump bitch\n"
     ]
    }
   ],
   "source": [
    "tweets_bitch = tweets.iloc[bitch]['text']\n",
    "for tweet in tweets_bitch:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-freight",
   "metadata": {},
   "source": [
    "Some of these are very angry about Trump, but probably still not really hate speech. This shows the limitations of the approach to use simple words and phrases.\n",
    "\n",
    "But this approach can still be useful to filter tweets for manual review by editors. Twitters and others actually have engines like this. It is frequently used in apps like http://www.huffingtonpost.com/entry/donald-trump-stock-alert_us_586e67dce4b0c4be0af325fc, which sends alerts when Donald Trump tweets about your stocks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}