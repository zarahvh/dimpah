{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Sensing 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of social, cultural and other datasets out there on the web. We have already learned how to extract data from web sites directly. \n",
    "\n",
    "Many data items can also be accessed via a so-called API (Application Programming Interface). You can think of an API as a window through which you have access to remotely stored data rather than web pages. Often access to these APIs is limited by some kind of registration key you have to use to open that window. \n",
    "\n",
    "First, we will use data from https://www.data.gov/, which is 'the home of the U.S. Government’s open data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sca\n",
    "from ipynb.fs.full.keys import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call an API in Python and access data from a website we need the requests package first.  Please, import it with `import requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the API calls we are doing as a direct way of accessing the data behind the web pages you see in your browser. As a taste go to a browser window, and paste the following URL: https://developer.nrel.gov/api/utility_rates/v3.json?api_key=DEMO_KEY&address=1600+Amphitheatre+Parkway%2C+Mountain+View%2C+CA\n",
    "\n",
    "What do you see? You should see details about an address in the United States '1600 Amphitheatre Parkway, Mountain View, CA'. What you see is the JSON format, which is a way of transmitting data rather than websites. According to https://en.wikipedia.org/wiki/JSON, JSON describes'data objects consisting of attribute–value pairs'. For instance, you should see something like \"address\":\"1600 Amphitheatre Parkway, Mountain View, CA\", which defines the address. It looks a bit like a Python dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to open a session with request. Type in `S = requests.Session()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = requests.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the URL we have to address with our call, which is everything before the ? in the URL above. So, type in `URL = 'https://developer.nrel.gov/api/utility_rates/v3.json'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://developer.nrel.gov/api/utility_rates/v3.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to play with electricity rates and associated coordinate information for US locations. Let’s first define a place we are interested in.  \n",
    "\n",
    "For our requests call, we next need a number of parameters like the location we are interested in. We also need the API key, which is something we will come back to later again when we look at Twitter. It is basically a way of authenticating our requests. For playing with data.gov, we can use the DEMO_KEY.\n",
    "\n",
    "Run:\n",
    "```\n",
    "PARAMS = {\n",
    "    \"address\": \"1600 Amphitheatre Parkway, Mountain View, CA\",\n",
    "    \"api_key\": 'DEMO_KEY'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"address\": \"1600 Amphitheatre Parkway, Mountain View, CA\",\n",
    "    \"api_key\": 'DEMO_KEY'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you know who ‘lives’ at this address? \n",
    "\n",
    "Next we run the actual request, which is called a get-request, because we want to retrieve data. We can also 'put' data with a put request, if we have the right access. For the call `R = S.get(url=URL, params=PARAMS, verify=False)`, we need the URL and PARAMS as parameters. We also say verify=False, as we are just experimenting and don't need verification. Please, ignore any warning you might get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'developer.nrel.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "R = S.get(url=URL, params=PARAMS, verify=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request has a json() function that can easily translate the received JSON into a Python object. Run:\n",
    "```\n",
    "DATA = R.json()\n",
    "DATA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': {'address': '1600 Amphitheatre Parkway, Mountain View, CA'},\n",
       " 'errors': [],\n",
       " 'warnings': [],\n",
       " 'version': '3.1.0',\n",
       " 'metadata': {'sources': ['Ventyx Research (2012)']},\n",
       " 'outputs': {'company_id': '14328',\n",
       "  'utility_name': 'Pacific Gas & Electric Co',\n",
       "  'utility_info': [{'company_id': '14328',\n",
       "    'utility_name': 'Pacific Gas & Electric Co'}],\n",
       "  'commercial': 0.1408,\n",
       "  'industrial': 0.0898,\n",
       "  'residential': 0.1559}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA = R.json()\n",
    "DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we got no errors and that we learned quite a bit about the utilities at the locations we are interested. This is because the data actually comes from https://nrel.gov/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the result is a rather complex structure still. It is a dictionary of dictionaries. But we know how to access the individual elements. Type `DATA['outputs']['utility_name']` to print out what? Do you remember how we use dictionaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pacific Gas & Electric Co'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA['outputs']['utility_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, we got the utitility name. Try now to get the residential electricity rate at the address, look for outputs and then residential. Tip: you need to replace 'utility_name' with 'residential'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1559"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA['outputs']['residential']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many useful APIs out there. Check out https://www.programmableweb.com/. Unfortunately, many are not as easy to access as data.gov. \n",
    "\n",
    "Let's check next the geo-locations for our address. You have of course already guessed that it is the location of the Googleplex, Google’s HQ. \n",
    "\n",
    "We will now repeat the same steps as above but with the wikipedia API. It really is always just the repetition of the same steps. \n",
    "\n",
    "Type in `URL = 'https://en.wikipedia.org/w/api.php'` to go to the Wikipedia API service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/w/api.php'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the latitude and longitude of the Googleplex. \n",
    "\n",
    "To this end we need to give the API the following paramenter, which define the type of action, the format, the title of Wikipedia page and what it should return, the coordinates.\n",
    "\n",
    "Run:\n",
    "```\n",
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"Googleplex\",\n",
    "    \"prop\": \"coordinates\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    \"action\": \"query\",\n",
    "    \"format\": \"json\",\n",
    "    \"titles\": \"Googleplex\",\n",
    "    \"prop\": \"coordinates\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell is given to you, as it just exectures the requests and decodes the returned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'en.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'773423': {'pageid': 773423,\n",
       "  'ns': 0,\n",
       "  'title': 'Googleplex',\n",
       "  'coordinates': [{'lat': 37.422,\n",
       "    'lon': -122.084,\n",
       "    'primary': '',\n",
       "    'globe': 'earth'}]}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keep cell\n",
    "\n",
    "R = S.get(url=URL, params=PARAMS, verify=False)\n",
    "DATA = R.json()\n",
    "pages = DATA['query']['pages']\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longitude and latitude are hidden in there somewhere. Print out the latitude with `pages['773423']['coordinates'][0]['lat']`. The first number might differ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.422"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages['773423']['coordinates'][0]['lat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could now use https://www.latlong.net/Show-Latitude-Longitude.html to map these longitude and latitude and would find the Googleplex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just worked through simple API requests that got us locations. More interesting will be to access social media applications like Twitter and Facebook. We can also get their data through APIs. Twitter is especially popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python there is a library that makes accessing Twitter data simple. Download and import tweepy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '':\n",
    "    consumer_key = twit_key\n",
    "    consumer_secret = twit_secr\n",
    "    access_token = twit_token\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "else:\n",
    "    print('No api key available, skip next cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are connected to Twitter and can run queries. Let’s get Barack Obama's timeline and his first tweet first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '':\n",
    "    tweets = api.user_timeline(screen_name = 'BarackObama', count=100)\n",
    "    first_tweet = tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the first tweet of the dataset by typing first_tweet.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Veterans Day, I want to share Tom Voutsos's story. He served in the U.S. Marine Corps, and has continued to li… https://t.co/uEiwBot9K1\n"
     ]
    }
   ],
   "source": [
    "if twit_key != '':\n",
    "    print(first_tweet.text)\n",
    "else:\n",
    "    obama_tweets = pd.read_csv('obama_tweets.csv')\n",
    "    first_tweet = obama_tweets.text[0]\n",
    "    print(first_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all the attributes of a tweet, simply type tweets[0], but for now we only want to check the date using first_tweet.created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-11 14:15:05+00:00\n"
     ]
    }
   ],
   "source": [
    "if twit_key != '':\n",
    "    print(first_tweet.created_at)\n",
    "else:\n",
    "    print(obama_tweets.iloc[0]['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we find a way to plot Obama his twitter activity?\n",
    "\n",
    "TB: I would provide the following as a helper function and add it to sca.py where we should collect all the helper functions.\n",
    "\n",
    "Then, just say please run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAL5ElEQVR4nO3cf4jkdR3H8der28silbSb5PCuRqlEi0xZzvIXeGGcFplgoJT9c7JICRpRrPRX/RGkYP7TP4eJRqYFaoWrqZmHSXY1Z6feefkTI828uX5wCmGp7/6Y797ObbM7392dz+x7d54PGHZ25nvf7/uzs/tk/M6MjggBAPJ623IPAACYH6EGgOQINQAkR6gBIDlCDQDJjZXY6bp166LZbJbYNQCsSjt37twfEY1e9xUJdbPZVKvVKrFrAFiVbP95rvs49QEAyRFqAEiOUANAcoQaAJIj1ACQHKEGgORqvT3P9guSXpX0pqQ3ImK85FAAgBkLeR/1ORGxv9gkAICeOPUBAMnVDXVIus/2TtsTvTawPWG7ZbvVbrcHNyGARWtOTqk5OTW0f4cy6ob6zIg4VdJ5kr5i++zZG0TEtogYj4jxRqPnx9UBAItQK9QR8VL1dZ+kOyVtKjkUAGBG31DbfpftI6avS/qUpN2lBwMAdNR518cxku60Pb39jyPil0WnAgAc1DfUEfG8pJOHMAsAoAfengcAyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBILnaoba9xvYfbd9VciAAwKEW8oz6Skl7Sw0CAOitVqhtb5D0aUk3lB0HADBb3WfU10v6hqS35trA9oTtlu1Wu90exGzAojUnp9ScnFruMbBKDfv3q2+obX9G0r6I2DnfdhGxLSLGI2K80WgMbEAAGHV1nlGfIemztl+QdJukzbZ/VHQqAMBBfUMdEVdHxIaIaEq6WNKvI+KLxScDAEjifdQAkN7YQjaOiO2StheZBADQE8+oASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAk1zfUtt9h+/e2H7O9x/a3hjEYAKBjrMY2r0vaHBGv2V4r6WHb90TE7wrPBgBQjVBHREh6rfp2bXWJkkMBAGbUOkdte43tXZL2Sbo/InYUnQoAcFCtUEfEmxHxMUkbJG2y/ZHZ29iesN2y3Wq32wMeM7fm5JSak1NL3geAlavk3/CC3vUREf+S9KCkLT3u2xYR4xEx3mg0BjQeAKDOuz4att9dXX+npHMl/anwXACASp13fayXdLPtNeqE/acRcVfZsQAA0+q86+NxSacMYRYAQA98MhEAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCS6xtq2xttP2j7Sdt7bF85jMEAAB1jNbZ5Q9LXIuJR20dI2mn7/oh4svBsAADVeEYdES9HxKPV9Vcl7ZV0bOnBAAAdCzpHbbsp6RRJO3rcN2G7ZbvVbrcHNN7iNSen1JycSrev0krOulJ+BqXMXv9cP4+5HoOFPjaZfu+GOctCfn5LnWmp6xrWz6V2qG0fLul2SVdFxIHZ90fEtogYj4jxRqMxyBkBYKTVCrXttepE+paIuKPsSACAbnXe9WFJP5C0NyKuKz8SAKBbnWfUZ0i6VNJm27uqy/mF5wIAVPq+PS8iHpbkIcwCAOiBTyYCQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiub6ht32h7n+3dwxgIAHCoOs+ob5K0pfAcAIA59A11RDwk6R9DmAUA0MPAzlHbnrDdst1qt9uL3k9zcur/vnZfureb7/Y6x6g7S6/be91X59j9tqsze6919ztG3dmWMtNiZxjEbAvdxyB/HkvZzyDWPYhjL2aWfjMs9Xd5EDMt5t8v5O9qWAYW6ojYFhHjETHeaDQGtVsAGHm86wMAkiPUAJBcnbfn3SrpEUkn2H7R9tbyYwEApo312yAiLhnGIACA3jj1AQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMkRagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJEeoASA5Qg0AyRFqAEiOUANAcoQaAJIj1ACQHKEGgOQINQAkR6gBIDlCDQDJEWoASI5QA0ByhBoAkiPUAJAcoQaA5Ag1ACRHqAEgOUINAMnVCrXtLbafsv2s7cnSQwEAZvQNte01kr4v6TxJJ0m6xPZJpQcDAHTUeUa9SdKzEfF8RPxH0m2SLig7FgBgmiNi/g3siyRtiYjLqu8vlXRaRFwxa7sJSRPVtydIemrw4xazTtL+5R5iGbH+0Vz/qK57Wrb1vz8iGr3uGBvUESJim6Rtg9rfMNluRcT4cs+xXFj/aK5/VNc9bSWtv86pj5ckbez6fkN1GwBgCOqE+g+SPmj7ONtvl3SxpF+UHQsAMK3vqY+IeMP2FZLulbRG0o0Rsaf4ZMO1Ik/ZDBDrH02juu5pK2b9fV9MBAAsLz6ZCADJEWoASG5Fhtr2RtsP2n7S9h7bV1a3H237ftvPVF+Pqm7/gu3HbT9h+7e2T+7a142299ne3eeYPT9Gb/s3tndVl7/a/lmhZXfPkmn9n7T9aLX+h21/oNS6u46Zaf2bq/Xvtn2z7YG95TXJuntuN9cxS0q2/s9XM7xlu/xb/CJixV0krZd0anX9CElPq/Px9mskTVa3T0r6bnX9dElHVdfPk7Sja19nSzpV0u55jrdG0nOSjpf0dkmPSTqpx3a3S/rSKK2/OvaJ1fUvS7ppVNavzhOdv0j6ULXdtyVtXS3rnm+7uY65mh73Pus/UZ0P9m2XNF587aUPMIyLpJ9LOledT0Ou73pQn+qx7VGSXpp1W7PPH+onJN3b9f3Vkq6etc2Rkv4p6chRWn91zNO6bv/OqKxfUkPSc123nyXp7tWy7vm2q3PM1bz+rvuGEupi/5k2LLabkk6RtEPSMRHxcnXX3yQd0+OfbJV0zwIPc6w6z5ymvSjptFnbfE7SAxFxYIH7XpIE679M0t22/y3pgKSPL3DfS7LM698vacz2eES0JF2kQz8cVsyQ1j2fOscsJsH6h2pFh9r24eqcbrgqIg7YPnhfRITtmLX9Oeo8YGcWGOcSSTcU2O+ckqz/q5LOj4gdtr8u6Tp14l3ccq+/OsbFkr5n+zBJ90l6cxD7ns9yr3u2XscsKdv6h2FFvpgoSbbXqvNg3RIRd1Q3v2J7fXX/ekn7urb/qDohvSAi/t5n3xs98wLh5erzMXrb69T5vwxOLX1l9WRYv+2GpJMjYkd1+0/UOS9YXIb1S1JEPBIRZ0XEJkkPqXPetJghr3s+cx6zpETrH65hn1caxEWSJf1Q0vWzbr9Wh76ocE11/X2SnpV0+hz7a2r+c5Rjkp6XdJxmXkz6cNf9l0u6edTWX92+XzMvpm2VdPuorL+6773V18MkPSBp82pZ93zbzXXM1fS419lOvJg47w/uTEkh6XFJu6rL+ZLeU/2xPCPpV5KOrra/QZ0X+qa3bXXt61ZJL0v6rzrnHnu+al/t/2l1Xv3/Zo8Ha8sorl/ShZKeUCde2yUdP2Lrv1bSXnVe0LpqFa6753ZzHXOE1n9h9f3rkl5R14vNJS58hBwAklux56gBYFQQagBIjlADQHKEGgCSI9QAkByhBoDkCDUAJPc/ODipNUH/GIcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "filenames": {
       "image/png": "/Users/Zarah/Documents/dimpah/_build/jupyter_execute/SocialAnalytics/SocialMedia_old/Social_Sensing_1_41_1.png"
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sca.plot_twitter_activity(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s move on from Obama. In order to access my own Twitter favourites, please type in api.favorites('tobias_blanke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'API' object has no attribute 'favorites'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d5f7d56633c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtwit_key\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfavorites\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfavorites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tobias_blanke'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfavorite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfavorites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfavorite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'favorites'"
     ]
    }
   ],
   "source": [
    "if twit_key != '':\n",
    "    favorites = api.favorites('tobias_blanke')\n",
    "    favorite = favorites[0] \n",
    "    print(favorite.text)\n",
    "else:\n",
    "    print('not possible without key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, so good. Of course, these requests were quite simple. So, let’s try something more complicated. We start with first a look at the retweet structures and then a typical content analysis in Twitter. Tweets are little pieces of texts with lots of metadata attached to them (https://en.wikipedia.org/wiki/Twitter). So, it is not surprising that many people try and run text analysis on the content of tweets. Let’s start with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to search Twitter for something of interest by running api.search(q=‘#uva’, count = 10). This search will look for the most recent tweets (count=10) with the hashtag uva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'API' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dca90017e94f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtwit_key\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muva_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'#uva'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0muva_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uva_tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "if twit_key != '':\n",
    "    uva_tweets = api.search(q='#uva', count =10)\n",
    "else:\n",
    "    uva_tweets = pd.read_csv('uva_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see each tweet has an id and a lot of other metadata attached to it such as retweets, locations, etc. Did you know that you produce so much information with each tweet?\n",
    "\n",
    "There are a lot of things we can do with tweets and their metadata. Have a look at the documentation of the package or the many examples online. A quick example would be to return retweets. The first step is to find them. Looking at the metadata of each tweet in kcl_tweets, there are two relevant fields with retweet_count and retweeted. With retweet_count, we can check whether a tweet has been retweeted (retweet_count > 0), while retweeted tells us whether a tweet was a retweet itself. We want to find only those tweets that have not been retweets (retweeted == False) but are not a retweet. So, please select those kcl_tweets that have been retweeted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uva_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-fdace45f3296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mretweeted_uva_tweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtwit_key\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0muva_tweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muva_tweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muva_tweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretweeted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muva_tweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretweet_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mretweeted_uva_tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muva_tweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uva_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "retweeted_uva_tweets = []\n",
    "if twit_key != '':\n",
    "    for uva_tweet in uva_tweets:\n",
    "        if uva_tweet.retweeted == False and uva_tweet.retweet_count > 0:\n",
    "            retweeted_uva_tweets.append(uva_tweet.text)\n",
    "else:\n",
    "    for index, row in uva_tweets.iterrows():\n",
    "            if uva_tweets.iloc[index]['retweeted'] == False and uva_tweets.iloc[index]['retweet_count'] > 0:\n",
    "                retweeted_uva_tweets.append(uva_tweets.iloc[index]['text'])\n",
    "print(retweeted_uva_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised, we would like to run some simple content analysis with the text in the tweets. We will produce a simple word cloud. But before we can do this we need to first create a corpus from the tweets, the same as we did with the speeches in the Text assignment.\n",
    "Let's first extract the text from the uva_tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uva_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-175f18a23bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtweet_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtwit_key\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muva_tweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mtweet_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uva_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "tweet_text = []\n",
    "if twit_key != '':\n",
    "    for t in uva_tweets:\n",
    "        text = t.text\n",
    "        tweet_text.append(text)\n",
    "else:\n",
    "    for index, row in uva_tweets.iterrows():\n",
    "        text = uva_tweets.iloc[index]['text']\n",
    "        tweet_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-ed3bd7b48be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_wordcloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/dimpah/SocialAnalytics/SocialMedia_new/sca.py\u001b[0m in \u001b[0;36mplot_wordcloud\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     wordcloud = WordCloud(width = 800, height = 800, \n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mbackground_color\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     min_font_size = 10).generate(str(corpus)) \n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \"\"\"\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \"\"\"\n\u001b[1;32m    613\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[0m\u001b[1;32m    404\u001b[0m                              \"got %d.\" % len(frequencies))\n\u001b[1;32m    405\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    " sca.plot_wordcloud(tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm https shows up there pretty big, let's check if everything went alright. \n",
    "Print the tweets that we just added to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = tweet_text[0]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is a link to the tweet in each text, we should have removed that first. We can use regular expressions for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tweet = re.sub(r'http\\S+', '', tweet)\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []\n",
    "for tweet in tweet_text:\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    cleaned_tweets.append(tweet)\n",
    "\n",
    "cleaned_tweets   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(cleaned_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very popular with Twitter is also the analysis of followers. I don’t have so many. In fact, I am not really using Twitter much. But let’s still try. You can get my Twitter information by entering me = api.get_user(screen_name = 'tobias_blanke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '':\n",
    "    me = api.get_user(screen_name = 'tobias_blanke')\n",
    "    print(me.description)\n",
    "else:\n",
    "    print('not possible without key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s me! My followers are a little bit more interesting. We can retrieve by going throug api.followers('tobias_blanke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first 20 followers\n",
    "\n",
    "if twit_key != '':\n",
    "    for follower in api.followers('tobias_blanke'): \n",
    "        print(follower.screen_name)\n",
    "else:\n",
    "    print('not possible without key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven’t explained it yet, but Twitter limits the amount of API calls you can do at any moment in time, which is often an issue if you retrieve a lot of followers from accounts like Donalds Trump’s. Rate limits are often one of the biggest issues in Twitter analysis once the data gets a bit bigger. Check it out at https://dev.twitter.com/rest/public/rate-limiting. You will find plenty of people online who complain.\n",
    "\n",
    "Which is also something that can happen while trying to access the information that we try to extract in this assignment, just in case that happens you can find a csv file with the same info in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first get the ids of all the followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_list = sca.get_follower_list(me, twit_key=twit_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key measure of my own importance on Twitter is the importance of the people who follow me. Does this make sense? Of course it does, as with important followers you can influence a lot of people. Let’s plot this measure and get an overview of the friends and followers of those who follow me. \n",
    "To do so we first want to create a dataframe that contains all my followers and their follower and friend count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "# if no key is used it is already a dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if twit_key != '':\n",
    "    df = pd.DataFrame(columns=['user','follower'])\n",
    "    df['follower'] = follower_list[0]\n",
    "    df['user'] = tb_id\n",
    "else:\n",
    "    df = pd.read_csv('followers_tobias.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '':\n",
    "    followers = follower_list[0]\n",
    "    fol_count = sca.get_follower_count(followers, 200)\n",
    "    df200 = df.head(200)\n",
    "    df200['follower_count'] = fol_count\n",
    "else:\n",
    "    df200 = pd.read_csv('200_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '':\n",
    "    followers = follower_list[0][:200]\n",
    "    friends_count = sca.get_friends_count(user, followers)\n",
    "    df200['friends_count'] = friends_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df200.plot.scatter(x='friends_count', y='follower_count',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, there are not too many strong performers in my followers’ list. In order to confirm this, let’s check the counts for all my followers with a plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df200['follower_count'].to_list()\n",
    "plt.hist(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, most of my followers do not have too many followers themselves apart from one outlier. My influence is really limited. Let’s quickly move on then.\n",
    "\n",
    "Social network analysis is really important both in social and cultural analytics. It uses graphs to explain and analyse social relations. We have already started talking about social networks. We looked into my followers and those friends that I am following. Then, we investigated the friends of these friends and the followers of these followers. To build these kinds of relationships and map them onto graphs to visualise and analyse them is really what social networks are all about.\n",
    "\n",
    "We would like to build a graph of my friends and followers. We already have the followers, let's now reterive their screen names.\n",
    "\n",
    "Because graph visualisation can quickly get confusing if there are too many items to represent, we would like to limit the number of friends and followers to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '':\n",
    "    sca.get_follower_count(followers,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '':  \n",
    "    df20 = df200.head(20)\n",
    "    df20['screen_name_follower'] = screen_names\n",
    "    df20['screen_name_user'] = api.get_user(tb_id).screen_name\n",
    "else:\n",
    "    df20 = pd.read_csv('20_followers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the friends of user/source\n",
    "get_friends_of20(me.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '': \n",
    "    df_friends = pd.DataFrame(columns=['user','follower'])\n",
    "    df_friends['user'] = friends\n",
    "    df_friends['follower'] = tb_id\n",
    "else:\n",
    "    df_friends = pd.read_csv('df_friends.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '': \n",
    "    df_friends['screen_name_follower'] = api.get_user(tb_id).screen_name\n",
    "    df_friends['screen_name_user'] = screen_name_friends\n",
    "    df_friends['follower_count'] = len(followers)\n",
    "    df_friends['friends_count'] = len(friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twit_key != '': \n",
    "    graphdata = pd.concat([df20,df_friends])\n",
    "else:\n",
    "    graphdata pd.read_csv('graphdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame(columns=['userid', 'user_name'])\n",
    "nodes['userid'] = graphdata.user.unique()\n",
    "nodes['user_name'] = graphdata.screen_name_user.unique()\n",
    "\n",
    "relations = pd.DataFrame(columns=['user', 'follower'])\n",
    "relations['user'] = graphdata['user']\n",
    "relations['follower'] = graphdata['follower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.from_pandas_edgelist(relations,source='follower', target='user', create_using=nx.DiGraph())\n",
    "\n",
    "node_attr = nodes.set_index('userid').to_dict('index')\n",
    "nx.set_node_attributes(G, node_attr)\n",
    "node_labels = nx.get_node_attributes(G, 'user_name')\n",
    "\n",
    "nx.draw(G, with_labels=True, labels=node_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And a function to plot the graph\n",
    "\n",
    "# import igraph\n",
    "# from igraph import *\n",
    "# graph = Graph(directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph = Graph.DictList(\n",
    "#           vertices=nodes.to_dict('records'),\n",
    "#           edges=relations.to_dict('records'),\n",
    "#           directed=True,\n",
    "#           vertex_name_attr='userid',\n",
    "#           edge_foreign_keys=('follower', 'user'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(graph, vertex_label=graph.vs['user_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}