
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Analysing Historical Cultures 1 &#8212; DIMPAH</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Analysing Historical Cultures 2" href="Text_2.html" />
    <link rel="prev" title="Social Sensing 2" href="../SocialMedia/Social_Sensing_2.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">DIMPAH</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Intro
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../Introduction.html">
   Introduction to Python
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../Introduction/Introduction_1.1.html">
     Introduction to Python 1.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Introduction/Introduction_1.2.html">
     Introduction to Python 1.2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Introduction/Introduction_2.1.html">
     Introduction to Python 2.1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Introduction/Introduction_2.2.html">
     Introduction to Python 2.2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="../../social_analytics.html">
   Social Analytics
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../SocialMedia/Social_Sensing_1.html">
     Social Sensing 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../SocialMedia/Social_Sensing_2.html">
     Social Sensing 2
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Analysing Historical Cultures 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="Text_2.html">
     Analysing Historical Cultures 2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../VisualsArts/Visuals_and_Arts_1.html">
     Visuals and Arts 1
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../VisualsArts/Visuals_and_Arts_2.html">
     Visuals and Arts 2
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../AI_for_society.html">
   AI for society
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../AI_for_society/ass1/Machine_Bias.html">
     Machine Bias
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../AI_for_society/ass2/Civility_in_Communication.html">
     Civility in Communication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../AI_for_society/ass3/Data_Exploration.html">
     Exploratory Data Analysis with Python
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../AI_for_society/ass3/US_Congress_Sponsors.html">
     Collaboration Networks in the US Congress
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../AI_for_society/ass3/Analysing_Communities_1.html">
     Detecting Political Communities of Practice
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../AI_for_society/ass3/Analysing_Communities_2.html">
     Social Data from the Web
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../PredictingModelling.html">
   AI for society
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../PredictingModelling/LinearPrediction.html">
     Linear Regression Prediction
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../PredictingModelling/PredictingModelling.html">
     Predicting Modelling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../PredictingModelling/Titanic.html">
     Predicting the survival of Titanic passengers
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/SocialAnalytics/Text/Text_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/zarahvh/dimpah"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/zarahvh/dimpah/issues/new?title=Issue%20on%20page%20%2FSocialAnalytics/Text/Text_1.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/zarahvh/dimpah/master?urlpath=tree/SocialAnalytics/Text/Text_1.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="analysing-historical-cultures-1">
<h1>Analysing Historical Cultures 1<a class="headerlink" href="#analysing-historical-cultures-1" title="Permalink to this headline">¶</a></h1>
<p>Today we work on historical cultures. The methods are not very different from social analytics, which we have already discussed. But the data is often only available in texts, as your readings have discussed. That’s why we use this opportunity to concentrate on how to analyse texts.</p>
<p>We have already talked about texts a little bit when we were looking at how to analyse the content of tweets. In this session, we will finally find out how to be able to do this by ourselves. Text analysis is an advanced field in the world of computational analytics, and we can rely on a very long tradition of doing text analysis and very established methods. Besides, it is fun and maybe at the same time the dominant form in social and cultural analytics – simply because we work so much with texts, documents, etc.</p>
<p>We will go back further in history and mainly work the US State of the Union Address. This allows to first of all understand a lot about the past state of the political constitution of the USA. The State of the Union (SOTU) data is taken from <a class="reference external" href="http://stateoftheunion.onetwothree.net/index.shtml">http://stateoftheunion.onetwothree.net/index.shtml</a> and provides access to the corpus of all the State of the Union addresses from 1790 to 2015 at the time of writing. SOTU allows you to explore how specific words gain and lose prominence over time, and to link to information on the historical context for their use.</p>
<p>We did all the hard work of loading this data for you. It should be available to you as the data frame sotu_all.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<p>The dataframe is an rdata file, which python cannot read, luckily python has a package for this.
(Are we going to let the students take this step or do we provide them with a csv file?)
Check out the file, after converting it into a pandas dataframe, by using head() and you will see three columns. The first one contains the full text, the second one the year and the third the exact date in the format YYYYMMDD.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyreadr</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pyreadr</span><span class="o">.</span><span class="n">read_r</span><span class="p">(</span><span class="s1">&#39;data/sotu_all.rda&#39;</span><span class="p">)</span> 

<span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;sotu_all&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>speechtext</th>
      <th>year</th>
      <th>date</th>
    </tr>
    <tr>
      <th>rownames</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>17900108.html</th>
      <td>Fellow-Citizens of the Senate and House of Re...</td>
      <td>1790</td>
      <td>17900108</td>
    </tr>
    <tr>
      <th>17901208.html</th>
      <td>Fellow-Citizens of the Senate and House of Re...</td>
      <td>1790</td>
      <td>17901208</td>
    </tr>
    <tr>
      <th>17911025.html</th>
      <td>Fellow-Citizens of the Senate and House of Re...</td>
      <td>1791</td>
      <td>17911025</td>
    </tr>
    <tr>
      <th>17921106.html</th>
      <td>Fellow-Citizens of the Senate and House of Re...</td>
      <td>1792</td>
      <td>17921106</td>
    </tr>
    <tr>
      <th>17931203.html</th>
      <td>Fellow-Citizens of the Senate and House of Re...</td>
      <td>1793</td>
      <td>17931203</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>You can see the structure of the datframe, but you cannot completely see what is in the speechtext column. Try and print out the first text entry of the dataframe (speechtext).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;speechtext&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39; Fellow-Citizens of the Senate and House of Representatives: I embrace with great satisfaction the opportunity which now presents itself of congratulating you on the present favorable prospects of our public affairs. The recent accession of the important state of North Carolina to the Constitution of the United States (of which official information has been received), the rising credit and respectability of our country, the general and increasing good will toward the government of the Union, and the concord, peace, and plenty with which we are blessed are circumstances auspicious in an eminent degree to our national prosperity. In resuming your consultations for the general good you can not but derive encouragement from the reflection that the measures of the last session have been as satisfactory to your constituents as the novelty and difficulty of the work allowed you to hope. Still further to realize their expectations and to secure the blessings which a gracious Providence has placed within our reach will in the course of the present important session call for the cool and deliberate exertion of your patriotism, firmness, and wisdom. Among the many interesting objects which will engage your attention that of providing for the common defense will merit particular regard. To be prepared for war is one of the most effectual means of preserving peace. A free people ought not only to be armed, but disciplined; to which end a uniform and well-digested plan is requisite; and their safety and interest require that they should promote such manufactories as tend to render them independent of others for essential, particularly military, supplies. The proper establishment of the troops which may be deemed indispensable will be entitled to mature consideration. In the arrangements which may be made respecting it it will be of importance to conciliate the comfortable support of the officers and soldiers with a due regard to economy. There was reason to hope that the pacific measures adopted with regard to certain hostile tribes of Indians would have relieved the inhabitants of our southern and western frontiers from their depredations, but you will perceive from the information contained in the papers which I shall direct to be laid before you (comprehending a communication from the Commonwealth of Virginia) that we ought to be prepared to afford protection to those parts of the Union, and, if necessary, to punish aggressors. The interests of the United States require that our intercourse with other nations should be facilitated by such provisions as will enable me to fulfill my duty in that respect in the manner which circumstances may render most conducive to the public good, and to this end that the compensation to be made to the persons who may be employed should, according to the nature of their appointments, be defined by law, and a competent fund designated for defraying the expenses incident to the conduct of foreign affairs. Various considerations also render it expedient that the terms on which foreigners may be admitted to the rights of citizens should be speedily ascertained by a uniform rule of naturalization. Uniformity in the currency, weights, and measures of the United States is an object of great importance, and will, I am persuaded, be duly attended to. The advancement of agriculture, commerce, and manufactures by all proper means will not, I trust, need recommendation; but I can not forbear intimating to you the expediency of giving effectual encouragement as well to the introduction of new and useful inventions from abroad as to the exertions of skill and genius in producing them at home, and of facilitating the intercourse between the distant parts of our country by a due attention to the post-office and post-roads. Nor am I less persuaded that you will agree with me in opinion that there is nothing which can better deserve your patronage than the promotion of science and literature. Knowledge is in every country the surest basis of public happiness. In one in which the measures of government receive their impressions so immediately from the sense of the community as in ours it is proportionably essential. To the security of a free constitution it contributes in various ways--by convincing those who are intrusted with the public administration that every valuable end of government is best answered by the enlightened confidence of the people, and by teaching the people themselves to know and to value their own rights; to discern and provide against invasions of them; to distinguish between oppression and the necessary exercise of lawful authority; between burthens proceeding from a disregard to their convenience and those resulting from the inevitable exigencies of society; to discriminate the spirit of liberty from that of licentiousness-- cherishing the first, avoiding the last--and uniting a speedy but temperate vigilance against encroachments, with an inviolable respect to the laws. Whether this desirable object will be best promoted by affording aids to seminaries of learning already established, by the institution of a national university, or by any other expedients will be well worthy of a place in the deliberations of the legislature. Gentlemen of the House of Representatives: I saw with peculiar pleasure at the close of the last session the resolution entered into by you expressive of your opinion that an adequate provision for the support of the public credit is a matter of high importance to the national honor and prosperity. In this sentiment I entirely concur; and to a perfect confidence in your best endeavors to devise such a provision as will be truly with the end I add an equal reliance on the cheerful cooperation of the other branch of the legislature. It would be superfluous to specify inducements to a measure in which the character and interests of the United States are so obviously so deeply concerned, and which has received so explicit a sanction from your declaration. Gentlemen of the Senate and House of Representatives: I have directed the proper officers to lay before you, respectively, such papers and estimates as regard the affairs particularly recommended to your consideration, and necessary to convey to you that information of the state of the Union which it is my duty to afford. The welfare of our country is the great object to which our cares and efforts ought to be directed, and I shall derive great satisfaction from a cooperation with you in the pleasing though arduous task of insuring to our fellow citizens the blessings which they have a right to expect from a free, efficient, and equal government. &#39;
</pre></div>
</div>
</div>
</div>
<p>As we saw above, the dataframe also contains the column ‘year’ and ‘date’.</p>
<p>Let’s check out the types of these values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">date</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">year</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">date</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">year</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;str&#39;&gt;
&lt;class &#39;str&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>Both of the types are strings!</p>
<p>But we want the years to be numbers and the dates to be actual dates. This can lead to issues later on when we will analyse the data. Let’s fix that so we can read and use them as the actual date values that they are when analyzing the data later on.</p>
<p>Sotu_all’s date is clearly a date-string in the format YYYYMMDD. Pandas has the powerful pd.to_datetime() function that can parse such character strings and transform them into Python’s date objects. The advantage is that we can then compare dates with each other, add dates to other dates, etc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We can then convert the years into integers by ysing Pandas’ to_numeric() function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Fighting with different formats of date and time is a key part of analysing culture and society. For some past events, for instance, we do not know exact dates or times and have to work with estimates. Sometimes, we only know the approximate time span of when, say, a Roman consul lived. This can cause a lot of issues.</p>
<p>-python package for this?</p>
<p>Runing type() again will confirm  that all the columns are now of the correct type.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">date</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">year</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">date</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">year</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas._libs.tslibs.timestamps.Timestamp&#39;&gt;
&lt;class &#39;numpy.int64&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>With sotu_all, we have a collection of documents reflecting over 200 years in the history of the US in the words of each president. It is time to do something with it and text mine it. As said, text mining is a very established discipline. Text Mining is the process of deriving meaningful information from natural language text.</p>
<p>Natural Language Processing is a component of text mining that performs a special kind of linguistic analysis that essentially helps a machine “read” text. The Natural Language Toolkit (NLTK) is a Python package for natural language processing.</p>
<p>-text mining python –&gt; NLTK</p>
<p>In this chapter, we are going to cover some of the most commonly used methods of the NLTK package and employ them to analyse the historical content of the State of the Union speeches. To this end, first we need to create a so-called corpus. A corpus is term originating from the linguistics community. All you need to know about a corpus is that it is basically a collection of text documents. In python, you can create a corpus from many sources (online or local) and from many different formats such as plain text or PDF.
(<a class="reference external" href="https://www.nltk.org">https://www.nltk.org</a>)</p>
<p>If you are interested, please check online for the many ways NLTK is used. For now we will just need ….</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
</pre></div>
</div>
</div>
</div>
<p>The first task we set ourselves is to understand historical policy differences. The last SOTU address of the Republican George W. Bush was in 2008, while the first one of the Democrat Barack Obama was in 2009. These were also the years of the height of a very severe financial crisis. So, we expect interesting changes in content. To identify these, we would like to produce a word cloud to compare the most frequent terms in both speeches in order to understand policy differences. Let us first create a smaller sample of the data containing only the speeches of 2008 and 2009. We can use pandas df.loc by selecting the rows that contain the column value 2008 or 2009 to create a new sotu_2008_2009 data frame containing only the speeches of 2008 and 2009.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sotu_2008</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2008</span><span class="p">]</span>
<span class="n">sotu_2009</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2009</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now we need to create a corpus. For this we first need to tokenize the words, which means we break the text into units, into words. We are only interested in the full texts and thus only need the values of the column speechtext. So let us first get these texts as strings so that afterwards we can tokenize them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">speechtext_2008</span> <span class="o">=</span> <span class="n">sotu_2008</span><span class="p">[</span><span class="s1">&#39;speechtext&#39;</span><span class="p">]</span>
<span class="n">text_2008</span> <span class="o">=</span> <span class="n">speechtext_2008</span><span class="o">.</span><span class="n">values</span>
<span class="n">strings_2008</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_2008</span><span class="p">)</span>

<span class="n">speechtext_2009</span> <span class="o">=</span> <span class="n">sotu_2009</span><span class="p">[</span><span class="s1">&#39;speechtext&#39;</span><span class="p">]</span>
<span class="n">text_2009</span> <span class="o">=</span> <span class="n">speechtext_2009</span><span class="o">.</span><span class="n">values</span>
<span class="n">strings_2009</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text_2009</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="n">corpus_2008</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">strings_2008</span><span class="p">)</span>

<span class="n">corpus_2009</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">strings_2009</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us first strip out any extra white space that we do not need by using python’s replace(). Such extra whitespace can easily confuse our word recognition. Extra whitespaces can actually be a real issue in historical documents, as they have often been OCR’ed with imperfect recognition. Look it up! There are many cases described on the web. The Bad Data Handbook - Cleaning Up The Data So You Can Get Back To Work by McCallum and published by O’Reilly is an excellent summary of this other side of big data. The bigger the data the more likely it is also somehow ‘bad’!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_2008</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">corpus_2008</span><span class="p">]</span>

<span class="n">corpus_2009</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">corpus_2009</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>But as we can see the tokenizer also sees punctuation as words, we need to do something about that. Python has a function isalpha(), that checks whether a token conists only of alphabetic lettters. This will also remove all numbers as they do not add to the content of the speeches. And maybe we also don’t want NLTK to distinguish between words with capital and lower letters, so let’s get rid of these as well. This makes it easier to count them. Why?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_2008</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">corpus_2008</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>

<span class="n">corpus_2009</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">corpus_2009</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
</pre></div>
</div>
</div>
</div>
<p>When I said earlier that the most frequently used words carry the meaning of a document, I was not entirely telling the truth. There are so-called stopwords such as the, a, or, etc., which usually carry less meaning than the other expressions in the corpus. You will see what kind of words I mean by checking out NLTK’s stopwords corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="n">stopwords</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="n">stopwords</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;i&#39;,
 &#39;me&#39;,
 &#39;my&#39;,
 &#39;myself&#39;,
 &#39;we&#39;,
 &#39;our&#39;,
 &#39;ours&#39;,
 &#39;ourselves&#39;,
 &#39;you&#39;,
 &quot;you&#39;re&quot;,
 &quot;you&#39;ve&quot;,
 &quot;you&#39;ll&quot;,
 &quot;you&#39;d&quot;,
 &#39;your&#39;,
 &#39;yours&#39;,
 &#39;yourself&#39;,
 &#39;yourselves&#39;,
 &#39;he&#39;,
 &#39;him&#39;,
 &#39;his&#39;,
 &#39;himself&#39;,
 &#39;she&#39;,
 &quot;she&#39;s&quot;,
 &#39;her&#39;,
 &#39;hers&#39;,
 &#39;herself&#39;,
 &#39;it&#39;,
 &quot;it&#39;s&quot;,
 &#39;its&#39;,
 &#39;itself&#39;,
 &#39;they&#39;,
 &#39;them&#39;,
 &#39;their&#39;,
 &#39;theirs&#39;,
 &#39;themselves&#39;,
 &#39;what&#39;,
 &#39;which&#39;,
 &#39;who&#39;,
 &#39;whom&#39;,
 &#39;this&#39;,
 &#39;that&#39;,
 &quot;that&#39;ll&quot;,
 &#39;these&#39;,
 &#39;those&#39;,
 &#39;am&#39;,
 &#39;is&#39;,
 &#39;are&#39;,
 &#39;was&#39;,
 &#39;were&#39;,
 &#39;be&#39;,
 &#39;been&#39;,
 &#39;being&#39;,
 &#39;have&#39;,
 &#39;has&#39;,
 &#39;had&#39;,
 &#39;having&#39;,
 &#39;do&#39;,
 &#39;does&#39;,
 &#39;did&#39;,
 &#39;doing&#39;,
 &#39;a&#39;,
 &#39;an&#39;,
 &#39;the&#39;,
 &#39;and&#39;,
 &#39;but&#39;,
 &#39;if&#39;,
 &#39;or&#39;,
 &#39;because&#39;,
 &#39;as&#39;,
 &#39;until&#39;,
 &#39;while&#39;,
 &#39;of&#39;,
 &#39;at&#39;,
 &#39;by&#39;,
 &#39;for&#39;,
 &#39;with&#39;,
 &#39;about&#39;,
 &#39;against&#39;,
 &#39;between&#39;,
 &#39;into&#39;,
 &#39;through&#39;,
 &#39;during&#39;,
 &#39;before&#39;,
 &#39;after&#39;,
 &#39;above&#39;,
 &#39;below&#39;,
 &#39;to&#39;,
 &#39;from&#39;,
 &#39;up&#39;,
 &#39;down&#39;,
 &#39;in&#39;,
 &#39;out&#39;,
 &#39;on&#39;,
 &#39;off&#39;,
 &#39;over&#39;,
 &#39;under&#39;,
 &#39;again&#39;,
 &#39;further&#39;,
 &#39;then&#39;,
 &#39;once&#39;,
 &#39;here&#39;,
 &#39;there&#39;,
 &#39;when&#39;,
 &#39;where&#39;,
 &#39;why&#39;,
 &#39;how&#39;,
 &#39;all&#39;,
 &#39;any&#39;,
 &#39;both&#39;,
 &#39;each&#39;,
 &#39;few&#39;,
 &#39;more&#39;,
 &#39;most&#39;,
 &#39;other&#39;,
 &#39;some&#39;,
 &#39;such&#39;,
 &#39;no&#39;,
 &#39;nor&#39;,
 &#39;not&#39;,
 &#39;only&#39;,
 &#39;own&#39;,
 &#39;same&#39;,
 &#39;so&#39;,
 &#39;than&#39;,
 &#39;too&#39;,
 &#39;very&#39;,
 &#39;s&#39;,
 &#39;t&#39;,
 &#39;can&#39;,
 &#39;will&#39;,
 &#39;just&#39;,
 &#39;don&#39;,
 &quot;don&#39;t&quot;,
 &#39;should&#39;,
 &quot;should&#39;ve&quot;,
 &#39;now&#39;,
 &#39;d&#39;,
 &#39;ll&#39;,
 &#39;m&#39;,
 &#39;o&#39;,
 &#39;re&#39;,
 &#39;ve&#39;,
 &#39;y&#39;,
 &#39;ain&#39;,
 &#39;aren&#39;,
 &quot;aren&#39;t&quot;,
 &#39;couldn&#39;,
 &quot;couldn&#39;t&quot;,
 &#39;didn&#39;,
 &quot;didn&#39;t&quot;,
 &#39;doesn&#39;,
 &quot;doesn&#39;t&quot;,
 &#39;hadn&#39;,
 &quot;hadn&#39;t&quot;,
 &#39;hasn&#39;,
 &quot;hasn&#39;t&quot;,
 &#39;haven&#39;,
 &quot;haven&#39;t&quot;,
 &#39;isn&#39;,
 &quot;isn&#39;t&quot;,
 &#39;ma&#39;,
 &#39;mightn&#39;,
 &quot;mightn&#39;t&quot;,
 &#39;mustn&#39;,
 &quot;mustn&#39;t&quot;,
 &#39;needn&#39;,
 &quot;needn&#39;t&quot;,
 &#39;shan&#39;,
 &quot;shan&#39;t&quot;,
 &#39;shouldn&#39;,
 &quot;shouldn&#39;t&quot;,
 &#39;wasn&#39;,
 &quot;wasn&#39;t&quot;,
 &#39;weren&#39;,
 &quot;weren&#39;t&quot;,
 &#39;won&#39;,
 &quot;won&#39;t&quot;,
 &#39;wouldn&#39;,
 &quot;wouldn&#39;t&quot;]
</pre></div>
</div>
</div>
</div>
<p>Do you agree that these words do not really carry meaning in English? Stopwords should be removed to concentrate on the most important words. Let us do so by checking for each word in our corpus whether it is in the sopwords corpus.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus_2008</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">corpus_2008</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>

<span class="n">corpus_2009</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">corpus_2009</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let us next prepare a simple word cloud for both the corpora, by joining them. For this we will need to install wordcloud, in your terminal type pip install wordcloud.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">wordcloud</span> <span class="kn">import</span> <span class="n">WordCloud</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">corpus_2008</span><span class="p">)</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">corpus_2009</span><span class="p">)</span>
<span class="n">wordcloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span> 
                <span class="n">background_color</span> <span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span>  
                <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span> 
  
<span class="c1"># plot the WordCloud image                        </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
  
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Text_1_31_0.png" src="../../_images/Text_1_31_0.png" />
</div>
</div>
<p>We can clearly see that America is important in 2008 and in 2009 as well as the people of course. More interesting is probably the comparison of the 2008 speech by Bush with the 2009 speech by Obama. But for these we need to create two different corpuses, let’s take the same steps as before but now create two corpora, one for 2008 and one for 2009.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># corpus_2008 = str(corpus_2008)</span>
<span class="n">wordcloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span> 
                <span class="n">background_color</span> <span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span>  
                <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">corpus_2008</span><span class="p">))</span> 
  
<span class="c1"># plot the WordCloud image                        </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
  
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Text_1_33_0.png" src="../../_images/Text_1_33_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># corpus_2009 = str(corpus_2009)</span>
<span class="n">wordcloud</span> <span class="o">=</span> <span class="n">WordCloud</span><span class="p">(</span><span class="n">width</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">800</span><span class="p">,</span> 
                <span class="n">background_color</span> <span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span>  
                <span class="n">min_font_size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">corpus_2009</span><span class="p">))</span> 
  
<span class="c1"># plot the WordCloud image                        </span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">pad</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> 
  
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Text_1_34_0.png" src="../../_images/Text_1_34_0.png" />
</div>
</div>
<p>We can clearly see that Obama concentrated more on the economy while Bush’s favourite topic was the war on terror. We are quite happy with this insight but we also feel we can do better. Wouldn’t it be nice if we could compare not just words but whole topics across documents in a collection? This is what the advanced technique topic modelling does. Topic Modelling is a popular technique in social and cultural analytics that summarises a collection of texts into a predefined number of topics. Have a look at <a class="reference external" href="http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/">http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/</a>.</p>
<p>Topic modelling is also popular, as it requires only minimal text organisation. Computers can learn topics by themselves. There are, however, known limitations of topic models with regard to the interpretation they help with. There is no guarantee that the automatically derived topics will correspond to what people would consider to be interesting topics/themes. They may be too specific or general, identical to other topics or they may be framings of larger topics, as opposed to genuinely distinct topics. Finally (and in common with other computational analysis techniques), the performance of topic modelling depends upon the quality and structure of the data. In our case the main issue will be that we only have 2 documents, which is not a lot of data. But as topic modelling is computationally quite expensive we should not overdo things here and just concentrate on this small corpus.</p>
<p><a class="reference external" href="https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0">https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0</a> :Topic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:
Dimensionality Reduction, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}
Unsupervised Learning, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight.</p>
<p>To create these topic models, we will create an LDA(Latent Dirichlet Allocation) model, using gensim, first install it using pip install gensim.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We need to install gensim for this</span>
<span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">import</span> <span class="nn">gensim.corpora</span> <span class="k">as</span> <span class="nn">corpora</span>

<span class="n">corpus_89</span> <span class="o">=</span> <span class="p">[</span><span class="n">corpus_2008</span><span class="p">,</span><span class="n">corpus_2009</span><span class="p">]</span>
<span class="n">Dict</span> <span class="o">=</span> <span class="n">corpora</span><span class="o">.</span><span class="n">Dictionary</span><span class="p">(</span><span class="n">corpus_89</span><span class="p">)</span>
<span class="n">texts</span> <span class="o">=</span> <span class="n">corpus_89</span>

<span class="c1"># Term Document Frequency</span>
<span class="n">td</span> <span class="o">=</span> <span class="p">[</span><span class="n">id2word</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">td</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">19</span><span class="o">-</span><span class="mi">783528061878</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># We need to install gensim for this</span>
<span class="ne">----&gt; </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">gensim</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">gensim.corpora</span> <span class="k">as</span> <span class="nn">corpora</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> 
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="n">corpus_89</span> <span class="o">=</span> <span class="p">[</span><span class="n">corpus_2008</span><span class="p">,</span><span class="n">corpus_2009</span><span class="p">]</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;gensim&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">LDA</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">LdaMulticore</span><span class="p">(</span><span class="n">corpus</span><span class="o">=</span><span class="n">td</span><span class="p">,</span>
                                       <span class="n">id2word</span><span class="o">=</span><span class="n">Dict</span><span class="p">,</span>
                                       <span class="n">num_topics</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we want to print the most keywords in the 10 topics</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>

<span class="n">pprint</span><span class="p">(</span><span class="n">LDA</span><span class="o">.</span><span class="n">print_topics</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(0,
  &#39;0.007*&quot;america&quot; + 0.006*&quot;new&quot; + 0.005*&quot;congress&quot; + 0.005*&quot;people&quot; + &#39;
  &#39;0.005*&quot;us&quot; + 0.004*&quot;american&quot; + 0.004*&quot;nation&quot; + 0.004*&quot;must&quot; + &#39;
  &#39;0.004*&quot;health&quot; + 0.003*&quot;every&quot;&#39;),
 (1,
  &#39;0.008*&quot;people&quot; + 0.007*&quot;american&quot; + 0.006*&quot;us&quot; + 0.006*&quot;must&quot; + &#39;
  &#39;0.005*&quot;health&quot; + 0.005*&quot;new&quot; + 0.005*&quot;year&quot; + 0.005*&quot;years&quot; + &#39;
  &#39;0.005*&quot;america&quot; + 0.005*&quot;help&quot;&#39;),
 (2,
  &#39;0.007*&quot;new&quot; + 0.006*&quot;people&quot; + 0.005*&quot;american&quot; + 0.005*&quot;america&quot; + &#39;
  &#39;0.005*&quot;us&quot; + 0.005*&quot;congress&quot; + 0.005*&quot;must&quot; + 0.004*&quot;economy&quot; + &#39;
  &#39;0.003*&quot;health&quot; + 0.003*&quot;future&quot;&#39;),
 (3,
  &#39;0.007*&quot;people&quot; + 0.006*&quot;new&quot; + 0.006*&quot;american&quot; + 0.006*&quot;america&quot; + &#39;
  &#39;0.006*&quot;us&quot; + 0.006*&quot;year&quot; + 0.005*&quot;must&quot; + 0.004*&quot;also&quot; + 0.004*&quot;country&quot; + &#39;
  &#39;0.004*&quot;nation&quot;&#39;),
 (4,
  &#39;0.008*&quot;must&quot; + 0.007*&quot;america&quot; + 0.007*&quot;us&quot; + 0.006*&quot;congress&quot; + &#39;
  &#39;0.006*&quot;people&quot; + 0.005*&quot;new&quot; + 0.005*&quot;american&quot; + 0.005*&quot;year&quot; + &#39;
  &#39;0.005*&quot;help&quot; + 0.005*&quot;also&quot;&#39;),
 (5,
  &#39;0.007*&quot;america&quot; + 0.006*&quot;must&quot; + 0.006*&quot;american&quot; + 0.006*&quot;congress&quot; + &#39;
  &#39;0.005*&quot;people&quot; + 0.005*&quot;new&quot; + 0.004*&quot;us&quot; + 0.004*&quot;iraq&quot; + 0.004*&quot;also&quot; + &#39;
  &#39;0.004*&quot;year&quot;&#39;),
 (6,
  &#39;0.008*&quot;people&quot; + 0.007*&quot;america&quot; + 0.005*&quot;must&quot; + 0.005*&quot;congress&quot; + &#39;
  &#39;0.005*&quot;nation&quot; + 0.005*&quot;american&quot; + 0.005*&quot;us&quot; + 0.005*&quot;year&quot; + 0.004*&quot;new&quot; &#39;
  &#39;+ 0.004*&quot;also&quot;&#39;),
 (7,
  &#39;0.006*&quot;must&quot; + 0.006*&quot;america&quot; + 0.006*&quot;new&quot; + 0.005*&quot;people&quot; + &#39;
  &#39;0.005*&quot;also&quot; + 0.005*&quot;us&quot; + 0.005*&quot;congress&quot; + 0.004*&quot;nation&quot; + &#39;
  &#39;0.004*&quot;country&quot; + 0.004*&quot;iraq&quot;&#39;),
 (8,
  &#39;0.007*&quot;new&quot; + 0.006*&quot;people&quot; + 0.006*&quot;american&quot; + 0.006*&quot;also&quot; + &#39;
  &#39;0.005*&quot;america&quot; + 0.005*&quot;must&quot; + 0.005*&quot;nation&quot; + 0.005*&quot;know&quot; + 0.005*&quot;us&quot; &#39;
  &#39;+ 0.005*&quot;congress&quot;&#39;),
 (9,
  &#39;0.007*&quot;american&quot; + 0.007*&quot;america&quot; + 0.006*&quot;new&quot; + 0.005*&quot;people&quot; + &#39;
  &#39;0.005*&quot;must&quot; + 0.004*&quot;economy&quot; + 0.004*&quot;health&quot; + 0.004*&quot;us&quot; + &#39;
  &#39;0.004*&quot;years&quot; + 0.004*&quot;congress&quot;&#39;)]
</pre></div>
</div>
</div>
</div>
<p>So how do we interpret this? It shows the top 10 keywords that contribute to each topic.</p>
<p>For example the keywords for topic 0 are: ‘america’, ‘new’, ‘congress’.. etc and the weight of ‘america’ on topic 0 is 0.007, it shows how important the keyword is to that topic.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LDA</span><span class="o">.</span><span class="n">show_topic</span><span class="p">(</span><span class="n">topicid</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">topn</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;must&#39;, 0.008085814),
 (&#39;america&#39;, 0.0070124874),
 (&#39;us&#39;, 0.006851775),
 (&#39;congress&#39;, 0.0063907523),
 (&#39;people&#39;, 0.006145501),
 (&#39;new&#39;, 0.0053699724),
 (&#39;american&#39;, 0.0052378536),
 (&#39;year&#39;, 0.0048897862),
 (&#39;help&#39;, 0.004876489),
 (&#39;also&#39;, 0.004811257),
 (&#39;nation&#39;, 0.0045913653),
 (&#39;iraq&#39;, 0.0038902515),
 (&#39;economy&#39;, 0.0035481474),
 (&#39;world&#39;, 0.0033375428),
 (&#39;health&#39;, 0.0033284824),
 (&#39;future&#39;, 0.0032581696),
 (&#39;years&#39;, 0.00321826),
 (&#39;know&#39;, 0.0031786195),
 (&#39;care&#39;, 0.0031698232),
 (&#39;one&#39;, 0.0031569204)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gensim LDA doesn&#39;t easily allow for the comparison of the topic models to each of the documents</span>
<span class="c1"># other options are possible though, but what would be suited for the assignment?</span>
</pre></div>
</div>
</div>
</div>
<p>Not bad. Both word clouds and topic modelling deliver some interesting insights. We now feel confident to explore the whole corpus. In the end, we would like to establish some simple linguistic statistics such as the most frequent words/terms in a collection as well as word trends that tell us about the ups and downs of concepts during the history of policy-making in the USA. Check out <a class="reference external" href="http://stateoftheunion.onetwothree.net/sotuGraph/index.html">http://stateoftheunion.onetwothree.net/sotuGraph/index.html</a> for a visualisation to compare two concepts in the SOTU speeches.</p>
<p>Let us start by creating first a corpus for each speechtext,</p>
<p>Remember the previous steps? Let’s first tokeninze the words</p>
<p>Next, we remove the numbers and whitespaces, please. Do you remember how?</p>
<p>And at last we remove the stopwords</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">clean_text</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">docs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">:</span>
        <span class="n">speech</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="s1">&#39;speechtext&#39;</span><span class="p">]</span>
        <span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">speech</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
        <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">]</span>
        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="s1">&#39;speechtext&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
        <span class="n">docs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">docs</span><span class="p">,</span> <span class="n">df</span>

<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">docs</span><span class="p">,</span> <span class="n">df2</span> <span class="o">=</span> <span class="n">clean_text</span><span class="p">(</span><span class="n">df2</span><span class="p">)</span>
<span class="n">df2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>speechtext</th>
      <th>year</th>
      <th>date</th>
    </tr>
    <tr>
      <th>rownames</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>17900108.html</th>
      <td>['senate', 'house', 'representatives', 'embrac...</td>
      <td>1790</td>
      <td>1790-01-08</td>
    </tr>
    <tr>
      <th>17901208.html</th>
      <td>['senate', 'house', 'representatives', 'meetin...</td>
      <td>1790</td>
      <td>1790-12-08</td>
    </tr>
    <tr>
      <th>17911025.html</th>
      <td>['senate', 'house', 'representatives', 'vain',...</td>
      <td>1791</td>
      <td>1791-10-25</td>
    </tr>
    <tr>
      <th>17921106.html</th>
      <td>['senate', 'house', 'representatives', 'abatem...</td>
      <td>1792</td>
      <td>1792-11-06</td>
    </tr>
    <tr>
      <th>17931203.html</th>
      <td>['senate', 'house', 'representatives', 'since'...</td>
      <td>1793</td>
      <td>1793-12-03</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>20130212.html</th>
      <td>['speaker', 'vice', 'president', 'members', 'c...</td>
      <td>2013</td>
      <td>2013-02-12</td>
    </tr>
    <tr>
      <th>20140128.html</th>
      <td>['speaker', 'vice', 'president', 'members', 'c...</td>
      <td>2014</td>
      <td>2014-01-28</td>
    </tr>
    <tr>
      <th>20150120.html</th>
      <td>['speaker', 'vice', 'president', 'members', 'c...</td>
      <td>2015</td>
      <td>2015-01-20</td>
    </tr>
    <tr>
      <th>20160112.html</th>
      <td>['speaker', 'vice', 'president', 'members', 'c...</td>
      <td>2016</td>
      <td>2016-01-12</td>
    </tr>
    <tr>
      <th>20170228.html</th>
      <td>['thank', 'much', 'speaker', 'vice', 'presiden...</td>
      <td>2017</td>
      <td>2017-02-28</td>
    </tr>
  </tbody>
</table>
<p>231 rows × 3 columns</p>
</div></div></div>
</div>
<p>We are ready to create a document term matrix, we will use tdidf for that.</p>
<p>Sklearn has a nice tool for that, the TfidfVectorizer, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a>
which converts a collection of documents o a matrix of TF-IDF features.</p>
<p>TF-IDF stands for term frequency–inverse document frequency, it shows you how important a word is for a document in a corpus.
(Maybe show the formula here? Do we need more explaining?)</p>
<p>Use the vectorizer to calculate the TF-IDF and then use these values to create a new dataframe, the document term matrix, that shows for each speech (as a column) the word (row) and its tf-idf (value).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="n">tfidf</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">tfidf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;speechtext&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
 
<span class="n">matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">tfidf</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>
 
<span class="n">matrix</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span>
<span class="n">matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>year</th>
      <th>1790</th>
      <th>1790</th>
      <th>1791</th>
      <th>1792</th>
      <th>1793</th>
      <th>1794</th>
      <th>1795</th>
      <th>1796</th>
      <th>1797</th>
      <th>1798</th>
      <th>...</th>
      <th>2008</th>
      <th>2009</th>
      <th>2010</th>
      <th>2011</th>
      <th>2012</th>
      <th>2013</th>
      <th>2014</th>
      <th>2015</th>
      <th>2016</th>
      <th>2017</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>aaa</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>aana</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>aaron</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>abandon</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.024749</td>
      <td>...</td>
      <td>0.009383</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.008981</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.008578</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>abandoned</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.027035</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>zone</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>zones</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>zoological</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>zooming</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>zuloaga</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>...</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>23070 rows × 231 columns</p>
</div></div></div>
</div>
<p>We could also get a word frequency count, which gives a more simple overview by using the same set up but instead we use CountVectorizer</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;speechtext&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">counts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">count</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span>
                      <span class="n">columns</span><span class="o">=</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>

<span class="n">count_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">count</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span>

<span class="n">count_matrix</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span>
<span class="n">count_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>year</th>
      <th>1790</th>
      <th>1790</th>
      <th>1791</th>
      <th>1792</th>
      <th>1793</th>
      <th>1794</th>
      <th>1795</th>
      <th>1796</th>
      <th>1797</th>
      <th>1798</th>
      <th>...</th>
      <th>2008</th>
      <th>2009</th>
      <th>2010</th>
      <th>2011</th>
      <th>2012</th>
      <th>2013</th>
      <th>2014</th>
      <th>2015</th>
      <th>2016</th>
      <th>2017</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>aaa</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>aana</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>aaron</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>abandon</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>...</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>abandoned</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>zone</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>zones</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>zoological</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>zooming</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>zuloaga</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>23070 rows × 231 columns</p>
</div></div></div>
</div>
<p>Well we mainly see a lot of zero values, probably because it the words are sorted in alphabetical order. Is there something we could do about that?</p>
<p>We can, for example, sort it in ascending order for a certain year.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">count_new</span> <span class="o">=</span> <span class="n">count_matrix</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">1791</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span> <span class="p">)</span>
<span class="n">count_new</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>year</th>
      <th>1790</th>
      <th>1790</th>
      <th>1791</th>
      <th>1792</th>
      <th>1793</th>
      <th>1794</th>
      <th>1795</th>
      <th>1796</th>
      <th>1797</th>
      <th>1798</th>
      <th>...</th>
      <th>2008</th>
      <th>2009</th>
      <th>2010</th>
      <th>2011</th>
      <th>2012</th>
      <th>2013</th>
      <th>2014</th>
      <th>2015</th>
      <th>2016</th>
      <th>2017</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>states</th>
      <td>4</td>
      <td>3</td>
      <td>18</td>
      <td>5</td>
      <td>24</td>
      <td>19</td>
      <td>5</td>
      <td>16</td>
      <td>18</td>
      <td>15</td>
      <td>...</td>
      <td>5</td>
      <td>4</td>
      <td>6</td>
      <td>5</td>
      <td>10</td>
      <td>12</td>
      <td>14</td>
      <td>9</td>
      <td>6</td>
      <td>12</td>
    </tr>
    <tr>
      <th>united</th>
      <td>4</td>
      <td>2</td>
      <td>17</td>
      <td>5</td>
      <td>22</td>
      <td>15</td>
      <td>5</td>
      <td>16</td>
      <td>18</td>
      <td>14</td>
      <td>...</td>
      <td>4</td>
      <td>5</td>
      <td>7</td>
      <td>4</td>
      <td>6</td>
      <td>9</td>
      <td>6</td>
      <td>7</td>
      <td>3</td>
      <td>14</td>
    </tr>
    <tr>
      <th>may</th>
      <td>5</td>
      <td>6</td>
      <td>13</td>
      <td>6</td>
      <td>8</td>
      <td>12</td>
      <td>7</td>
      <td>12</td>
      <td>8</td>
      <td>6</td>
      <td>...</td>
      <td>1</td>
      <td>4</td>
      <td>4</td>
      <td>7</td>
      <td>5</td>
      <td>1</td>
      <td>3</td>
      <td>4</td>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>public</th>
      <td>5</td>
      <td>5</td>
      <td>10</td>
      <td>5</td>
      <td>7</td>
      <td>4</td>
      <td>7</td>
      <td>11</td>
      <td>5</td>
      <td>4</td>
      <td>...</td>
      <td>3</td>
      <td>2</td>
      <td>5</td>
      <td>2</td>
      <td>5</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>upon</th>
      <td>0</td>
      <td>0</td>
      <td>9</td>
      <td>11</td>
      <td>3</td>
      <td>8</td>
      <td>5</td>
      <td>2</td>
      <td>5</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>fading</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>fades</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>faded</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>fade</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>zuloaga</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>23070 rows × 231 columns</p>
</div></div></div>
</div>
<p>We can also view only subparts the dataframe and sort for each year seperately and compare them with another year.</p>
<p>Let’s try and do this for 2016 and 2017</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mat_2016</span> <span class="o">=</span> <span class="n">count_matrix</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">2016</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">mat_1617</span> <span class="o">=</span> <span class="n">mat_2016</span><span class="p">[[</span><span class="mi">2016</span><span class="p">,</span> <span class="mi">2017</span><span class="p">]]</span>
<span class="n">mat_1617</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>year</th>
      <th>2016</th>
      <th>2017</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>us</th>
      <td>33</td>
      <td>20</td>
    </tr>
    <tr>
      <th>america</th>
      <td>26</td>
      <td>29</td>
    </tr>
    <tr>
      <th>world</th>
      <td>25</td>
      <td>17</td>
    </tr>
    <tr>
      <th>people</th>
      <td>21</td>
      <td>15</td>
    </tr>
    <tr>
      <th>work</th>
      <td>20</td>
      <td>10</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>farmer</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>farmed</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>farman</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>farm</th>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>zuloaga</th>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>23070 rows × 2 columns</p>
</div></div></div>
</div>
<p>If we want, we could also check the wordcount of each speech by summing up the values of each of the columns, pandas has a good function for that, look it up.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">count_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>year
1790     497
1790     629
1791    1064
1792     961
1793     896
        ... 
2013    3379
2014    3544
2015    3418
2016    2716
2017    2569
Length: 231, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Interesting! If we would run this function on the whole dtm, we could filter out the shortest and longest speeches using the subset, min and max functions. Any idea how?</p>
<p>But we move on, as we are more interested in the words/terms and their frequencies, which will tell us more about the contents of the various speeches.
We could also use the same function to check how many times a certain word occurs in all of the documents, try it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_freq</span> <span class="o">=</span> <span class="n">count_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">word_freq</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>aaa            1
aana           2
aaron          2
abandon       80
abandoned     82
              ..
zone          37
zones         19
zoological     1
zooming        1
zuloaga        5
Length: 23070, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Again, a bit much since there are so many words, let’s have a look at the 10 most frequent ones.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_freq</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>government    6951
states        6475
congress      4996
united        4819
people        4064
upon          3956
year          3899
would         3787
country       3422
may           3407
dtype: int64
</pre></div>
</div>
</div>
</div>
<p>We can calculate the correlation between two words by calculating the correlation between two dataframe columns (this does mean that we have to transpose the datframe first)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">count_matrix</span><span class="o">.</span><span class="n">T</span>

<span class="k">def</span> <span class="nf">find_cors</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">term</span><span class="p">):</span>
    <span class="n">cors</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">c</span><span class="p">:</span>
        <span class="n">corr</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">term</span><span class="p">]</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">column</span><span class="p">])</span>
        <span class="n">cors</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">cors</span>

<span class="n">find_cors</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">term</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;aaa&#39;: -0.034390862711353935,
 &#39;aana&#39;: -0.04660345192937502,
 &#39;aaron&#39;: 0.09838336447525685,
 &#39;abandon&#39;: 0.04441150031372005,
 &#39;abandoned&#39;: -0.10904370137885133,
 &#39;abandoning&#39;: 0.055699483647555885,
 &#39;abandonment&#39;: -0.018739808064903732,
 &#39;abandons&#39;: -0.061723759700299204,
 &#39;abate&#39;: -0.001623104935356706,
 &#39;abated&#39;: -0.07230799445625986,
 &#39;abatement&#39;: -0.083926795584274,
 &#39;abating&#39;: -0.04008766183873756,
 &#39;abbas&#39;: 0.07552244025083649,
 &#39;abbreviation&#39;: -0.03439086271135391,
 &#39;abdicate&#39;: -0.0038593896663010633,
 &#39;abdicated&#39;: -0.04049715732036449,
 &#39;abdicating&#39;: -0.01845156397717592,
 &#39;abdication&#39;: -0.012078428381600346,
 &#39;abducted&#39;: -0.028284568102343286,
 &#39;abduction&#39;: 0.13658538634094228,
 &#39;aberdeen&#39;: -0.04660345192937499,
 &#39;abess&#39;: 0.06330985103281535,
 &#39;abet&#39;: -0.02710600312180059,
 &#39;abetted&#39;: -0.046603451929374866,
 &#39;abetting&#39;: 0.05109726181479416,
 &#39;abettors&#39;: -0.06172375970029923,
 &#39;abeyance&#39;: -0.0890401874443134,
 &#39;abhor&#39;: -0.046603451929375074,
 &#39;abhorrence&#39;: -0.05274560931597226,
 &#39;abhorrent&#39;: 0.029147851318259657,
 &#39;abhors&#39;: -0.04920470964959925,
 &#39;abide&#39;: -0.09802180465810655,
 &#39;abides&#39;: -0.028284568102343286,
 &#39;abideth&#39;: -0.03439086271135386,
 &#39;abiding&#39;: -0.027935100230908606,
 &#39;abilities&#39;: 0.015279594048669827,
 &#39;ability&#39;: 0.08475037681111515,
 &#39;abject&#39;: -0.042122910316853175,
 &#39;abjured&#39;: 0.05720355642380466,
 &#39;ablaze&#39;: -0.016071978884322196,
 &#39;able&#39;: 0.058358386133148636,
 &#39;ablest&#39;: -0.06380346315005443,
 &#39;ably&#39;: -0.08014422863540521,
 &#39;abnormal&#39;: -0.05677756507839409,
 &#39;abnormally&#39;: -0.034390862711353935,
 &#39;aboard&#39;: -0.01967552161993825,
 &#39;abode&#39;: -0.06200535886055213,
 &#39;abodes&#39;: -0.04920470964959923,
 &#39;abolish&#39;: -0.11242498410412251,
 &#39;abolished&#39;: -0.09246926018926736,
 &#39;abolishes&#39;: -0.06605097927261155,
 &#39;abolishing&#39;: -0.04991809643109044,
 &#39;abolishment&#39;: -0.041624731426334265,
 &#39;abolition&#39;: -0.16367643941713123,
 &#39;abominable&#39;: -0.03286538349768473,
 &#39;aboriginal&#39;: -0.09780013283252235,
 &#39;aborigines&#39;: -0.09527071309473385,
 &#39;abortion&#39;: 0.15460540493764702,
 &#39;abortions&#39;: 0.2844538060846866,
 &#39;abortive&#39;: -0.09220022890887637,
 &#39;abound&#39;: -0.07256876889433052,
 &#39;abounding&#39;: -0.07075514809770597,
 &#39;abounds&#39;: -0.0704501076478371,
 &#39;abra&#39;: -0.05274560931597229,
 &#39;abraham&#39;: 0.15447551902858292,
 &#39;abreast&#39;: -0.04700522216293226,
 &#39;abridge&#39;: -0.07753190698058318,
 &#39;abridged&#39;: -0.09074753085301727,
 &#39;abridges&#39;: -0.0404971573203645,
 &#39;abridging&#39;: -0.05739654012798686,
 &#39;abridgment&#39;: -0.02710600312180058,
 &#39;abroad&#39;: -0.0007346869234702268,
 &#39;abrogate&#39;: 0.010578256413850235,
 &#39;abrogated&#39;: -0.08093929459577011,
 &#39;abrogates&#39;: -0.040497157320364414,
 &#39;abrogating&#39;: -0.04049715732036442,
 &#39;abrogation&#39;: -0.06616148232400741,
 &#39;abrupt&#39;: -0.07508919974902767,
 &#39;abruptly&#39;: -0.027829671529748856,
 &#39;absconded&#39;: -0.04049715732036443,
 &#39;absconding&#39;: -0.034390862711353824,
 &#39;absence&#39;: -0.14952254828689862,
 &#39;absent&#39;: -0.010425531771042985,
 &#39;absentee&#39;: -0.049204709649599256,
 &#39;absolute&#39;: -0.13367739686230976,
 &#39;absolutely&#39;: -0.11056826561694899,
 &#39;absolve&#39;: -0.05306932055567454,
 &#39;absolved&#39;: -0.08152776265463854,
 &#39;absolves&#39;: -0.04049715732036442,
 &#39;absorb&#39;: -0.13079705072264258,
 &#39;absorbed&#39;: -0.09913691100523492,
 &#39;absorbent&#39;: -0.003859389666301052,
 &#39;absorbing&#39;: -0.030744921121652415,
 &#39;absorbs&#39;: -0.0487421009833622,
 &#39;absorption&#39;: -0.06564284098098352,
 &#39;abstain&#39;: -0.09363823521329169,
 &#39;abstained&#39;: -0.04422911271743675,
 &#39;abstaining&#39;: -0.0885973696713559,
 &#39;abstention&#39;: -0.0038593896663010603,
 &#39;abstinence&#39;: 0.08815502577522079,
 &#39;abstract&#39;: -0.10385218617869166,
 &#39;abstracted&#39;: 0.008353199551720059,
 &#39;abstracting&#39;: -0.02828456810234329,
 &#39;abstraction&#39;: 0.011838973029010326,
 &#39;abstractly&#39;: -0.02217827349333275,
 &#39;abstracts&#39;: -0.05274560931597222,
 &#39;absurd&#39;: -0.05056014236787161,
 &#39;absurdity&#39;: -0.03446932342686263,
 &#39;abundance&#39;: -0.016995308019368893,
 &#39;abundant&#39;: -0.23115479055692947,
 &#39;abundantly&#39;: 0.011807071082048143,
 &#39;abuse&#39;: 0.08248761317271718,
 &#39;abused&#39;: -0.015030381853044355,
 &#39;abuses&#39;: -0.09258789560177257,
 &#39;abusing&#39;: 0.003908785345996146,
 &#39;abusive&#39;: 0.2954141358914447,
 &#39;abutting&#39;: -0.028284568102343328,
 &#39;abyss&#39;: -0.04049715732036442,
 &#39;abyssinia&#39;: -0.0404971573203645,
 &#39;academia&#39;: 0.13658538634094228,
 &#39;academic&#39;: 0.03045601182735768,
 &#39;academies&#39;: -0.0038593896663010633,
 &#39;academy&#39;: -0.1116056724498217,
 &#39;acapulco&#39;: -0.022178273493332984,
 &#39;accede&#39;: -0.07894346869152287,
 &#39;acceded&#39;: -0.09721724442453444,
 &#39;acceding&#39;: -0.038582010650480135,
 &#39;accelerate&#39;: 0.2553869849515366,
 &#39;accelerated&#39;: -0.004190813651087767,
 &#39;accelerates&#39;: -0.009965684275311634,
 &#39;accelerating&#39;: 0.05460357309014205,
 &#39;acceleration&#39;: -0.008998045909196513,
 &#39;accents&#39;: -0.04008766183873756,
 &#39;accentuate&#39;: -0.03576044226642524,
 &#39;accentuated&#39;: -0.06336830831509133,
 &#39;accentuating&#39;: -0.022778783549488247,
 &#39;accept&#39;: 0.038997875454583844,
 &#39;acceptability&#39;: -0.009965684275311614,
 &#39;acceptable&#39;: -0.0827344182459625,
 &#39;acceptance&#39;: -0.13719865841054224,
 &#39;accepted&#39;: -0.19940501695937718,
 &#39;accepting&#39;: -0.09832681998494261,
 &#39;accepts&#39;: -0.014598859236297463,
 &#39;access&#39;: 0.33828024389971795,
 &#39;accessibility&#39;: 0.03780229046288432,
 &#39;accessible&#39;: 0.02030600922094762,
 &#39;accessing&#39;: 0.1487979755589635,
 &#39;accession&#39;: -0.08047549946681197,
 &#39;accessions&#39;: -0.028284568102343293,
 &#39;accessories&#39;: -0.03143322269411292,
 &#39;accessory&#39;: -0.055538827642953904,
 &#39;accident&#39;: 0.07030109520930013,
 &#39;accidental&#39;: 0.04393844093952542,
 &#39;accidentally&#39;: -0.05739654012798685,
 &#39;accidents&#39;: -0.02322412203113514,
 &#39;acclaim&#39;: 0.06567582646118683,
 &#39;acclaimed&#39;: -0.04660345192937506,
 &#39;acclaims&#39;: -0.034390862711353914,
 &#39;acclimated&#39;: -0.0343908627113539,
 &#39;accomac&#39;: -0.046603451929375,
 &#39;accommodate&#39;: -0.047963368944676475,
 &#39;accommodated&#39;: -0.08517092316781597,
 &#39;accommodates&#39;: -0.009965684275311635,
 &#39;accommodating&#39;: -0.0902117848772099,
 &#39;accommodation&#39;: -0.18980523147324857,
 &#39;accommodations&#39;: -0.12744586001220032,
 &#39;accompanied&#39;: -0.145425098049873,
 &#39;accompanies&#39;: -0.10587934539348343,
 &#39;accompany&#39;: -0.03434111528115189,
 &#39;accompanying&#39;: -0.20265868976645054,
 &#39;accomplish&#39;: -0.12279926600987368,
 &#39;accomplished&#39;: -0.22358145837945048,
 &#39;accomplishes&#39;: -0.06616148232400738,
 &#39;accomplishing&#39;: -0.12039923601265026,
 &#39;accomplishment&#39;: -0.17309299438323175,
 &#39;accomplishments&#39;: 0.1403852180503252,
 &#39;accord&#39;: -0.09076332308168641,
 &#39;accordance&#39;: -0.16540729251889755,
 &#39;accorded&#39;: -0.146287258170036,
 &#39;according&#39;: -0.23815312974603312,
 &#39;accordingly&#39;: -0.1641052752368523,
 &#39;accords&#39;: 0.026036199659779283,
 &#39;account&#39;: -0.19167743765672637,
 &#39;accountabilities&#39;: -0.04660345192937489,
 &#39;accountability&#39;: 0.13333887189175941,
 &#39;accountable&#39;: 0.32450318475897905,
 &#39;accountants&#39;: 0.19720372161240765,
 &#39;accounted&#39;: 0.0293715694780289,
 &#39;accounting&#39;: 0.06221200017027517,
 &#39;accounts&#39;: 0.10563040698570077,
 &#39;accoutrements&#39;: 0.007511753456698004,
 &#39;accredit&#39;: -0.05839608954097942,
 &#39;accredited&#39;: -0.1238086658605441,
 &#39;accrediting&#39;: -0.05274560931597226,
 &#39;accretions&#39;: -0.009965684275311614,
 &#39;accrue&#39;: -0.10871054362016894,
 &#39;accrued&#39;: -0.12777523323422715,
 &#39;accrues&#39;: -0.035760442266425246,
 &#39;accruing&#39;: -0.15575911410141294,
 &#39;accumulate&#39;: -0.03451535076787472,
 &#39;accumulated&#39;: -0.15202231500410762,
 &#39;accumulates&#39;: 0.00036788567962315564,
 &#39;accumulating&#39;: -0.13815681128013443,
 &#39;accumulation&#39;: -0.22170597396916575,
 &#39;accumulations&#39;: -0.10456169301495907,
 &#39;accuracy&#39;: -0.08290711424599816,
 &#39;accurate&#39;: -0.09050803626846399,
 &#39;accurately&#39;: -0.06325813009155554,
 &#39;accurs&#39;: -0.04049715732036448,
 &#39;accusation&#39;: -0.09336440756906789,
 &#39;accusations&#39;: -0.08767427478689094,
 &#39;accuse&#39;: -0.04874210098336221,
 &#39;accused&#39;: -0.11034671644828126,
 &#39;accuser&#39;: -0.04049715732036442,
 &#39;accusers&#39;: -0.02828456810234331,
 &#39;accustom&#39;: 0.03347507089057198,
 &#39;accustomed&#39;: -0.12597961271400923,
 &#39;accustoming&#39;: -0.040497157320364414,
 &#39;ace&#39;: -0.04660345192937506,
 &#39;acheen&#39;: -0.046603451929375,
 &#39;achievable&#39;: 0.28111645314825556,
 &#39;achieve&#39;: 0.37650858840580315,
 &#39;achieveable&#39;: 0.11216020790489994,
 &#39;achieved&#39;: 0.12046944154260339,
 &#39;achievement&#39;: 0.2610086085337113,
 &#39;achievements&#39;: 0.07354475490092767,
 &#39;achieves&#39;: -0.04049715732036439,
 &#39;achieving&#39;: 0.1054238075134238,
 &#39;aching&#39;: 0.14165556019838005,
 &#39;acid&#39;: 0.24565140970189348,
 &#39;acknowledge&#39;: 0.008151894872040868,
 &#39;acknowledged&#39;: -0.19701167153215224,
 &#39;acknowledgement&#39;: 0.11216020790489994,
 &#39;acknowledgements&#39;: -0.06605097927261158,
 &#39;acknowledges&#39;: -0.04441488141104988,
 &#39;acknowledging&#39;: -0.004517135817509084,
 &#39;acknowledgment&#39;: -0.12929412661362683,
 &#39;acknowledgments&#39;: -0.11692767707919971,
 &#39;acquaint&#39;: -0.06336830831509131,
 &#39;acquaintance&#39;: -0.06201865457038211,
 &#39;acquainted&#39;: -0.13513144868560437,
 &#39;acquainting&#39;: -0.04874210098336223,
 &#39;acquaints&#39;: -0.02828456810234331,
 &#39;acquiesce&#39;: -0.05841897740875961,
 &#39;acquiesced&#39;: -0.09613138502958336,
 &#39;acquiescence&#39;: -0.15306940834228508,
 &#39;acquiescently&#39;: -0.046603451929375,
 &#39;acquiesces&#39;: -0.04660345192937502,
 &#39;acquiescing&#39;: -0.040497157320364394,
 &#39;acquire&#39;: -0.089266581766476,
 &#39;acquired&#39;: -0.15036752189501915,
 &#39;acquirement&#39;: -0.05274560931597226,
 &#39;acquirements&#39;: -0.05628650898234526,
 &#39;acquires&#39;: -0.04920470964959907,
 &#39;acquiring&#39;: -0.11720873488362493,
 &#39;acquisition&#39;: -0.136076053018389,
 &#39;acquisitions&#39;: -0.045954306795353055,
 &#39;acquit&#39;: -0.057396540127986895,
 &#39;acquittal&#39;: -0.04008766183873757,
 &#39;acquittance&#39;: -0.04049715732036445,
 &#39;acquitted&#39;: -0.07004833803963345,
 &#39;acre&#39;: -0.06950374160339984,
 &#39;acreage&#39;: -0.04588070625702972,
 &#39;acres&#39;: -0.12264407478717451,
 &#39;acrimonious&#39;: -0.028284568102343335,
 &#39;across&#39;: 0.4995490923584838,
 &#39;act&#39;: -0.038824315744693826,
 &#39;acted&#39;: 0.06282162109276393,
 &#39;acting&#39;: -0.11697409758619215,
 &#39;action&#39;: -0.08329407696460026,
 &#39;actions&#39;: 0.20227904541215283,
 &#39;activated&#39;: 0.038884672596772994,
 &#39;active&#39;: -0.11065809630333745,
 &#39;actively&#39;: -0.012419137821984921,
 &#39;activist&#39;: 0.27655805965671937,
 &#39;activities&#39;: -0.004836024144748257,
 &#39;activity&#39;: -0.1533914705470879,
 &#39;actor&#39;: -0.0038593896663010603,
 &#39;actors&#39;: -0.007922522857217673,
 &#39;acts&#39;: -0.2294802389309606,
 &#39;actual&#39;: -0.24133781816417585,
 &#39;actualities&#39;: -0.05628650898234527,
 &#39;actually&#39;: 0.03947769233242677,
 &#39;actuarial&#39;: -0.034390862711353935,
 &#39;actuaries&#39;: -0.02828456810234335,
 &#39;actuate&#39;: -0.04566380998322621,
 &#39;actuated&#39;: -0.10933636707156287,
 &#39;actuates&#39;: 0.00835319955172007,
 &#39;acute&#39;: -0.05895222699126287,
 &#39;acutely&#39;: -0.000871982458950652,
 &#39;ad&#39;: -0.07752376773320911,
 &#39;adam&#39;: -0.046603451929375,
 &#39;adams&#39;: -0.07402653092933147,
 &#39;adapt&#39;: 0.04244166654209348,
 &#39;adaptabilities&#39;: -0.009965684275311623,
 &#39;adaptability&#39;: -0.07349088192044949,
 &#39;adaptable&#39;: -0.038582010650480114,
 &#39;adaptation&#39;: -0.0850788431628133,
 &#39;adaptations&#39;: -0.03439086271135386,
 &#39;adapted&#39;: -0.20517978171450763,
 &#39;adapting&#39;: -0.07686170946803107,
 &#39;adapts&#39;: -0.03439086271135385,
 &#39;add&#39;: -0.1029485693741929,
 &#39;added&#39;: -0.10613140231389286,
 &#39;addict&#39;: 0.13732834062606764,
 &#39;addicted&#39;: 0.3095713922174365,
 &#39;addiction&#39;: 0.25163012537094814,
 &#39;addicts&#39;: 0.09243127700532193,
 &#39;adding&#39;: 0.11388263430906398,
 &#39;addition&#39;: -0.07016519604054874,
 &#39;additional&#39;: -0.11555376991159583,
 &#39;additionally&#39;: 0.04212951003519664,
 &#39;additions&#39;: -0.1495145974481331,
 &#39;additives&#39;: -0.009965684275311635,
 &#39;address&#39;: 0.46241389038184677,
 &#39;addressed&#39;: -0.0891928272893429,
 &#39;addresses&#39;: 0.0023136209067496113,
 &#39;addressing&#39;: 0.08057305195341281,
 &#39;adds&#39;: -0.009117700336068929,
 &#39;adduce&#39;: 0.020565788769741185,
 &#39;adduced&#39;: -0.08656864321538113,
 &#39;adepts&#39;: -0.0343908627113539,
 &#39;adequacy&#39;: -0.02230001977936334,
 &#39;adequate&#39;: -0.19911996960181455,
 &#39;adequately&#39;: -0.07735329239261896,
 &#39;adhere&#39;: -0.12898017698967082,
 &#39;adhered&#39;: -0.09223523485749373,
 &#39;adherence&#39;: -0.155508936844302,
 &#39;adherents&#39;: -0.07464290247762148,
 &#39;adheres&#39;: -0.0720680986571549,
 &#39;adhering&#39;: -0.0827401673734354,
 &#39;adhesion&#39;: -0.07230799445625986,
 &#39;adjacent&#39;: -0.13813524284790218,
 &#39;adjoining&#39;: -0.16342716524449136,
 &#39;adjourn&#39;: -0.06308822625788121,
 &#39;adjourned&#39;: -0.11192269987364507,
 &#39;adjournment&#39;: -0.17272893819443896,
 &#39;adjourns&#39;: -0.05306932055567455,
 &#39;adjudged&#39;: -0.09759881259596873,
 &#39;adjudicate&#39;: -0.07482297715952173,
 &#39;adjudicated&#39;: -0.05694171412562876,
 &#39;adjudicating&#39;: -0.035760442266425246,
 &#39;adjudication&#39;: -0.17107019292799058,
 &#39;adjudications&#39;: -0.0617237597002992,
 &#39;adjunct&#39;: -0.025235164116918715,
 &#39;adjure&#39;: 0.00835319955172007,
 &#39;adjust&#39;: -0.11598679502345705,
 &#39;adjustable&#39;: -0.016071978884322196,
 &#39;adjusted&#39;: -0.17594200160498996,
 &#39;adjusting&#39;: -0.17139779097273186,
 &#39;adjustment&#39;: -0.2597895778437629,
 &#39;adjustments&#39;: -0.05841914451383842,
 &#39;adjusts&#39;: -0.0617237597002992,
 &#39;adjutant&#39;: -0.028284568102343335,
 &#39;adlai&#39;: -0.034390862711353894,
 &#39;administer&#39;: -0.13099304573919393,
 &#39;administered&#39;: -0.11450296112529111,
 &#39;administering&#39;: -0.03185041928977907,
 &#39;administers&#39;: -0.0404971573203645,
 &#39;administration&#39;: 0.074143886937578,
 &#39;administrations&#39;: 0.06321335522445962,
 &#39;administrative&#39;: -0.12586531386074168,
 &#39;administratively&#39;: 0.09710911999545188,
 &#39;administrator&#39;: 0.05256808538245037,
 &#39;administrators&#39;: -0.010425531771042992,
 &#39;admirable&#39;: -0.02807848779028582,
 &#39;admirably&#39;: -0.011148918582296974,
 &#39;admiral&#39;: -0.04146643169963332,
 &#39;admirals&#39;: 0.02293457969786508,
 &#39;admiralty&#39;: -0.05694171412562874,
 &#39;admiration&#39;: -0.07199183718249003,
 &#39;admire&#39;: 0.1490856716672902,
 &#39;admired&#39;: -0.03134497552721064,
 &#39;admissible&#39;: -0.058866600549547295,
 &#39;admission&#39;: -0.14764274301628255,
 &#39;admissions&#39;: 0.09372051433353448,
 &#39;admit&#39;: -0.17402504732265303,
 &#39;admits&#39;: -0.09403514146775048,
 &#39;admittance&#39;: -0.045663809983226195,
 &#39;admitted&#39;: -0.20955723260585515,
 &#39;admittedly&#39;: -0.0384108563171492,
 &#39;admitting&#39;: -0.12691337338916656,
 &#39;admixed&#39;: -0.0038593896663010603,
 &#39;admonish&#39;: -0.1250972253381886,
 &#39;admonished&#39;: -0.11882542088170125,
 &#39;admonishes&#39;: -0.06308822625788121,
 &#39;admonishing&#39;: -0.046603451929374866,
 &#39;admonition&#39;: -0.06616148232400745,
 &#39;admonitions&#39;: -0.08308761266662221,
 &#39;admonitory&#39;: 0.01616619260132265,
 &#39;adnan&#39;: 0.18543574321302703,
 &#39;adolescent&#39;: 0.11216020790489994,
 &#39;adolescents&#39;: 0.11216020790489994,
 &#39;adolfo&#39;: 0.11216020790489994,
 &#39;adopt&#39;: -0.07077252520543599,
 &#39;adopted&#39;: -0.24458215951890397,
 &#39;adopting&#39;: -0.021855254707647438,
 &#39;adoption&#39;: -0.15412998664811162,
 &#39;adoptions&#39;: 0.18543574321302703,
 &#39;adoptive&#39;: 0.21089107335537716,
 &#39;adopts&#39;: -0.0011426856879266192,
 &#39;adore&#39;: -0.0282845681023433,
 &#39;adorn&#39;: -0.07845450658851236,
 &#39;adorned&#39;: -0.04874210098336225,
 &#39;adornment&#39;: -0.04049715732036443,
 &#39;adrianople&#39;: -0.046603451929374956,
 &#39;adriatic&#39;: -0.04660345192937495,
 &#39;adrift&#39;: -0.027106003121800577,
 &#39;adriondacks&#39;: -0.028284568102343328,
 &#39;adroit&#39;: -0.046603451929374935,
 &#39;ads&#39;: 0.19358219506612795,
 &#39;adult&#39;: 0.16999781659654892,
 &#39;adulterated&#39;: -0.028284568102343328,
 &#39;adulteration&#39;: -0.07928484903094773,
 &#39;adulthood&#39;: 0.07552244025083649,
 &#39;adults&#39;: 0.30527137581097497,
 &#39;advance&#39;: -0.015121454180356406,
 &#39;advanced&#39;: -0.022110973936125107,
 &#39;advancement&#39;: -0.16441867647624872,
 &#39;advances&#39;: 0.06394469407263778,
 &#39;advancing&#39;: -0.15750888013474793,
 &#39;advantage&#39;: -0.2154152951881271,
 &#39;advantageous&#39;: -0.20272739546538615,
 &#39;advantageously&#39;: -0.16032782617548988,
 &#39;advantages&#39;: -0.2932404374703602,
 &#39;advent&#39;: -0.03857688633132648,
 &#39;adventitious&#39;: -0.003859389666301061,
 &#39;adventure&#39;: -0.1030510450015463,
 &#39;adventurer&#39;: -0.053069320555674565,
 &#39;adventurers&#39;: -0.0742109165148349,
 &#39;adventures&#39;: 0.007449685012369185,
 &#39;adventuring&#39;: -0.04660345192937496,
 &#39;adventurous&#39;: -0.09525264658660426,
 &#39;adversaries&#39;: 0.2058952608724631,
 &#39;adversary&#39;: 0.014630274499767776,
 &#39;adverse&#39;: -0.16167996372410154,
 &#39;adversely&#39;: -0.020877512318614976,
 &#39;adversity&#39;: 0.2616483300506454,
 &#39;advert&#39;: -0.11278428162648575,
 &#39;adverted&#39;: -0.12450236664548825,
 &#39;adverting&#39;: -0.126705045461484,
 &#39;advertise&#39;: -0.05739654012798687,
 &#39;advertised&#39;: -0.034390862711353894,
 &#39;advertisement&#39;: -0.0968775656559831,
 &#39;advertisements&#39;: -0.05162691403157038,
 &#39;advertising&#39;: 0.09376038105365835,
 &#39;advice&#39;: -0.008505441540737985,
 &#39;advices&#39;: -0.10536451430039685,
 &#39;advisability&#39;: -0.022034727908203378,
 &#39;advisable&#39;: -0.24069359094276438,
 &#39;advise&#39;: -0.10334698370184617,
 &#39;advised&#39;: -0.13717539324959713,
 &#39;advisedly&#39;: 0.03888467259677303,
 &#39;advisement&#39;: -0.0404971573203644,
 &#39;adviser&#39;: -0.004031169160335222,
 &#39;advisers&#39;: 0.05772068697834175,
 &#39;advises&#39;: -0.07905423291667456,
 &#39;advising&#39;: -0.06380346315005443,
 &#39;advisors&#39;: -0.02828456810234335,
 &#39;advisory&#39;: 0.07227210972291949,
 &#39;advocacy&#39;: 0.14281242452745824,
 &#39;advocate&#39;: 0.06714329598308298,
 &#39;advocated&#39;: -0.07378968672844007,
 &#39;advocates&#39;: -0.10435697376662201,
 &#39;advocating&#39;: -0.015471052906012712,
 &#39;aegis&#39;: -0.05739654012798689,
 &#39;aerial&#39;: -0.005469905260238964,
 &#39;aero&#39;: 0.038884672596772994,
 &#39;aeronautical&#39;: -0.02828456810234335,
 &#39;aeronautics&#39;: 0.005238588371792483,
 &#39;aerospace&#39;: 0.03888467259677302,
 &#39;afar&#39;: -0.028284568102343328,
 &#39;affair&#39;: -0.06934513834826532,
 &#39;affairs&#39;: -0.3071009140795062,
 &#39;affect&#39;: -0.03218006341173738,
 &#39;affectation&#39;: -0.04660345192937502,
 &#39;affected&#39;: -0.15388347315072284,
 &#39;affecting&#39;: -0.1828208631546034,
 &#39;affection&#39;: -0.09850547536823444,
 &#39;affectionate&#39;: -0.11558691264440987,
 &#39;affections&#39;: -0.13423236943922867,
 &#39;affects&#39;: -0.10946960987923449,
 &#39;affidavits&#39;: -0.046603451929375,
 &#39;affiliate&#39;: 0.11216020790489994,
 &#39;affiliated&#39;: 0.04088191993988535,
 &#39;affiliates&#39;: 0.2941469140063101,
 &#39;affiliation&#39;: 0.07414412364602568,
 &#39;affiliations&#39;: -0.02087751231861503,
 &#39;affinity&#39;: -0.04660345192937499,
 &#39;affirm&#39;: 0.16505329218199355,
 &#39;affirmation&#39;: -0.04049715732036442,
 &#39;affirmative&#39;: -0.037866626632002846,
 &#39;affirmatively&#39;: -0.04451931363348657,
 &#39;affirmed&#39;: -0.07772925181686408,
 &#39;affirming&#39;: -0.026209153464366506,
 &#39;affirms&#39;: 0.11208736951131355,
 &#39;affix&#39;: -0.0444148814110499,
 &#39;affixed&#39;: -0.005469905260238978,
 &#39;affixing&#39;: -0.03439086271135382,
 &#39;afflict&#39;: -0.06336830831509128,
 &#39;afflicted&#39;: -0.018097647851842168,
 &#39;afflicting&#39;: -0.08767427478689105,
 &#39;affliction&#39;: -0.07482297715952171,
 &#39;afflictions&#39;: 0.01681643146140058,
 &#39;afflictive&#39;: -0.009965684275311604,
 &#39;affluence&#39;: -0.027106003121800584,
 &#39;affluent&#39;: -0.04874210098336224,
 &#39;afford&#39;: 0.02842927087517525,
 &#39;affordability&#39;: 0.11826650251391065,
 &#39;affordable&#39;: 0.5597386738538624,
 &#39;afforded&#39;: -0.2577168237013339,
 &#39;affording&#39;: -0.23095255354461336,
 &#39;affords&#39;: -0.2027808236988503,
 &#39;affront&#39;: 0.06809282746907053,
 &#39;affronts&#39;: 0.10605391329588931,
 &#39;afghan&#39;: 0.35901021220540374,
 &#39;afghanistan&#39;: 0.44199770873138516,
 &#39;afghanistanafter&#39;: 0.05109726181479416,
 &#39;afghanistans&#39;: 0.05109726181479416,
 &#39;afghans&#39;: 0.1988358882959683,
 &#39;afield&#39;: -0.040497157320364484,
 &#39;afloat&#39;: -0.02709226268134405,
 &#39;afognak&#39;: -0.0038593896663010603,
 &#39;aforesaid&#39;: -0.03982830332767419,
 &#39;afraid&#39;: 0.21973132591787548,
 &#39;afresh&#39;: 0.00835319955172007,
 &#39;africa&#39;: 0.19106117872588674,
 &#39;african&#39;: 0.08069545668639562,
 &#39;africanize&#39;: -0.04049715732036442,
 &#39;africans&#39;: 0.04332535531731355,
 &#39;aftermath&#39;: -0.04535276335902605,
 &#39;afternoon&#39;: 0.12903803201562408,
 &#39;afterschool&#39;: 0.18543574321302703,
 &#39;afterward&#39;: 0.11569224276450607,
 &#39;afterwards&#39;: -0.12706399398904109,
 &#39;age&#39;: 0.23856377369014392,
 &#39;aged&#39;: -0.03893551441162596,
 &#39;agencies&#39;: 0.04121785616556569,
 &#39;agency&#39;: -0.058368361784121955,
 &#39;agenda&#39;: 0.6082575204854216,
 &#39;agent&#39;: -0.18349804997818162,
 &#39;agents&#39;: -0.18720529830642635,
 &#39;ages&#39;: -0.011885036501618724,
 &#39;aggrandize&#39;: -0.06172375970029925,
 &#39;aggrandizement&#39;: -0.10777103079678556,
 &#39;aggrandizing&#39;: -0.046603451929374935,
 &#39;aggravate&#39;: -0.09001685015989218,
 &#39;aggravated&#39;: -0.10685069291464197,
 &#39;aggravates&#39;: -0.01607197888432222,
 &#39;aggravating&#39;: -0.059827408648718154,
 &#39;aggravation&#39;: -0.0562865089823452,
 &#39;aggravations&#39;: -0.06605097927261157,
 &#39;aggregate&#39;: -0.21050036174414538,
 &#39;aggregated&#39;: -0.08271449169950994,
 &#39;aggregates&#39;: -0.07230799445625986,
 &#39;aggregating&#39;: -0.055292347764639926,
 &#39;aggregation&#39;: -0.07000095592429516,
 &#39;aggregations&#39;: -0.0723219292009583,
 &#39;aggression&#39;: 0.01495303573273297,
 &#39;aggressions&#39;: -0.14498470319408463,
 &#39;aggressive&#39;: 0.11332815250771752,
 &#39;aggressively&#39;: 0.27187706715765525,
 &#39;aggressiveness&#39;: -0.05739654012798685,
 &#39;aggressor&#39;: -0.05482892631779057,
 &#39;aggressors&#39;: -0.013253552369331343,
 &#39;aggrieved&#39;: -0.03723701452251055,
 &#39;agile&#39;: 0.15490427016797412,
 &#39;aging&#39;: 0.33311565447050606,
 &#39;agitate&#39;: -0.010254813319495928,
 &#39;agitated&#39;: -0.11685551473675562,
 &#39;agitates&#39;: -0.03439086271135382,
 &#39;agitating&#39;: -0.050029070638220194,
 &#39;agitation&#39;: -0.079498249976416,
 &#39;agitations&#39;: -0.05002907063822025,
 &#39;agitator&#39;: 0.0023928606557705643,
 &#39;agitators&#39;: 0.041886609742579385,
 &#39;ago&#39;: 0.4953305117512718,
 &#39;agonies&#39;: 0.09384132407786816,
 &#39;agonizing&#39;: 0.026672083378751845,
 &#39;agony&#39;: 0.04023576203443475,
 &#39;agree&#39;: 0.3516387402632271,
 &#39;agreeable&#39;: -0.1134597014084035,
 &#39;agreeably&#39;: -0.09021178487720974,
 &#39;agreed&#39;: -0.09376175623369173,
 &#39;agreeing&#39;: 0.08558878746438893,
 &#39;agreement&#39;: 0.12998379129779072,
 &#39;agreements&#39;: 0.10974838037803318,
 &#39;agrees&#39;: 0.010578256413850117,
 &#39;agribusinesses&#39;: 0.06330985103281535,
 &#39;agricultural&#39;: -0.13169132613930148,
 &#39;agriculture&#39;: -0.16702246044744695,
 &#39;agriculturist&#39;: -0.10623223915019651,
 &#39;agriculturists&#39;: -0.06954913878284144,
 &#39;agua&#39;: -0.028284568102343335,
 &#39;ague&#39;: -0.01607197888432219,
 &#39;ah&#39;: -0.04049715732036449,
 &#39;ahead&#39;: 0.4003497463135444,
 &#39;ai&#39;: 0.11216020790490007,
 &#39;aid&#39;: -0.17794041288722667,
 &#39;aidbut&#39;: 0.05109726181479416,
 &#39;aided&#39;: -0.18780381238374771,
 &#39;aiding&#39;: -0.1035991107929822,
 &#39;aids&#39;: 0.2795596156454281,
 &#39;ailing&#39;: 0.23827964485811143,
 &#39;ailments&#39;: -0.003859389666301066,
 &#39;ails&#39;: 0.14269168094995288,
 &#39;aim&#39;: 0.06777259881351651,
 &#39;aimed&#39;: 0.012666287788303334,
 &#39;aiming&#39;: -0.050737740772875906,
 &#39;aimless&#39;: -0.004696361001482964,
 &#39;aims&#39;: 0.07019772988095975,
 &#39;ainsworth&#39;: -0.03439086271135389,
 &#39;air&#39;: 0.25075294972054146,
 &#39;aircraft&#39;: 0.05992547499233978,
 &#39;aires&#39;: -0.03985273513793337,
 &#39;airfares&#39;: 0.020565788769741272,
 &#39;airlift&#39;: 0.07520829671779887,
 &#39;airlifting&#39;: 0.11216020790489994,
 &#39;airline&#39;: 0.20422820223531032,
 &#39;airlines&#39;: 0.23172557004191466,
 &#39;airman&#39;: 0.08162873485984708,
 &#39;airmen&#39;: 0.22819995164462648,
 &#39;airplane&#39;: 0.04421427478555457,
 &#39;airplanes&#39;: -0.025536629154981883,
 &#39;airport&#39;: 0.05919102726378054,
 &#39;airports&#39;: 0.12371514166816376,
 &#39;airpower&#39;: -0.009797124832551287,
 &#39;airsheds&#39;: 0.002246904942709505,
 &#39;airwaves&#39;: 0.2718966849498769,
 &#39;airway&#39;: -0.04566380998322618,
 &#39;airways&#39;: -0.03982830332767416,
 &#39;aisle&#39;: 0.3421170947532392,
 &#39;akin&#39;: -0.03074492112165238,
 &#39;al&#39;: 0.38948416757652865,
 &#39;alabama&#39;: -0.11394895141999677,
 &#39;alabamas&#39;: -0.04660345192937492,
 &#39;alacrity&#39;: -0.13834688782725893,
 &#39;alamein&#39;: -0.022178273493332786,
 &#39;alamogordo&#39;: -0.028284568102343467,
 &#39;alan&#39;: 0.20271710952145275,
 &#39;alarm&#39;: -0.09404789100031681,
 &#39;alarmed&#39;: -0.08517092316781599,
 &#39;alarming&#39;: -0.05016597492889596,
 &#39;alarmingly&#39;: -0.05739654012798685,
 &#39;alarmist&#39;: -0.02828456810234338,
 &#39;alarmists&#39;: -0.016071978884322196,
 &#39;alarms&#39;: -0.039009556628752856,
 &#39;alaska&#39;: -0.05405317877530693,
 &#39;alaskan&#39;: -0.1015750458367904,
 &#39;albania&#39;: -0.028284568102343335,
 &#39;albert&#39;: 0.04447573605653663,
 &#39;alcibiades&#39;: -0.0343908627113539,
 &#39;alcohol&#39;: 0.018142397079686713,
 &#39;alert&#39;: 0.0070579249376875606,
 &#39;alertness&#39;: -0.024418411984988033,
 &#39;aleshire&#39;: -0.028284568102343335,
 &#39;aleutian&#39;: -0.03439086271135389,
 &#39;alexander&#39;: -0.030352997330680052,
 &#39;alexandria&#39;: -0.08584249116898908,
 &#39;alfred&#39;: -0.017336612652242005,
 &#39;algeciras&#39;: 0.00835319955172007,
 &#39;algeria&#39;: 0.08508718494635106,
 &#39;algerine&#39;: -0.046603451929374935,
 &#39;algiers&#39;: -0.12963974378708942,
 &#39;alhajuela&#39;: -0.009965684275311628,
 &#39;alianza&#39;: -0.02828456810234338,
 &#39;alice&#39;: 0.24649868930313276,
 &#39;alien&#39;: -0.1240164176266806,
 &#39;alienage&#39;: -0.04049715732036442,
 &#39;alienate&#39;: -0.10974956119768392,
 &#39;alienated&#39;: 0.04645672960750898,
 &#39;alienating&#39;: 0.026036199659779324,
 &#39;alienation&#39;: -0.11744578977744698,
 &#39;alienations&#39;: -0.046603451929374866,
 &#39;aliens&#39;: -0.04707690716000334,
 &#39;align&#39;: 0.06376560789675824,
 &#39;aligned&#39;: 0.05511116875213357,
 &#39;aligning&#39;: -0.022178273493332804,
 &#39;alike&#39;: -0.1081512074805005,
 &#39;aliment&#39;: -0.05274560931597213,
 &#39;alito&#39;: 0.21596721625808002,
 &#39;alive&#39;: 0.19179110547529082,
 &#39;alkali&#39;: -0.0343908627113539,
 &#39;allah&#39;: 0.05109726181479416,
 &#39;allawi&#39;: 0.07552244025083649,
 &#39;allay&#39;: -0.031374849355745464,
 &#39;allayed&#39;: -0.04772194592725011,
 &#39;allaying&#39;: -0.08152776265463853,
 &#39;allegany&#39;: -0.02828456810234329,
 &#39;allegation&#39;: -0.04796336894467641,
 &#39;allegations&#39;: -0.0390095566287529,
 &#39;allege&#39;: -0.01025481331949582,
 &#39;alleged&#39;: -0.20453699157963015,
 &#39;alleges&#39;: -0.04441488141104989,
 &#39;alleghanies&#39;: -0.03439086271135386,
 &#39;alleghany&#39;: -0.06605097927261157,
 &#39;alleghenies&#39;: -0.00546990526023897,
 &#39;allegheny&#39;: -0.046603451929374866,
 &#39;allegiance&#39;: -0.121368463854882,
 &#39;alleging&#39;: -0.056286508982345294,
 &#39;allen&#39;: 0.09201589997697121,
 &#39;allentown&#39;: 0.09994761868687886,
 &#39;alleviate&#39;: 0.03716512046890714,
 &#39;alleviated&#39;: -0.05306932055567454,
 &#39;alleviating&#39;: 0.04285868167609945,
 &#39;alleviation&#39;: -0.07631811563427766,
 &#39;alley&#39;: -0.010254813319495903,
 &#39;alleys&#39;: -0.04874210098336222,
 &#39;alleyways&#39;: 0.13658538634094228,
 &#39;allianca&#39;: -0.04049715732036445,
 &#39;alliance&#39;: 0.11635584193470483,
 &#39;alliances&#39;: 0.22946214481275168,
 &#39;allied&#39;: -0.02094243859267901,
 &#39;allies&#39;: 0.39096601908906575,
 &#39;allison&#39;: 0.04499096720578357,
 &#39;allocate&#39;: 0.08180857800620282,
 &#39;allocated&#39;: 0.18060053634919093,
 &#39;allocating&#39;: 0.053481380675218626,
 &#39;allocation&#39;: 0.03525596785815798,
 &#39;allocations&#39;: -0.05306932055567452,
 &#39;allot&#39;: -0.028284568102343338,
 &#39;allotment&#39;: -0.079944234928217,
 &#39;allotments&#39;: -0.08620912278882822,
 &#39;allotted&#39;: -0.09259983181405818,
 &#39;allottees&#39;: -0.0626341156828463,
 &#39;allotting&#39;: -0.02710600312180058,
 &#39;allow&#39;: 0.3397138466988839,
 &#39;allowable&#39;: 0.11216020790489994,
 &#39;allowance&#39;: -0.12125408330399036,
 &#39;allowances&#39;: -0.08983568485598231,
 &#39;allowed&#39;: -0.10578798907084243,
 &#39;allowing&#39;: 0.12532136909318617,
 &#39;allows&#39;: 0.21155756925777353,
 &#39;alloyed&#39;: -0.05739654012798692,
 &#39;alltime&#39;: 0.15894783526094983,
 &#39;allude&#39;: -0.08119264036659725,
 &#39;alluded&#39;: -0.1694075873911442,
 &#39;alludes&#39;: -0.04660345192937499,
 &#39;allure&#39;: 0.13658538634094228,
 &#39;allured&#39;: -0.046603451929374866,
 &#39;allurements&#39;: -0.035760442266425246,
 &#39;alluring&#39;: -0.059827408648718314,
 &#39;allusion&#39;: -0.09453889728242976,
 &#39;allusions&#39;: -0.03439086271135383,
 &#39;ally&#39;: 0.004821916875899133,
 &#39;almighty&#39;: -0.09541090964786565,
 &#39;almirante&#39;: -0.0038593896663010603,
 &#39;almodovar&#39;: -0.0038593896663010603,
 &#39;almond&#39;: -0.0343908627113539,
 &#39;almost&#39;: 0.07656922591411537,
 &#39;alone&#39;: 0.04574744195224406,
 &#39;along&#39;: 0.07097563621752506,
 &#39;alongside&#39;: 0.09951307633806795,
 &#39;aloof&#39;: -0.06458898642369806,
 &#39;aloofness&#39;: -0.055159256002321216,
 &#39;aloud&#39;: -0.04441488141104988,
 &#39;already&#39;: -0.04963321093171578,
 &#39;alsace&#39;: -0.04049715732036444,
 &#39;also&#39;: 0.19143909996080818,
 &#39;alsop&#39;: -0.006325316415995526,
 &#39;altar&#39;: -0.04049715732036442,
 &#39;alter&#39;: -0.019365635917894766,
 &#39;alteration&#39;: -0.12030519298484887,
 &#39;alterations&#39;: -0.11682828675401236,
 &#39;altercation&#39;: -0.03439086271135386,
 &#39;altercations&#39;: -0.04874210098336221,
 &#39;altered&#39;: 0.0339753574674603,
 &#39;altering&#39;: -0.05386845805950248,
 &#39;alternate&#39;: -0.08462266305582138,
 &#39;alternately&#39;: -0.06605097927261158,
 &#39;alternating&#39;: -0.04049715732036445,
 &#39;alternations&#39;: -0.009965684275311604,
 &#39;alternative&#39;: 0.12185375706073173,
 &#39;alternatives&#39;: 0.01413399651173138,
 &#39;alters&#39;: -0.04660345192937496,
 &#39;although&#39;: -0.2854891710377373,
 &#39;alto&#39;: -0.040497157320364414,
 &#39;altogether&#39;: -0.17748832210816062,
 &#39;alton&#39;: 0.11216020790489994,
 &#39;altruism&#39;: 0.03888467259677303,
 &#39;aluminum&#39;: -0.02126051711436894,
 &#39;alverstone&#39;: -0.028284568102343328,
 &#39;always&#39;: 0.16055222406865519,
 &#39;alzheimer&#39;: 0.42931978877360893,
 &#39;amable&#39;: -0.028284568102343328,
 &#39;amalgam&#39;: -0.028284568102343366,
 &#39;amalgamate&#39;: -0.028284568102343335,
 &#39;amalgamating&#39;: -0.05306932055567455,
 &#39;amalgamation&#39;: -0.059827408648718314,
 &#39;amanda&#39;: 0.16101056477698486,
 &#39;amapala&#39;: -0.04049715732036444,
 &#39;amass&#39;: -0.0314332226941129,
 &#39;amassed&#39;: -0.038582010650480135,
 &#39;amassing&#39;: -0.027106003121800597,
 &#39;amazed&#39;: 0.013743175395274398,
 &#39;amazement&#39;: -0.01886484142234341,
 &#39;amazing&#39;: 0.1927735339604465,
 &#39;amazingly&#39;: -0.028284568102343328,
 &#39;amazon&#39;: -0.09120537268785814,
 &#39;ambassador&#39;: -0.026612586239663418,
 &#39;ambassadors&#39;: -0.012735198360261131,
 &#39;ambiguity&#39;: -0.05628650898234529,
 &#39;ambiguous&#39;: -0.04566380998322619,
 &#39;ambition&#39;: -0.07110719517226329,
 &#39;ambitions&#39;: 0.1468278953878329,
 &#39;ambitious&#39;: 0.2910304806503701,
 &#39;ambitiously&#39;: 0.11216020790489994,
 &#39;ambristie&#39;: -0.034390862711353824,
 &#39;ambuscade&#39;: -0.046603451929375025,
 &#39;ambush&#39;: -0.02217827349333275,
 &#39;amelia&#39;: -0.04742524195889997,
 &#39;ameliorate&#39;: -0.07000095592429519,
 &#39;ameliorated&#39;: -0.013795712985868949,
 &#39;ameliorates&#39;: -0.04049715732036445,
 &#39;ameliorating&#39;: 0.12429937400267906,
 &#39;amelioration&#39;: -0.07865715122639534,
 &#39;ameliorations&#39;: -0.05739654012798687,
 &#39;amen&#39;: 0.020565788769741255,
 &#39;amenable&#39;: -0.038582010650480156,
 &#39;amend&#39;: -0.08677761448728627,
 &#39;amendable&#39;: -0.04660345192937496,
 &#39;amendatory&#39;: -0.11123044557616628,
 &#39;amended&#39;: -0.07785018393887641,
 &#39;amending&#39;: -0.1012291357684553,
 &#39;amendment&#39;: -0.04623891741465412,
 &#39;amendments&#39;: -0.09137867556855273,
 &#39;america&#39;: 1.0,
 &#39;american&#39;: 0.31195756678495423,
 &#39;americanism&#39;: -0.0650074763302393,
 &#39;americanization&#39;: -0.04660345192937506,
 &#39;americanizing&#39;: -0.04660345192937506,
 &#39;americans&#39;: 0.816472549222325,
 &#39;americas&#39;: 0.13636024094309043,
 &#39;americorps&#39;: 0.3497350008360275,
 &#39;amerika&#39;: -0.009965684275311635,
 &#39;amiable&#39;: 0.011838973029010323,
 &#39;amicable&#39;: -0.22924455296873533,
 &#39;amicably&#39;: -0.11265242141649517,
 &#39;amid&#39;: 0.12032358280336257,
 &#39;amidst&#39;: 0.0023582954052228772,
 &#39;amin&#39;: -0.028284568102343293,
 &#39;amiss&#39;: -0.027106003121800577,
 &#39;amistad&#39;: -0.06086867528306476,
 &#39;amities&#39;: -0.04660345192937489,
 &#39;amity&#39;: -0.2297196099993567,
 &#39;ammunition&#39;: 0.0005819301674280255,
 &#39;amnesty&#39;: 0.08138322961260458,
 &#39;among&#39;: -0.2594491612682058,
 &#39;amongst&#39;: -0.10176943952544533,
 &#39;amortization&#39;: -0.044414881411049885,
 &#39;amortizing&#39;: -0.02828456810234335,
 &#39;amount&#39;: -0.2916479753511773,
 &#39;amounted&#39;: -0.1953786081485323,
 &#39;amounting&#39;: -0.17558558943186145,
 &#39;amounts&#39;: -0.14691538108381777,
 &#39;amphibious&#39;: -0.020356192806926448,
 &#39;amphitheater&#39;: -0.014124344404863605,
 &#39;amphitrite&#39;: -0.04212291031685319,
 &#39;ample&#39;: -0.19021188478060713,
 &#39;ampler&#39;: -0.028284568102343328,
 &#39;amplest&#39;: -0.042323598637965455,
 &#39;amplification&#39;: -0.04008766183873756,
 &#39;amplified&#39;: -0.03504111098410711,
 &#39;amply&#39;: -0.15966150696198503,
 &#39;amsterdam&#39;: -0.046603451929374866,
 &#39;amtrak&#39;: 0.06867370709866252,
 &#39;amusement&#39;: -0.04008766183873756,
 &#39;amusing&#39;: 0.0022469049427095203,
 &#39;amusingly&#39;: 0.00835319955172007,
 &#39;anachronistic&#39;: -0.028284568102343328,
 &#39;analogous&#39;: -0.04236198460381683,
 &#39;analogy&#39;: -0.021600421063624833,
 &#39;analyses&#39;: 0.014136484797410621,
 &#39;analysis&#39;: 0.0746769895880021,
 &#39;analyst&#39;: -0.01607197888432221,
 &#39;analysts&#39;: 0.18543574321302703,
 &#39;analytical&#39;: -0.04049715732036442,
 &#39;analyze&#39;: 0.13278028796792732,
 &#39;analyzed&#39;: -0.03439086271135391,
 &#39;analyzes&#39;: -0.0314332226941129,
 &#39;anarchist&#39;: -0.040223311357500666,
 &#39;anarchistic&#39;: -0.038600117090723925,
 &#39;anarchists&#39;: -0.03439086271135384,
 &#39;anarchy&#39;: -0.07481744452393534,
 &#39;anatolia&#39;: -0.04049715732036444,
 &#39;anbar&#39;: 0.1849277559215033,
 &#39;ance&#39;: -0.034390862711353935,
 &#39;ancestors&#39;: -0.0512567397530064,
 &#39;ancestry&#39;: 0.17322315399500585,
 &#39;anchor&#39;: 0.08787837997490969,
 &#39;anchorage&#39;: -0.04008766183873757,
 &#39;anchored&#39;: 0.11569224276450613,
 &#39;ancient&#39;: -0.08351183595170168,
 &#39;andean&#39;: 0.09876508315275828,
 &#39;anderson&#39;: 0.01831387741017893,
 &#39;andersons&#39;: 0.026672083378751845,
 &#39;andes&#39;: 0.11216020790489994,
 &#39;andra&#39;: 0.16101056477698475,
 &#39;andrade&#39;: -0.009965684275311623,
 &#39;andreotti&#39;: 0.08162873485984708,
 &#39;andrew&#39;: 0.03495698641278811,
 &#39;andrews&#39;: 0.1487979755589635,
 &#39;anemic&#39;: -0.02828456810234338,
 &#39;aneurysm&#39;: 0.026672083378751845,
 &#39;anew&#39;: 0.03343098669708329,
 &#39;angel&#39;: -0.028284568102343286,
 &#39;angeles&#39;: 0.18411286776128363,
 &#39;angels&#39;: 0.07373759273989283,
 &#39;anger&#39;: 0.18477942207009457,
 &#39;angered&#39;: -0.0404971573203645,
 &#39;angle&#39;: -0.05739654012798686,
 &#39;angles&#39;: -0.05306932055567459,
 &#39;angola&#39;: 0.2154787571052725,
 &#39;angostura&#39;: -0.04049715732036439,
 &#39;angry&#39;: 0.00870725481385497,
 &#39;anguish&#39;: 0.0767472666136952,
 &#39;angular&#39;: 0.03888467259677301,
 &#39;animadversion&#39;: -0.046603451929374956,
 &#39;animadversions&#39;: -0.057396540127986916,
 &#39;animal&#39;: -0.10117652018284241,
 &#39;animals&#39;: -0.11445224946929528,
 &#39;animate&#39;: -0.0907475308530173,
 &#39;animated&#39;: -0.10522428955488942,
 &#39;animates&#39;: -0.04874210098336223,
 &#39;animating&#39;: -0.05628650898234513,
 &#39;animation&#39;: -0.046603451929374935,
 &#39;animosities&#39;: -0.03611841422521523,
 &#39;animosity&#39;: -0.03925089375629813,
 &#39;anna&#39;: -0.04049715732036483,
 &#39;annals&#39;: -0.09348887744382162,
 &#39;annapolis&#39;: 0.005120653859440889,
 &#39;annetter&#39;: 0.03888467259677303,
 &#39;annex&#39;: -0.05375414298765314,
 &#39;annexation&#39;: -0.07955957966199258,
 &#39;annexations&#39;: -0.04049715732036448,
 &#39;annexed&#39;: -0.12901201601075707,
 &#39;annexing&#39;: -0.0385021777288714,
 &#39;annihilate&#39;: -0.03143322269411291,
 &#39;annihilated&#39;: -0.061723759700299245,
 &#39;annihilating&#39;: -0.0038593896663010603,
 &#39;annihilation&#39;: -0.03332890441133383,
 &#39;anniversaries&#39;: 0.15262657133366336,
 &#39;anniversary&#39;: 0.15474333411540758,
 &#39;announce&#39;: 0.07493516995047532,
 &#39;announced&#39;: -0.0027319042028275562,
 &#39;announcement&#39;: -0.0055578443956153965,
 &#39;announcements&#39;: -0.04049715732036445,
 &#39;announces&#39;: -0.0573965401279869,
 &#39;announcing&#39;: 0.16887562005084067,
 &#39;annoy&#39;: -0.0530693205556746,
 &#39;annoyance&#39;: -0.10519745334245327,
 &#39;annoyed&#39;: -0.07538125052238613,
 &#39;annoying&#39;: -0.06248704547554215,
 &#39;annual&#39;: -0.246844140649894,
 &#39;annualized&#39;: 0.07552244025083645,
 &#39;annually&#39;: -0.1746282907746902,
 &#39;annuities&#39;: -0.10097946740310983,
 &#39;annuity&#39;: -0.08033273416425536,
 &#39;annul&#39;: -0.058286916107484175,
 &#39;annulled&#39;: -0.07004833803963342,
 &#39;annulling&#39;: -0.06172375970029925,
 &#39;annum&#39;: -0.14286707792383307,
 &#39;annunciation&#39;: -0.061723759700299224,
 &#39;anomalous&#39;: -0.14759854030554362,
 &#39;anomaly&#39;: -0.05829370614532078,
 &#39;anonymous&#39;: 0.0022469049427095203,
 &#39;another&#39;: 0.10121275839125882,
 &#39;answer&#39;: 0.10153685479092148,
 &#39;answerable&#39;: -0.0851247415807121,
 &#39;answered&#39;: -0.014582798979395356,
 &#39;answering&#39;: -0.054925752911450944,
 &#39;answers&#39;: 0.05617611179879165,
 &#39;ant&#39;: -0.03576044226642524,
 &#39;antagonism&#39;: -0.036646315160797846,
 &#39;antagonisms&#39;: 0.04499096720578383,
 &#39;antagonist&#39;: -0.05306932055567455,
 &#39;antagonistic&#39;: -0.018451563977175945,
 &#39;antagonists&#39;: -0.017336612652241994,
 &#39;antagonize&#39;: 0.044990967205783564,
 &#39;antarctic&#39;: -0.04049715732036449,
 &#39;antarctica&#39;: -0.0011426856879266417,
 &#39;antecedent&#39;: -0.08856874213760033,
 &#39;antecedently&#39;: -0.046603451929374956,
 &#39;antecedents&#39;: -0.04049715732036442,
 &#39;anterior&#39;: -0.024418411984988023,
 &#39;anthem&#39;: 0.1979094146384402,
 &#39;anthony&#39;: 0.14269168094995288,
 &#39;anthracite&#39;: -0.028726283387407764,
 &#39;anthrax&#39;: 0.22035384549978532,
 &#39;anti&#39;: -0.04660345192937506,
 &#39;anticipate&#39;: -0.16761116589258268,
 &#39;anticipated&#39;: -0.16028133670731723,
 &#39;anticipates&#39;: -0.009797124832551284,
 &#39;anticipating&#39;: -0.08904018744431343,
 &#39;anticipation&#39;: -0.13301738236743002,
 &#39;anticipations&#39;: -0.17939973807805606,
 &#39;antidemocratic&#39;: -0.04049715732036444,
 &#39;antidote&#39;: 0.007449685012369281,
 &#39;antidrug&#39;: 0.11216020790490001,
 &#39;antiforeign&#39;: -0.045632157349365265,
 ...}
</pre></div>
</div>
</div>
</div>
<p>Well that’s a lot of correlations, let’s make it clearer by only printing the 10 highest correlations using Counter (ofcourse the highest correlation is with the term itself)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span> 
<span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">cors</span><span class="p">)</span> 
  
<span class="n">highest</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">highest</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;america&#39;, 1.0), (&#39;americans&#39;, 0.816472549222325), (&#39;tonight&#39;, 0.7859819080193189), (&#39;chamber&#39;, 0.729127239070812), (&#39;child&#39;, 0.713061577767325), (&#39;bless&#39;, 0.6871248076324099), (&#39;let&#39;, 0.6849575245476137), (&#39;bipartisan&#39;, 0.6790242792149452), (&#39;help&#39;, 0.6768882113093627), (&#39;guests&#39;, 0.6660797941651454)]
</pre></div>
</div>
</div>
</div>
<p>We can find talk on jobs, children, families, etc. and lots of other promises to America. What about peace? Run find_cors() again but then with the term ‘peace’</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cors_peace</span> <span class="o">=</span> <span class="n">find_cors</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;peace&#39;</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">cors_peace</span><span class="p">)</span> 
  
<span class="n">highest</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">highest</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;peace&#39;, 1.0), (&#39;war&#39;, 0.5934693644098213), (&#39;forces&#39;, 0.5477506504609714), (&#39;military&#39;, 0.5396034510531938), (&#39;nations&#39;, 0.5314085270251379), (&#39;army&#39;, 0.5254546322919678), (&#39;contributions&#39;, 0.5170881559320253), (&#39;peoples&#39;, 0.48644654058611475), (&#39;occupation&#39;, 0.4742004312536388), (&#39;nation&#39;, 0.4714783974953244)]
</pre></div>
</div>
</div>
</div>
<p>Well, it seems that peace is mainly a thing of war! Quite sad now about the world, we move on from the simple linguistic statistics we have done so far - mainly based on word frequencies. Next, we enter the domain of word trends, which are currently hotly debated as a digital method. We would like to plot them as we have seen them earlier on the SOTU website. We already have the dataframe for that, the one with all the word counts for each year, let’s use that to plot the trend of ‘america’ over the years.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_america</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">items</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;america&#39;</span><span class="p">])</span>
<span class="n">df_america</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_america</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;year&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/Text_1_68_1.png" src="../../_images/Text_1_68_1.png" />
</div>
</div>
<p>For comparison, we add another line-plot about citizens</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_compare</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">items</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;america&#39;</span><span class="p">,</span> <span class="s1">&#39;citizen&#39;</span><span class="p">])</span>
<span class="n">df_compare</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Text_1_70_0.png" src="../../_images/Text_1_70_0.png" />
</div>
</div>
<p>Overall, the plot is not very nice. This is part of the limitation of the built-in plotting function. It is a quick solution but often not a very pretty one. Good for data exploration but not representation! Later in the course, we will meet a much more powerful plotting package called seaborn. As a little preview, we would like to use it next to visualise word trends. Use the same code as above, but before doing so install and import seaborn and type sns.set()</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">()</span>
<span class="n">df_compare</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Text_1_72_0.png" src="../../_images/Text_1_72_0.png" />
</div>
</div>
<p>Let’s check what we have learned next.</p>
<p>What does TF-IDF stand for? (Have to find some other answers for these)</p>
<ol class="simple">
<li><p>TextDealingsManager</p></li>
<li><p>TexasDallasMountains</p></li>
<li><p>TermDocumentMatrix</p></li>
<li><p>Don’t know</p></li>
</ol>
<p>Term frequency–inverse document frequency</p>
<p>In the content transformation workflow of the tm package for corp_small, how do I transform the content into lower cases?</p>
<ol class="simple">
<li><p>word.lower()</p></li>
<li><p>tm_map(corp_small, content_transformer(tolower))</p></li>
<li><p>tm_map(corp_small, removeNumbers)</p></li>
<li><p>tm_map(corp_small, tolower)</p></li>
</ol>
<p>word.lower()</p>
<p>Create a new plot for the historical development of the idea of ‘history’.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_history</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">items</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;history&#39;</span><span class="p">])</span>
<span class="n">df_history</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:xlabel=&#39;year&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/Text_1_74_1.png" src="../../_images/Text_1_74_1.png" />
</div>
</div>
<p>Find the word associations with ‘history’ in the speeches.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cors_history</span> <span class="o">=</span> <span class="n">find_cors</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="s1">&#39;history&#39;</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">cors_history</span><span class="p">)</span> 
  
<span class="n">highest</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">highest</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;history&#39;, 1.0), (&#39;world&#39;, 0.5506352648608028), (&#39;years&#39;, 0.5299129845559173), (&#39;new&#39;, 0.5035340261157771), (&#39;ago&#39;, 0.46575563901563594), (&#39;us&#39;, 0.4579664853277646), (&#39;people&#39;, 0.45311911535390026), (&#39;president&#39;, 0.44309687427868005), (&#39;life&#39;, 0.4401603707428053), (&#39;peaceful&#39;, 0.4398454376297889)]
</pre></div>
</div>
</div>
</div>
<p>Within your working group, go back to the ten topics you have found and try and give each of them a title that describes it. Sometimes this is not so easy, because the corpus is quite small. But do try your best.</p>
<p>Let’s move on to the second part of this session.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./SocialAnalytics/Text"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../SocialMedia/Social_Sensing_2.html" title="previous page">Social Sensing 2</a>
    <a class='right-next' id="next-link" href="Text_2.html" title="next page">Analysing Historical Cultures 2</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By DIMPAH<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>