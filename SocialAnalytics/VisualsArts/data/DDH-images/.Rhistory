stops <- stopwords("english")
stops_file <- file("data/stopwords.txt")
writeLines(stops, stops_file)
close(stops_file)
reuters <- VCorpus(DirSource(reut21578),                   + readerControl = list(reader = readReut21578XMLasPlain))
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- VCorpus(DirSource(reut21578), readerControl = list(reader = readReut21578XMLasPlain))
inspect(reuters[1:2])
lapply(reuters[1:2], as.character)
lapply(reuters, as.character)
reuters <- tm_map(reuters, stripWhitespace) #strip whitespacesreuters <- tm_map(reuters, content_transformer(tolower)) #to lower reuters <- tm_map(reuters, removeWords, stopwords("english")) #remove stopwordstm_map(reuters, stemDocument)
reuters_dtm <- DocumentTermMatrix(reuters)
inspect(reuters_dtm[5:10, 740:743])
library(topicmodels)
k <- 5
SEED <- 1000
reuters_TM <- list(VEM = LDA(reuters_dtm, k = k, control = list(seed = SEED)), VEM_fixed = LDA(reuters_dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)), Gibbs = LDA(reuters_dtm, k = k, method = "Gibbs", control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000)), CTM = CTM(reuters_dtm, k = k, control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))))
Terms <- terms(reuters_TM[["VEM"]], 5)
Terms[,1:5]
Topics <- topics(reuters_TM[["VEM"]], 5)
Topics
str(Topics)
library(LDAvis)
install.packages('servr')
install.packages(‘LDAvis’)
install.packages('LDAvis')
VEM = LDA(reuters_dtm, k = k, control = list(seed = SEED))
VEM
reuters_TM[["VEM"]]
library(servr)
library(mallet)
vem_mallet = MalletLDA(k)
instance = mallet.import(names(reuters, as.character), reuters, as.character))
instance = mallet.import(names(reuters, as.character), reuters, as.character)
instance = mallet.import(names(as.character(reuters)), as.character(reuters))
as.charcter(reuters)
as.character(reuters)
reuters[1]
lapply(reuters, as.character)
reuters[i]
class(reuters)
length(reuters, as.character)
length(reuters)
seq(1:length(reuters))
instance = mallet.import(seq(1:length(reuters)),lapply(reuters, as.character)), /Users/tobiasblanke/Downloads/stopwords.txt, FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
instance = mallet.import(seq(1:length(reuters)),lapply(reuters, as.character)), "/Users/tobiasblanke/Downloads/stopwords.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
instance = mallet.import(seq(1:length(reuters)),lapply(reuters, as.character), "/Users/tobiasblanke/Downloads/stopwords.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
str(lapply(reuters, as character))
str(lapply(reuters, as.character))
str(lapply(reuters[[2]], as.character))
str(lapply(reuters[[,2]], as.character))
str(lapply(reuters[,[2]], as.character))
str(lapply(reuters[,2], as.character))
instance = mallet.import(seq(index = 1:length(reuters)),lapply(reuters[[index]], as.character), "/Users/tobiasblanke/Downloads/stopwords.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
instance = mallet.import(index = seq(1:length(reuters)),lapply(reuters[[index]], as.character), "/Users/tobiasblanke/Downloads/stopwords.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
index = seq(1:length(reuters))
index
names(reuters, as.character)
names(reuters)
as.character(reuters)
instance = mallet.import(names(reuters), as.character(reuters)), "/Users/tobiasblanke/Downloads/stopwords.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
instance = mallet.import(names(reuters), as.character(reuters), "/Users/tobiasblanke/Downloads/stopwords.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
instance = mallet.import(names(reuters), as.character(reuters), "en.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
instance = mallet.import(as.character(names(reuters)), as.character(reuters), "/Users/tobiasblanke/Downloads/stopwords.txt", FALSE, token.regexp="\\p{L}[\\p{L}\\p{P}]+\\p{L}")
reuters$topics
reuters[[1]]$topics
reuters[[3]]$topics
library(mallet)
library(tm)
library(dplyr)
k <- 30
tracts <- mallet.read.dir("/Users/tobiasblanke/Downloads/OralHistoryTemp/")
stops <- stopwords("SMART")
stops_file <- file("/Users/tobiasblanke/Downloads/stopwords.txt")
writeLines(stops, stops_file)
close(stops_file)
inst <- mallet.import(tracts$id, tracts$text, "/Users/tobiasblanke/Downloads/stopwords.txt")
model = MalletLDA(k)
model$loadDocuments(inst)
model$train(30)
phi = t(mallet.topic.words(model, smoothed = TRUE, normalized = TRUE))
phi.count =mallet.topic.words(model, smoothed = TRUE, normalized = FALSE))
phi.count =mallet.topic.words(model, smoothed = TRUE, normalized = FALSE)
topic.counts = rowSums(topic.words)
topic.counts = rowSums(mallet.topic.words)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(model, smoothed=T, normalized=T)
topic.counts = rowSums(topic.words)
topic.proportions =  topic.counts/sum(topic.counts)
vocab = model$getVocabulary()
term.frequency = apply(phi.count, 1, sum)
EHRIOralTest <- list(K = k, phi = phi, term.frequency = term.frequency, vocab = vocab, topic.proportion = topic.proportion)
EHRIOralTest <- list(K = k, phi = phi, term.frequency = term.frequency, vocab = vocab, topic.proportion = topic.proportions)
json <- createJSON(K = K, phi = EHRIOralTest$phi, term.frequency = EHRIOralTest$term.frequency, vocab = EHRIOralTest$vocab, topic.proportion=EHRIOralTest$topic.proportion, n.terms=30))
json <- createJSON(K = K, phi = EHRIOralTest$phi, term.frequency = EHRIOralTest$term.frequency, vocab = EHRIOralTest$vocab, topic.proportion=EHRIOralTest$topic.proportion, n.terms=30)
library(LDAvis)
json <- createJSON(K = K, phi = EHRIOralTest$phi, term.frequency = EHRIOralTest$term.frequency, vocab = EHRIOralTest$vocab, topic.proportion=EHRIOralTest$topic.proportion, n.terms=30)
json <- createJSON(K = EHRIOralTest$K, phi = EHRIOralTest$phi, term.frequency = EHRIOralTest$term.frequency, vocab = EHRIOralTest$vocab, topic.proportion=EHRIOralTest$topic.proportion, n.terms=30)
serVis(json, out.dir = 'vis', open.browser = FALSE)
serVis(json, out.dir = "/Users/tobiasblanke/Downloads/vis", open.browser = FALSE)
serVis(json)
serVis(json)
library(recommenderlab)
library(ggplot2)
data(MovieLense)
view(MovieLense)
MovieLense
image(sample(MovieLense, 500), main = "Raw ratings")
qplot(getRatings(MovieLense), binwidth = 1, main = "Histogram of ratings", xlab = "Rating")
qplot(rowCounts(MovieLense), binwidth = 10, main = "Movies Rated on average", xlab = "# of users", ylab = "# of movies rated")
r <- Recommender(MovieLense[1:860], method = "POPULAR") names(getModel(r))
r <- Recommender(MovieLense[1:860], method = "POPULAR")
names(getModel(r))
recom <- predict(r, MovieLense[870:871], n=5)
as(recom, "list")
r<-Recommender(MovieLense[1:860], method = "UBCF")
recom<- predict(r, MovieLense[10,], n=5)
as(recom, "list")
recom_rate <- predict(r, MovieLense[10,], type="ratings")
as(recom_rate, "list")
map_w <- ggplot2::fortify(world, region="iso_a3")
library(ggvis)
library(openNLP)
install.packages("openNLP")
install.packages("openNLP")
install.packages("rvest")
library(rvest)
lego_movie <- html("http://www.imdb.com/title/tt1490017/")
summary(lego_movie)
lego_movie %>%
html_node("strong span") %>%
html_text() %>%
as.numeric()
demo(package = "rvest")
library(rvest)
lego_movie <- html("http://www.parliament.uk/business/committees/committees-a-z/joint-select/draft-investigatory-powers-bill/publications/?type=Written#pnlPublicationFilter")
lego_movie %>%
html_node("strong span") %>%
html_text() %>%
as.numeric()
draf.bill <- html("http://www.parliament.uk/business/committees/committees-a-z/joint-select/draft-investigatory-powers-bill/publications/?type=Written#pnlPublicationFilter")
require(devtools)
install_github("ngramr", "seancarmody")
require(ngramr)
require(ggplot2)
ng  <- ngram(c("hacker", "programmer"), year_start = 1950)
ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) +
geom_line()
ggram(c("monarchy", "democracy"), year_start = 1500, year_end = 2000,
corpus = "eng_gb_2012", ignore_case = TRUE,
geom = "area", geom_options = list(position = "stack")) +
labs(y = NULL)
ggram(c("Holocaust", "Shoah"), year_start = 1500, year_end = 2000,
corpus = "eng_gb_2012", ignore_case = TRUE,
geom = "area", geom_options = list(position = "stack")) +
labs(y = NULL)
View(ng)
library(RJSONIO)
library(RCurl)
library(ggplot2)
devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)
page <- "Digital Humanities"
raw_data <- getURL(paste("http://stats.grok.se/json/en/latest90/", page, sep=""))
page <- "Digital%20Humanities"
raw_data <- getURL(paste("http://stats.grok.se/json/en/latest90/", page, sep=""))
data <- fromJSON(raw_data)
views <- data.frame(timestamp=paste(names(data$daily_views), " 12:00:00", sep=""), stringsAsFactors=F)
views$count <- data$daily_views
views$timestamp <- as.POSIXlt(views$timestamp) # Transform to POSIX datetime
views <- views[order(views$timestamp),]
ggplot(views, aes(timestamp, count)) + geom_line() + scale_x_datetime() + xlab("") + ylab("views")
res = AnomalyDetectionVec(views$count, max_anoms=0.05, direction='both', plot=TRUE, period=7)
res$plot
summary(res)
summary(res$plot)
library(calibrator)
install.package("calibrator")
install.packages("calibrator")
library(calibrator)
data(toys)
create.new.toy.datasets(D1=D1.toy , D2=D2.toy)
require(ngramr)
require(ggplot2)
ng  <- ngram(c("hacker", "programmer"), year_start = 1950)
ggplot(ng, aes(x=Year, y=Frequency, colour=Phrase)) + geom_line()
View(ng)
library(RJSONIO)
library(RCurl)
library(ggplot2)
install.packages("devtools")
devtools::install_github("twitter/AnomalyDetection")
library(AnomalyDetection)
page <- "USA"
raw_data <- getURL(paste("http://stats.grok.se/json/en/latest90/", page, sep=""))
data <- fromJSON(raw_data)
views <- data.frame(timestamp=paste(names(data$daily_views), " 12:00:00", sep=""), stringsAsFactors=F)
views$count <- data$daily_views
views$timestamp <- as.POSIXlt(views$timestamp) # Transform to POSIX datetime
views <- views[order(views$timestamp),]
View(ng)
View(views)
View(ng)
View(views)
View(ng)
View(views)
View(ng)
res = AnomalyDetectionVec(views$count, max_anoms=0.05, direction='both', plot=TRUE, period=7)
res$plot
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.05, direction='both', plot=TRUE, period=7)
res$plot
str(res)
ng  <- ngram(c("digital humanities"), year_start = 1950)
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.05, direction='both', plot=TRUE, period=7)
ng  <- ngram(c("digital humanities"), year_start = 1950)
res$plot
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
ng  <- ngram(c("digital humanities"), year_start = 1990)
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.05, direction='both', plot=TRUE, period=7)
res$plot
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.01, direction='both', plot=TRUE, period=7)
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', plot=TRUE, period=7)
res$plot
ng  <- ngram(c("war"), year_start = 1800)
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', plot=TRUE, period=7)
res$plot
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
ng  <- ngram(c("Muslim"), year_start = 1900)
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', plot=TRUE, period=7)
res$plot
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', plot=TRUE, period=7)
res$plot
ng  <- ngram(c("Hitler"), year_start = 1900)
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', plot=TRUE, period=7)
res$plot
ng  <- ngram(c("capitalism", "capital"), year_start = 1900)
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
ng  <- ngram(c(capital"), year_start = 1900)
""
ng  <- ngram(c("capital"), year_start = 1900)
ggram(ng, year_start = 1800, google_theme = TRUE) +
theme(legend.direction = "vertical")
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', plot=TRUE, period=7)
res$plot
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', plot=TRUE)
res = AnomalyDetectionVec(ng$Frequency, max_anoms=0.1, direction='both', period=1)
install.packages("geonames")
library(geonames)
options(geonamesUsername="tobiasb")
options(geonamesHost="api.geonames.org")
source(system.file("tests","testing.R",package="geonames"),echo=TRUE)
source(system.file("tests","testing.R",package="geonames"),echo=TRUE)
results<-GNwikipediaSearch("oriole", maxRows = 10)
View(results)
cities <- c("Gereshk", "Lahkar Gah", "Lashkar Gah", "Marjah", "Nad-e Ali")
# conveninence function to look up and format results
GNsearchAF <- function(x) {
res <- GNsearch(name=x, country="AF")
return(res[1, ])
}
GNresult <- sapply(cities, GNsearchAF)
GNresult <- do.call("rbind", GNresult)
GNresult <- cbind(city=row.names(GNresult),
subset(GNresult, select=c("lng", "lat", "adminName1")))
View(results)
View(results)
View(GNresult)
View(GNresult)
GNsearchAF('Berlin')
results<-GNwikipediaSearch("Berlin", maxRows = 10)
View(results)
View(results)
results<-GNwikipediaSearch("Auschwitz", maxRows = 10)
View(results)
results<-GNwikipediaSearch("Oświęcim", maxRows = 10)
View(results)
library(dplyr)
# From http://stackoverflow.com/questions/1181060
stocks <- data_frame(
time = as.Date('2009-01-01') + 0:9,
X = rnorm(10, 0, 1),
Y = rnorm(10, 0, 2),
Z = rnorm(10, 0, 4)
)
gather(stocks, stock, price, -time)
library(tidyr)
gather(stocks, stock, price, -time)
library("rjson")
library(rjson)
json_file <- "http://newmedia.report/js/newspackages.json"
json_data <- fromJSON(file=json_file)
library(jsonlite)
json_data <- fromJSON(file=json_file, flatten = T)
json_data <- fromJSON(json_file, flatten = T)
View(json_data)
library(rvest)
lfs1 <- json_data[1,4]
lfs1_html <- html(lfs1)
head(lfs1_html)
lfs1_html_txt <- html_text(lfs1_html)
head(lfs_1_html_txt)
head(lfs1_html_txt)
library (EBImage)
setwd("/Users/tobiasblanke/Downloads/DDH-images")
filenames <- Sys.glob(file.path(getwd(), "*.jpg"))
images_list <- lapply(filenames, readImage)
names(images_list) <- basename(filenames)
ImageAvg <- Reduce("+", images_list) / length(images_list)
display(ImageAvg)
ImageAvg_matrix <- as.matrix(Image_Avg)
ImageAvg_matrix <- as.matrix(ImageAvg)
View(ImageAvg_matrix)
View(ImageAvg_matrix)
h <- dim(ImageAvg)
h <- dim(ImageAvg)[1]
w <- dim(ImageAvg)[2]
m <- matrix(ImageAvg, h*w)
View(m)
View(m)
pca <- prcomp(m)
extractColors <- function(x) rgb(x[1], x[2], x[3])
(colors <- apply(abs(pca$rotation), 2, extractColors))
pie(pca$sdev, col = colors, labels = colors)
ImageAvg_matrix <- NULL
image(matrix(pca$x[, 1], h), col = gray.colors(100))
image(matrix(pca$x[, 2], h), col = gray.colors(100))
image(matrix(pca$x[, 3], h), col = gray.colors(100))
summary(pca)
display(AverageImage, "Average DH Person")
display(ImageAvg, "Average DH Person")
display(ImageAvg, "Average DH Person")
library(httr)
faceURL = "https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&returnFaceLandmarks=true&returnFaceAttributes=age,gender,smile,facialHair"
img.url = '/Users/tobiasblanke/Downloads/DDH-images/AverageImage/2a7015e87846.png'
faceKEY = 'f4fa517135bd4a6e92bd4a28e4fcf668'
mybody = list(url = img.url)
faceResponse = POST(
url = faceURL,
content_type('application/json'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = faceKEY)),
body = mybody,
encode = 'json'
)
AvgFace = = content(faceResponse)[[1]]
AvgFace = content(faceResponse)[[1]]
names(AvgFace)
AvgFace$code
img.url = 'http://www.buro247.com/images/Angelina-Jolie-2.jpg'
mybody = list(url = img.url)
faceResponse = POST(
url = faceURL,
content_type('application/json'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = faceKEY)),
body = mybody,
encode = 'json'
)
AvgFace = content(faceResponse)[[1]]
names(AvgFace)
require("devtools")
install_github("flovv/Roxford")
install_github("flovv/Roxford")
faceResponse
faceResponse = POST(
url = faceURL,
content_type('application/json'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = faceKEY)),
body = mybody,
encode = 'json'
)
faceResponse
faceKEY = "2304d1bf83184a198a90add8ad5a6b21"
faceResponse = POST(
url = faceURL,
content_type('application/json'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = faceKEY)),
body = mybody,
encode = 'json'
)
faceResponse
AvgFace = content(faceResponse)[[1]]
library(httr)
faceURL = "https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&returnFaceLandmarks=true&returnFaceAttributes=age,gender,smile,facialHair"
img.url = 'http://www.buro247.com/images/Angelina-Jolie-2.jpg'
mybody = list(url = img.url)
faceResponse = POST(
url = faceURL,
content_type('application/json'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = faceKEY)),
body = mybody,
encode = 'json'
)
faceResponse
faceURL
img.url = '/Users/tobiasblanke/Downloads/DDH-images/AverageImage/2a7015e87846.png'
faceURL = "https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&returnFaceLandmarks=true&returnFaceAttributes=age"
mybody = list(url = img.url)
faceResponse = POST(
url = faceURL,
content_type('application/json'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = faceKEY)),
body = mybody,
encode = 'json'
)
faceResponse
x = combine(images_list)
if (interactive()) display(x)
x = combine(image_list, flip(image_list), flop(image_list))
x = combine(images_list, flip(images_list), flop(images_list))
lena = readImage(system.file("images", "lena-color.png", package="EBImage"))
display(x, all=TRUE)
x = combine(images_list[[1]])
display(x, all=TRUE)
x = combine(images_list[[1]], images_list[[2]])
display(x, all=TRUE)
images_list[[2]]
images_list[[1]]
images_list[[3]]
images_list[[4]]
getFaceResponse <- function(img.path, key){
faceURL = "https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&returnFaceAttributes=age,gender,smile,facialHair,headPose"
mybody = upload_file(img.path)
faceResponse = POST(
url = faceURL,
content_type('application/octet-stream'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
body = mybody,
encode = 'multipart'
)
}
getFaceResponse(img.url, faceKEY)
dataframeFromJSON <- function(l) {
l1 <- lapply(l, function(x) {
x[sapply(x, is.null)] <- NA
unlist(x)
})
keys <- unique(unlist(lapply(l1, names)))
l2 <- lapply(l1, '[', keys)
l3 <- lapply(l2, setNames, keys)
res <- data.frame(do.call(rbind, l3))
return(res)
}
getFaceResponse <- function(img.path, key){
faceURL = "https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&returnFaceAttributes=age,gender,smile,facialHair,headPose"
mybody = upload_file(img.path)
faceResponse = POST(
url = faceURL,
content_type('application/octet-stream'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
body = mybody,
encode = 'multipart'
)
# con <- content(faceResponse)[[1]]
#  df <- data.frame(t(unlist(con$faceAttributes)))
better <- dataframeFromJSON(content(faceResponse))
# cn <- c("faceAttributes.smile", "faceAttributes.gender", "faceAttributes.age", "faceAttributes.facialHair.moustache", "faceAttributes.facialHair.beard", "faceAttributes.facialHair.sideburns")
df <-   better
return(df)
}
getFaceResponse(img.url, faceKEY)
faceURL = "https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&returnFaceAttributes=age"
mybody = upload_file(img.url)
faceResponse = POST(
url = faceURL,
content_type('application/octet-stream'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
body = mybody,
encode = 'multipart'
)
key <- faceKEY
faceResponse = POST(
url = faceURL,
content_type('application/octet-stream'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
body = mybody,
encode = 'multipart'
)
faceRsponse
faceResponse
better <- dataframeFromJSON(content(faceResponse))
View(better)
AngelinaFace = content(faceResponse)[[1]]
AngelinaFace = content(faceResponse)
mybody = upload_file("/Users/tobiasblanke/Downloads/DDH-images/BtihajAjana-1.jpg")
faceResponse = POST(
url = faceURL,
content_type('application/octet-stream'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
body = mybody,
encode = 'multipart'
)
faceResponse
better <- dataframeFromJSON(content(faceResponse))
View(better)
mybody = upload_file("/Users/tobiasblanke/Downloads/DDH-images/AverageImage/pjimage.jpg")
faceResponse = POST(
url = faceURL,
content_type('application/octet-stream'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
body = mybody,
encode = 'multipart'
)
better <- dataframeFromJSON(content(faceResponse))
View(better)
faceURL = "https://api.projectoxford.ai/face/v1.0/detect?returnFaceId=true&returnFaceAttributes=gender"
faceResponse = POST(
url = faceURL,
content_type('application/octet-stream'), add_headers(.headers = c('Ocp-Apim-Subscription-Key' = key)),
body = mybody,
encode = 'multipart'
)
df_gender <- dataframeFromJSON(content(faceResponse))
View(df_gender)
